{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from utils.transformation import *\n",
    "from utils.models import *\n",
    "\n",
    "VAR_SEED = 42\n",
    "VAR_TESTSET_SIZE = 0.20\n",
    "VAR_DIR_DATA_CLEAN = '../data/cleaning'\n",
    "\n",
    "random.seed(VAR_SEED)\n",
    "np.random.seed(VAR_SEED)\n",
    "\n",
    "catalogo = pd.read_csv(f\"{VAR_DIR_DATA_CLEAN}/catalogo.csv\", sep=\",\", encoding=\"latin1\")[['id_ejercicio', 'h1', 'h2', 'h3', 'h4', 's1', 's2', 's3', 's4', 'k1', 'k2', 'k3', 'k4']]\n",
    "mf_dataset = pd.read_csv(f\"{VAR_DIR_DATA_CLEAN}/mf_dataset.csv\", sep=\",\", encoding=\"latin1\")\n",
    "diagnostico = pd.read_csv(f\"{VAR_DIR_DATA_CLEAN}/prueba_diagnostico.csv\", sep=\",\", encoding=\"latin1\")[['id_estudiante', 'score_a', 'score_p', 'score_d', 'score_s']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Definición del modelo UserTowerRecommender\n",
    "class UserTowerRecommender(nn.Module):\n",
    "    def __init__(self, user_feature_dim, hidden_dims, num_categories):\n",
    "        super(UserTowerRecommender, self).__init__()\n",
    "        \n",
    "        # Define the user tower as a series of dense layers\n",
    "        layers = []\n",
    "        input_dim = user_feature_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.5))  # Add dropout for regularization\n",
    "            input_dim = hidden_dim\n",
    "        self.user_tower = nn.Sequential(*layers)\n",
    "        \n",
    "        # Final layer to produce predictions for each category\n",
    "        self.output_layer = nn.Linear(input_dim, num_categories)\n",
    "\n",
    "    def forward(self, user_features):\n",
    "        user_out = self.user_tower(user_features)\n",
    "        return self.output_layer(user_out)\n",
    "\n",
    "# Suposiciones:\n",
    "# m características generales de los usuarios\n",
    "# 3 características por ítem\n",
    "# n ítems con los que interactúa cada usuario\n",
    "\n",
    "m = 5  # Por ejemplo, 5 características generales del usuario\n",
    "n = 10  # El usuario ha interactuado con 10 ítems\n",
    "item_features_per_item = 3  # Cada ítem tiene 3 características\n",
    "\n",
    "# Total de características de entrada por usuario\n",
    "user_feature_dim = m + (item_features_per_item * n)  # m características + 3 * n características de los ítems\n",
    "\n",
    "hidden_dims = [128, 64]  # Dimensiones de las capas ocultas\n",
    "num_categories = 5  # Número de categorías a predecir (por ejemplo, 5 tipos de productos)\n",
    "\n",
    "# Crear el modelo\n",
    "model = UserTowerRecommender(user_feature_dim, hidden_dims, num_categories)\n",
    "\n",
    "# Simulamos datos de entrenamiento\n",
    "num_samples = 1000  # Número de usuarios en el dataset de entrenamiento\n",
    "train_features = torch.randn(num_samples, user_feature_dim)  # Características del usuario\n",
    "train_labels = torch.randint(0, 2, (num_samples, num_categories)).float()  # Etiquetas binarias de interés (0 o 1)\n",
    "\n",
    "# Función de entrenamiento\n",
    "def train_model(model, train_features, train_labels, num_epochs=50, learning_rate=0.001):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.BCEWithLogitsLoss()  # Usamos BCEWithLogitsLoss para clasificación binaria\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Establecer el modelo en modo de entrenamiento\n",
    "        optimizer.zero_grad()  # Limpiar los gradientes previos\n",
    "        \n",
    "        # Realizar la predicción\n",
    "        outputs = model(train_features)\n",
    "        \n",
    "        # Calcular la pérdida\n",
    "        loss = criterion(outputs, train_labels)\n",
    "        \n",
    "        # Realizar la retropropagación\n",
    "        loss.backward()\n",
    "        \n",
    "        # Actualizar los pesos del modelo\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Imprimir la pérdida cada cierto número de épocas\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Entrenar el modelo\n",
    "train_model(model, train_features, train_labels, num_epochs=50, learning_rate=0.001)\n",
    "\n",
    "# Realizar predicciones\n",
    "user_features_test = torch.randn(1, user_feature_dim)  # Características de un usuario de prueba\n",
    "category_scores = model(user_features_test)\n",
    "\n",
    "print(\"Predicciones para el usuario de prueba:\", category_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional two towers recommender systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class UserTowerRecommender(nn.Module):\n",
    "    def __init__(self, user_feature_dim, hidden_dims, num_categories):\n",
    "        super(UserTowerRecommender, self).__init__()\n",
    "        \n",
    "        # Define the user tower as a series of dense layers\n",
    "        layers = []\n",
    "        input_dim = user_feature_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.5))  # Add dropout for regularization\n",
    "            input_dim = hidden_dim\n",
    "        self.user_tower = nn.Sequential(*layers)\n",
    "        \n",
    "        # Final layer to produce predictions for each category\n",
    "        self.output_layer = nn.Linear(input_dim, num_categories)\n",
    "\n",
    "    def forward(self, user_features):\n",
    "        user_out = self.user_tower(user_features)\n",
    "        return self.output_layer(user_out)\n",
    "\n",
    "# Example usage\n",
    "user_feature_dim = 10  # Suppose each user is represented by 10 features\n",
    "hidden_dims = [128, 64]  # Two hidden layers\n",
    "num_categories = 5  # Predict interest in 5 categories\n",
    "\n",
    "model = UserTowerRecommender(user_feature_dim, hidden_dims, num_categories)\n",
    "user_features = torch.rand((1, user_feature_dim))\n",
    "category_scores = model(user_features)\n",
    "\n",
    "print(category_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class ItemTowerRecommender(nn.Module):\n",
    "    def __init__(self, item_feature_dim, hidden_dims):\n",
    "        super(ItemTowerRecommender, self).__init__()\n",
    "        \n",
    "        # Define the item tower as a series of dense layers\n",
    "        layers = []\n",
    "        input_dim = item_feature_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.5))  # Add dropout for regularization\n",
    "            input_dim = hidden_dim\n",
    "        self.item_tower = nn.Sequential(*layers)\n",
    "        \n",
    "        # Final layer to produce a score for the item's predicted popularity\n",
    "        self.output_layer = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, item_features):\n",
    "        item_out = self.item_tower(item_features)\n",
    "        return self.output_layer(item_out)\n",
    "\n",
    "# Example usage\n",
    "item_feature_dim = 8  # Suppose each item is represented by 8 features\n",
    "hidden_dims = [64, 32]  # Two hidden layers\n",
    "\n",
    "model = ItemTowerRecommender(item_feature_dim, hidden_dims)\n",
    "item_features = torch.rand((1, item_feature_dim))\n",
    "popularity_score = model(item_features)\n",
    "\n",
    "print(popularity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense and Sparse Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample user-item interaction matrix (ratings)\n",
    "R = np.array([\n",
    "    [5, 3, 0, 1],\n",
    "    [4, 0, 0, 1],\n",
    "    [1, 1, 0, 5],\n",
    "    [1, 0, 0, 4],\n",
    "    [0, 1, 5, 4],\n",
    "])\n",
    "\n",
    "# Perform SVD\n",
    "U, sigma, Vt = np.linalg.svd(R, full_matrices=False)\n",
    "\n",
    "print(\"User embeddings (dense vectors):\")\n",
    "print(U)\n",
    "\n",
    "print(\"\\nItem embeddings (dense vectors):\")\n",
    "print(Vt.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "# Define the Neural Network\n",
    "class ComplexDenseRepresentationNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dense_dim):\n",
    "        super(ComplexDenseRepresentationNetwork, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, dense_dim)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Custom Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = 10\n",
    "hidden_dim = 20\n",
    "dense_dim = 5\n",
    "batch_size = 5\n",
    "learning_rate = 0.01\n",
    "epochs = 2000\n",
    "\n",
    "# Sample data: 10 one-hot encoded vectors representing 10 items\n",
    "data = torch.eye(10)\n",
    "targets = torch.rand(10, 5)\n",
    "\n",
    "# Split data into training and validation sets (80% train, 20% validation)\n",
    "dataset = CustomDataset(data, targets)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the network\n",
    "model = ComplexDenseRepresentationNetwork(input_dim, hidden_dim, dense_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training Loop with Validation\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_data, batch_targets in train_loader:\n",
    "        outputs = model(batch_data)\n",
    "        loss = criterion(outputs, batch_targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_targets in val_loader:\n",
    "            outputs = model(batch_data)\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    # Print losses every 100 epochs\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "# Extract and print dense vectors\n",
    "with torch.no_grad():\n",
    "    dense_vectors = model(data)\n",
    "    print(dense_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class ComplexContextualRecommender(nn.Module):\n",
    "    def __init__(self, num_users, num_items, num_contexts, user_embed_dim, item_embed_dim, context_embed_dim):\n",
    "        super(ComplexContextualRecommender, self).__init__()\n",
    "\n",
    "        # Embedding layers with different dimensions\n",
    "        self.user_embedding = nn.Embedding(num_users, user_embed_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, item_embed_dim)\n",
    "        self.context_embedding = nn.Embedding(num_contexts, context_embed_dim)\n",
    "\n",
    "        # Calculate total embeddings dimension\n",
    "        total_embed_dim = user_embed_dim + item_embed_dim + context_embed_dim\n",
    "        \n",
    "        # Dense layers with InstanceNorm and Dropout\n",
    "        self.fc1 = nn.Linear(total_embed_dim, 256)\n",
    "        self.in1 = nn.InstanceNorm1d(256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.in2 = nn.InstanceNorm1d(128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.in3 = nn.InstanceNorm1d(64)\n",
    "        self.fc_out = nn.Linear(64, 1)  # Output a score\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, user, item, context):\n",
    "        user_embed = self.user_embedding(user)\n",
    "        item_embed = self.item_embedding(item)\n",
    "        context_embed = self.context_embedding(context)\n",
    "\n",
    "        # Concatenate the embeddings\n",
    "        x = torch.cat([user_embed, item_embed, context_embed], 1)\n",
    "\n",
    "        # Pass through dense layers with activations, instance normalization, and dropout\n",
    "        x = self.dropout(F.relu(self.in1(self.fc1(x).unsqueeze(1)).squeeze(1)))\n",
    "        x = self.dropout(F.relu(self.in2(self.fc2(x).unsqueeze(1)).squeeze(1)))\n",
    "        x = self.dropout(F.relu(self.in3(self.fc3(x).unsqueeze(1)).squeeze(1)))\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "# Hyperparameters and Data\n",
    "num_users = 1000\n",
    "num_items = 5000\n",
    "num_contexts = 3\n",
    "user_embed_dim = 50\n",
    "item_embed_dim = 100\n",
    "context_embed_dim = 10\n",
    "batch_size = 32\n",
    "\n",
    "# Initialize the complex model\n",
    "model = ComplexContextualRecommender(num_users, num_items, num_contexts, user_embed_dim, item_embed_dim, context_embed_dim)\n",
    "criterion = nn.MSELoss() # Assuming a regression problem\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Generate random sample data\n",
    "users = torch.randint(0, num_users, (1000,))\n",
    "items = torch.randint(0, num_items, (1000,))\n",
    "contexts = torch.randint(0, num_contexts, (1000,))\n",
    "ratings = torch.rand((1000, 1)) # Random ratings\n",
    "\n",
    "# Sample Training loop for 100 epochs\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Create mini-batches\n",
    "    for i in range(0, len(users), batch_size):\n",
    "        batch_users = users[i:i+batch_size]\n",
    "        batch_items = items[i:i+batch_size]\n",
    "        batch_contexts = contexts[i:i+batch_size]\n",
    "        batch_ratings = ratings[i:i+batch_size]\n",
    "\n",
    "        # Skip the batch if size is 1\n",
    "        if len(batch_users) == 1:\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_users, batch_items, batch_contexts)\n",
    "        loss = criterion(outputs, batch_ratings)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        avg_loss = total_loss / (len(users) // batch_size)\n",
    "        print(f'Epoch [{epoch+1}/100], Loss: {avg_loss:.4f}')\n",
    "\n",
    "\n",
    "# Example prediction\n",
    "user = torch.tensor([5])\n",
    "item = torch.tensor([100])\n",
    "context = torch.tensor([2])\n",
    "score = model(user, item, context)\n",
    "print(f\"Predicted Score: {score.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".sistema-de-recomendacion-unab-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
