{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "255\n",
      "8\n",
      "273\n",
      "2303\n",
      "514\n"
     ]
    }
   ],
   "source": [
    "c = '00000001'\n",
    "print(int(str(c), 2))\n",
    "\n",
    "c = '11111111'\n",
    "print(int(str(c), 2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1\t0\t0\t0\t1\t1\t0\t0\t0\t1\t1\t1\n",
    "minimo = '000100010001'\n",
    "maximo = '100011111111'\n",
    "print(int(str(minimo), 2))\n",
    "print(int(str(maximo), 2))\n",
    "\n",
    "c = '001000000010'\n",
    "print(int(str(c), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from learntools.time_series.utils import plot_periodogram, seasonal_plot\n",
    "from learntools.time_series.style import *\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown, display\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "# Function to display markdown\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "# Define directory and read data\n",
    "comp_dir = Path('../input/amazon-product-reviews')\n",
    "electronics_data = pd.read_csv(\n",
    "    comp_dir / \"ratings_Electronics (1).csv\",\n",
    "    dtype={'rating': 'int8'},\n",
    "    names=['userId', 'productId', 'rating', 'timestamp'],\n",
    "    index_col=None,\n",
    "    header=0\n",
    ")\n",
    "\n",
    "# Display some basic information\n",
    "printmd(f\"Number of Rating: {electronics_data.shape[0]:,}\")\n",
    "printmd(f\"Columns: {np.array2string(electronics_data.columns.values)}\")\n",
    "printmd(f\"Number of Users: {len(electronics_data.userId.unique()):,}\")\n",
    "printmd(f\"Number of Products: {len(electronics_data.productId.unique()):,}\")\n",
    "electronics_data.describe()['rating'].reset_index()\n",
    "\n",
    "# Check for missing values\n",
    "printmd('**Number of missing values**:')\n",
    "pd.DataFrame(\n",
    "    electronics_data.isnull().sum().reset_index()\n",
    ").rename(columns={0: \"Total missing\", \"index\": \"Columns\"})\n",
    "\n",
    "# Process data by date\n",
    "data_by_date = electronics_data.copy()\n",
    "data_by_date.timestamp = pd.to_datetime(electronics_data.timestamp, unit=\"s\")\n",
    "data_by_date = data_by_date.sort_values(by=\"timestamp\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "printmd(\"Number of Ratings each day:\")\n",
    "data_by_date.groupby(\"timestamp\")[\"rating\"].count().tail(10).reset_index()\n",
    "\n",
    "# Add year and month columns\n",
    "data_by_date[\"year\"] = data_by_date.timestamp.dt.year\n",
    "data_by_date[\"month\"] = data_by_date.timestamp.dt.month\n",
    "rating_by_year = data_by_date.groupby([\"year\", \"month\"])[\"rating\"].count().reset_index()\n",
    "\n",
    "# Create date column and plot data\n",
    "rating_by_year[\"date\"] = pd.to_datetime(rating_by_year[\"year\"].astype(str) + \"-\" + rating_by_year[\"month\"].astype(str) + \"-1\")\n",
    "rating_by_year.plot(x=\"date\", y=\"rating\")\n",
    "plt.title(\"Number of Ratings over Years\")\n",
    "plt.show()\n",
    "\n",
    "# Group by product and calculate statistics\n",
    "rating_by_product = electronics_data.groupby(\"productId\").agg({\n",
    "    \"userId\": \"count\",\n",
    "    \"rating\": \"mean\"\n",
    "}).rename(columns={\"userId\": \"Number of Ratings\", \"rating\": \"Average Rating\"}).reset_index()\n",
    "\n",
    "# Filter top-rated products\n",
    "cutoff = 50\n",
    "top_rated = rating_by_product.loc[\n",
    "    rating_by_product[\"Number of Ratings\"] > cutoff\n",
    "].sort_values(by=\"Average Rating\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Define TensorFlow model\n",
    "class RankingModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        embedding_dimension = 32\n",
    "\n",
    "        # Embedding layers for users and products\n",
    "        self.user_embeddings = tf.keras.Sequential([\n",
    "            tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=unique_userIds, mask_token=None),\n",
    "            tf.keras.layers.Embedding(len(unique_userIds) + 1, embedding_dimension)\n",
    "        ])\n",
    "\n",
    "        self.product_embeddings = tf.keras.Sequential([\n",
    "            tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=unique_productIds, mask_token=None),\n",
    "            tf.keras.layers.Embedding(len(unique_productIds) + 1, embedding_dimension)\n",
    "        ])\n",
    "\n",
    "        # Ratings layers\n",
    "        self.ratings = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "\n",
    "    def call(self, userId, productId):\n",
    "        user_embeddings = self.user_embeddings(userId)\n",
    "        product_embeddings = self.product_embeddings(productId)\n",
    "        return self.ratings(tf.concat([user_embeddings, product_embeddings], axis=1))\n",
    "\n",
    "class amazonModel(tfrs.models.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ranking_model = RankingModel()\n",
    "        self.task = tfrs.tasks.Ranking(\n",
    "            loss=tf.keras.losses.MeanSquaredError(),\n",
    "            metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, features, training=False):\n",
    "        rating_predictions = self.ranking_model(features[\"userId\"], features[\"productId\"])\n",
    "        return self.task(labels=features[\"rating\"], predictions=rating_predictions)\n",
    "\n",
    "# Filter recent data\n",
    "cutoff_no_rat = 50\n",
    "cutoff_year = 2011\n",
    "recent_data = data_by_date.loc[data_by_date[\"year\"] > cutoff_year]\n",
    "print(f\"Number of Rating: {recent_data.shape[0]:,}\")\n",
    "print(f\"Number of Users: {len(recent_data.userId.unique()):,}\")\n",
    "print(f\"Number of Products: {len(recent_data.productId.unique()):,}\")\n",
    "del data_by_date  # Free up memory\n",
    "\n",
    "recent_prod = recent_data.loc[\n",
    "    recent_data.groupby(\"productId\")[\"rating\"].transform('count').ge(cutoff_no_rat)\n",
    "].reset_index(drop=True).drop([\"timestamp\", \"year\", \"month\"], axis=1)\n",
    "del recent_data  # Free up memory\n",
    "\n",
    "# Prepare data for training\n",
    "userIds = recent_prod.userId.unique()\n",
    "productIds = recent_prod.productId.unique()\n",
    "total_ratings = len(recent_prod.index)\n",
    "\n",
    "ratings = tf.data.Dataset.from_tensor_slices({\n",
    "    \"userId\": tf.cast(recent_prod.userId.values, tf.string),\n",
    "    \"productId\": tf.cast(recent_prod.productId.values, tf.string),\n",
    "    \"rating\": tf.cast(recent_prod.rating.values, tf.int8)\n",
    "})\n",
    "\n",
    "# Shuffle and split data\n",
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "train = shuffled.take(int(total_ratings * 0.8))\n",
    "test = shuffled.skip(int(total_ratings * 0.8)).take(int(total_ratings * 0.2))\n",
    "\n",
    "unique_productIds = productIds\n",
    "unique_userIds = userIds\n",
    "\n",
    "# Compile and train model\n",
    "model = amazonModel()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n",
    "\n",
    "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
    "cached_test = test.batch(4096).cache()\n",
    "model.fit(cached_train, epochs=10)\n",
    "\n",
    "# Evaluate model\n",
    "model.evaluate(cached_test, return_dict=True)\n",
    "\n",
    "# Recommend products for a random user\n",
    "user_rand = userIds[123]\n",
    "test_rating = {}\n",
    "for m in test.take(5):\n",
    "    test_rating[m[\"productId\"].numpy()] = RankingModel()(\n",
    "        tf.convert_to_tensor([user_rand]), tf.convert_to_tensor([m[\"productId\"]])\n",
    "    )\n",
    "\n",
    "print(f\"Top 5 recommended products for User {user_rand}:\")\n",
    "for m in sorted(test_rating, key=test_rating.get, reverse=True):\n",
    "    print(m.decode())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los sistemas de calificación ponderada se utilizan para puntuar la calificación de cada película. Esta es la fórmula de la puntuación ponderada.\n",
    "WR = (v ÷ (v+m)) × R + (m ÷ (v+m)) × C\n",
    "R es la puntuación media del artículo.\n",
    "v es el número de votos del artículo.\n",
    "m es el mínimo de votos necesarios para figurar en los artículos populares (definido por > percentil 80 del total de votos).\n",
    "C es la valoración media de todo el conjunto de datos.\n",
    "\n",
    "\n",
    "\n",
    "Vemos que cada método tiene su punto fuerte. Lo mejor sería poder combinar todos esos puntos fuertes y ofrecer una recomendación mejor. Esta idea nos lleva a otra mejora de la recomendación, que es el método híbrido. Por ejemplo, podemos combinar las recomendaciones de filtrado colaborativo basadas en el contenido y en los elementos para aprovechar las características de ambos dominios (géneros e interacción usuario-elemento).\n",
    "\n",
    "Traducción realizada con la versión gratuita del traductor DeepL.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.externals import joblib\n",
    "import scipy.sparse\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "\n",
    "\n",
    "electronics_data=pd.read_csv(\"/kaggle/input/amazon-product-reviews/ratings_Electronics (1).csv\",names=['userId', 'productId','Rating','timestamp'])\n",
    "\n",
    "\n",
    "# Display the data\n",
    "\n",
    "electronics_data.head()\n",
    "\n",
    "#Shape of the data\n",
    "electronics_data.shape\n",
    "\n",
    "#Taking subset of the dataset\n",
    "electronics_data=electronics_data.iloc[:1048576,0:]\n",
    "\n",
    "#Check the datatypes\n",
    "electronics_data.dtypes\n",
    "\n",
    "\n",
    "electronics_data.info()\n",
    "\n",
    "\n",
    "\n",
    "#Five point summary \n",
    "\n",
    "electronics_data.describe()['Rating'].T\n",
    "\n",
    "\n",
    "#Find the minimum and maximum ratings\n",
    "print('Minimum rating is: %d' %(electronics_data.Rating.min()))\n",
    "print('Maximum rating is: %d' %(electronics_data.Rating.max()))\n",
    "\n",
    "#Check for missing values\n",
    "print('Number of missing values across columns: \\n',electronics_data.isnull().sum())\n",
    "\n",
    "\n",
    "# Check the distribution of the rating\n",
    "with sns.axes_style('white'):\n",
    "    g = sns.factorplot(\"Rating\", data=electronics_data, aspect=2.0,kind='count')\n",
    "    g.set_ylabels(\"Total number of ratings\")\n",
    "\n",
    "\n",
    "print(\"Total data \")\n",
    "print(\"-\"*50)\n",
    "print(\"\\nTotal no of ratings :\",electronics_data.shape[0])\n",
    "print(\"Total No of Users   :\", len(np.unique(electronics_data.userId)))\n",
    "print(\"Total No of products  :\", len(np.unique(electronics_data.productId)))\n",
    "\n",
    "\n",
    "#Dropping the Timestamp column\n",
    "\n",
    "electronics_data.drop(['timestamp'], axis=1,inplace=True)\n",
    "\n",
    "#Analysis of rating given by the user \n",
    "\n",
    "no_of_rated_products_per_user = electronics_data.groupby(by='userId')['Rating'].count().sort_values(ascending=False)\n",
    "\n",
    "no_of_rated_products_per_user.head()\n",
    "\n",
    "no_of_rated_products_per_user.describe()\n",
    "\n",
    "quantiles = no_of_rated_products_per_user.quantile(np.arange(0,1.01,0.01), interpolation='higher')\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title(\"Quantiles and their Values\")\n",
    "quantiles.plot()\n",
    "# quantiles with 0.05 difference\n",
    "plt.scatter(x=quantiles.index[::5], y=quantiles.values[::5], c='orange', label=\"quantiles with 0.05 intervals\")\n",
    "# quantiles with 0.25 difference\n",
    "plt.scatter(x=quantiles.index[::25], y=quantiles.values[::25], c='m', label = \"quantiles with 0.25 intervals\")\n",
    "plt.ylabel('No of ratings by user')\n",
    "plt.xlabel('Value at the quantile')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "print('\\n No of rated product more than 50 per user : {}\\n'.format(sum(no_of_rated_products_per_user >= 50)) )\n",
    "\n",
    "#Getting the new dataframe which contains users who has given 50 or more ratings\n",
    "\n",
    "new_df=electronics_data.groupby(\"productId\").filter(lambda x:x['Rating'].count() >=50)\n",
    "\n",
    "no_of_ratings_per_product = new_df.groupby(by='productId')['Rating'].count().sort_values(ascending=False)\n",
    "\n",
    "fig = plt.figure(figsize=plt.figaspect(.5))\n",
    "ax = plt.gca()\n",
    "plt.plot(no_of_ratings_per_product.values)\n",
    "plt.title('# RATINGS per Product')\n",
    "plt.xlabel('Product')\n",
    "plt.ylabel('No of ratings per product')\n",
    "ax.set_xticklabels([])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Average rating of the product \n",
    "\n",
    "new_df.groupby('productId')['Rating'].mean().head()\n",
    "\n",
    "\n",
    "new_df.groupby('productId')['Rating'].mean().sort_values(ascending=False).head()\n",
    "\n",
    "#Total no of rating for product\n",
    "\n",
    "new_df.groupby('productId')['Rating'].count().sort_values(ascending=False).head()\n",
    "\n",
    "ratings_mean_count = pd.DataFrame(new_df.groupby('productId')['Rating'].mean())\n",
    "\n",
    "ratings_mean_count['rating_counts'] = pd.DataFrame(new_df.groupby('productId')['Rating'].count())\n",
    "\n",
    "ratings_mean_count.head()\n",
    "ratings_mean_count['rating_counts'].max()\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.rcParams['patch.force_edgecolor'] = True\n",
    "ratings_mean_count['rating_counts'].hist(bins=50)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.rcParams['patch.force_edgecolor'] = True\n",
    "ratings_mean_count['Rating'].hist(bins=50)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.rcParams['patch.force_edgecolor'] = True\n",
    "sns.jointplot(x='Rating', y='rating_counts', data=ratings_mean_count, alpha=0.4)\n",
    "\n",
    "popular_products = pd.DataFrame(new_df.groupby('productId')['Rating'].count())\n",
    "most_popular = popular_products.sort_values('Rating', ascending=False)\n",
    "most_popular.head(30).plot(kind = \"bar\")\n",
    "\n",
    "\n",
    "\n",
    "from surprise import KNNWithMeans\n",
    "from surprise import Dataset\n",
    "from surprise import accuracy\n",
    "from surprise import Reader\n",
    "import os\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#Reading the dataset\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(new_df,reader)\n",
    "\n",
    "\n",
    "#Splitting the dataset\n",
    "trainset, testset = train_test_split(data, test_size=0.3,random_state=10)\n",
    "\n",
    "\n",
    "# Use user_based true/false to switch between user-based or item-based collaborative filtering\n",
    "algo = KNNWithMeans(k=5, sim_options={'name': 'pearson_baseline', 'user_based': False})\n",
    "algo.fit(trainset)\n",
    "\n",
    "\n",
    "\n",
    "# run the trained model against the testset\n",
    "test_pred = algo.test(testset)\n",
    "\n",
    "\n",
    "# get RMSE\n",
    "print(\"Item-based Model : Test Set\")\n",
    "accuracy.rmse(test_pred, verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "new_df1=new_df.head(10000)\n",
    "ratings_matrix = new_df1.pivot_table(values='Rating', index='userId', columns='productId', fill_value=0)\n",
    "ratings_matrix.head()\n",
    "ratings_matrix.shape\n",
    "\n",
    "\n",
    "X = ratings_matrix.T\n",
    "X.head()\n",
    "\n",
    "X.shape\n",
    "X1 = X\n",
    "\n",
    "#Decomposing the Matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "SVD = TruncatedSVD(n_components=10)\n",
    "decomposed_matrix = SVD.fit_transform(X)\n",
    "decomposed_matrix.shape\n",
    "\n",
    "\n",
    "#Correlation Matrix\n",
    "\n",
    "correlation_matrix = np.corrcoef(decomposed_matrix)\n",
    "correlation_matrix.shape\n",
    "\n",
    "\n",
    "X.index[75]\n",
    "\n",
    "i = \"B00000K135\"\n",
    "\n",
    "product_names = list(X.index)\n",
    "product_ID = product_names.index(i)\n",
    "product_ID\n",
    "\n",
    "\n",
    "correlation_product_ID = correlation_matrix[product_ID]\n",
    "correlation_product_ID.shape\n",
    "\n",
    "Recommend = list(X.index[correlation_product_ID > 0.65])\n",
    "\n",
    "# Removes the item already bought by the customer\n",
    "Recommend.remove(i) \n",
    "\n",
    "Recommend[0:24]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "%matplotlib inline\n",
    "\n",
    "# Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from surprise import Reader, Dataset, SVD\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "df = pd.read_csv(\"/kaggle/input/amazon-product-reviews/ratings_Electronics (1).csv\",\n",
    "                             names=['userId', 'productId','rating','timestamp'])\n",
    "\n",
    "df.head()\n",
    "\n",
    "print(\"Total Reviews:\",df.shape[0])\n",
    "print(\"Total Columns:\",df.shape[1])\n",
    "\n",
    "# Taking subset of the dataset\n",
    "df = df.iloc[:5000,0:]\n",
    "\n",
    "print(\"Total Reviews:\",df.shape[0])\n",
    "print(\"Total Columns:\",df.shape[1])\n",
    "\n",
    "print(\"Total number of ratings :\",df.rating.nunique())\n",
    "print(\"Total number of users   :\", df.userId.nunique())\n",
    "print(\"Total number of products  :\", df.productId.nunique())\n",
    "\n",
    "df.info()\n",
    "\n",
    "# Check missing value\n",
    "df.isnull().sum()\n",
    "\n",
    "# Check Duplicate data\n",
    "df[df.duplicated()].any()\n",
    "\n",
    "# rating describe summary \n",
    "df.describe()['rating']\n",
    "\n",
    "print(\"Unique value of Rating:\",df.rating.unique())\n",
    "\n",
    "# Find the minimum and maximum ratings\n",
    "print('Minimum rating is: %d' %(df.rating.min()))\n",
    "print('Maximum rating is: %d' %(df.rating.max()))\n",
    "\n",
    "# Average rating of products\n",
    "ratings = pd.DataFrame(df.groupby('productId')['rating'].mean())\n",
    "ratings['ratings_count'] = pd.DataFrame(df.groupby('productId')['rating'].count())\n",
    "ratings['ratings_average'] = pd.DataFrame(df.groupby('productId')['rating'].mean())\n",
    "ratings.head(10)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "ratings['rating'].hist(bins=70)\n",
    "\n",
    "sns.jointplot(x='rating',y='ratings_count',data=ratings,alpha=0.5)\n",
    "\n",
    "# Most top 30 products\n",
    "popular_products = pd.DataFrame(df.groupby('productId')['rating'].count())\n",
    "most_popular = popular_products.sort_values('rating', ascending=False)\n",
    "most_popular.head(30).plot(kind = \"bar\",figsize=(12, 4))\n",
    "\n",
    "vote_counts = ratings[ratings['ratings_count'].notnull()]['ratings_count'].astype('int')\n",
    "vote_averages = ratings[ratings['ratings_average'].notnull()]['ratings_average'].astype('int')\n",
    "C = vote_averages.mean()\n",
    "print(\"Average rating of product across the whole dataset is\",C)\n",
    "\n",
    "m = vote_counts.quantile(0.95)\n",
    "print(\"Minimum votes required to be listed in the chart is\",m)\n",
    "\n",
    "ratings.head()\n",
    "\n",
    "qualified = ratings[(ratings['ratings_count'] >= m) & (ratings['ratings_count'].notnull()) & (ratings['ratings_average'].notnull())][['ratings_count', 'ratings_average']]\n",
    "\n",
    "qualified['ratings_count'] = qualified['ratings_count'].astype('int')\n",
    "qualified['ratings_average'] = qualified['ratings_average'].astype('int')\n",
    "qualified.head().sort_values(by='ratings_count', ascending=False)\n",
    "\n",
    "qualified.shape\n",
    "\n",
    "def weighted_rating(x):\n",
    "    v = x['ratings_count']\n",
    "    R = x['ratings_average']\n",
    "    return (v/(v+m) * R) + (m/(m+v) * C)\n",
    "\n",
    "qualified['wr'] = qualified.apply(weighted_rating, axis=1)\n",
    "qualified = qualified.sort_values('wr', ascending=False).head(20)\n",
    "qualified.head(10)\n",
    "\n",
    "# Add color\n",
    "from matplotlib import cm\n",
    "color = cm.inferno_r(np.linspace(.4, .8, 30))\n",
    "\n",
    "rating_plot_count = qualified['ratings_count'].plot.bar(figsize=(12, 4),color=color)\n",
    "rating_plot_count.set_title(\"Rating Count Bar-Plot\")\n",
    "rating_plot_count.set_xlabel(\"productId\")\n",
    "rating_plot_count.set_ylabel(\"Count\")\n",
    "\n",
    "\n",
    "rating_plot_avg = qualified['ratings_average'].plot.bar(figsize=(12, 4),color=color)\n",
    "rating_plot_avg.set_title(\"Rating Average Bar-Plot\")\n",
    "rating_plot_avg.set_xlabel(\"productId\")\n",
    "rating_plot_avg.set_ylabel(\"rating\")\n",
    "\n",
    "wr_plot = qualified['wr'].plot.bar(figsize=(12, 4),color=color)\n",
    "wr_plot.set_title(\"Weight Rating Bar-Plot\")\n",
    "wr_plot.set_xlabel(\"productId\")\n",
    "wr_plot.set_ylabel(\"rating\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "reader = Reader()\n",
    "df.head()\n",
    "\n",
    "data = Dataset.load_from_df(df[['userId', 'productId', 'rating']], reader)\n",
    "\n",
    "# Use the famous SVD algorithm\n",
    "svd = SVD()\n",
    "\n",
    "# Run 5-fold cross-validation and then print results\n",
    "cross_validate(svd, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n",
    "\n",
    "trainset = data.build_full_trainset()\n",
    "svd.fit(trainset)\n",
    "\n",
    "df[df['userId'] == 'AKM1MP6P0OYPR']\n",
    "\n",
    "svd.predict(uid='A17HMM1M7T9PJ1', iid='0970407998', r_ui=None)\n",
    "svd.predict(uid='A17HMM1M7T9PJ1', iid='0970407998', r_ui=None).est\n",
    "\n",
    "\n",
    "df_users=df.groupby('userId').filter(lambda x: x['rating'].count()>=50)\n",
    "df_users.head()\n",
    "df_users.shape\n",
    "\n",
    "\n",
    "matrix=pd.pivot_table(data=df_users, values='rating', index='userId',columns='productId')\n",
    "matrix.head()\n",
    "\n",
    "# Function that takes in productId and useId as input and outputs up to 5 most similar products.\n",
    "def hybrid_recommendations(userId, productId):\n",
    "    \n",
    "    # Get the Id of the top five products that are correlated with the ProductId chosen by the user.\n",
    "    top_five=matrix.corrwith(matrix[productId]).sort_values(ascending=False).head(5)\n",
    "    \n",
    "    # Predict the ratings the user might give to these top 5 most correlated products.\n",
    "    est_rating=[]\n",
    "    for x in list(top_five.index):\n",
    "        if str(top_five[x])!='nan':\n",
    "            est_rating.append(svd.predict(userId, iid=x, r_ui=None).est)\n",
    "           \n",
    "    return pd.DataFrame({'productId':list(top_five.index)[:len(est_rating)], 'estimated_rating':est_rating}).sort_values(by='estimated_rating', ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "hybrid_recommendations('A2NYK9KWFMJV4Y', 'B00LI4ZZO8')\n",
    "\n",
    "# df.head()\n",
    "\n",
    "# df['userId'].value_counts()\n",
    "\n",
    "# # Check specific userId review\n",
    "# df[df['userId'] == 'A3LDPF5FMB782Z']\n",
    "\n",
    "# # predict based on this data\n",
    "# svd.predict('A3LDPF5FMB782Z', '140053271X', 5.0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "# Build a model.\n",
    "class RankingModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        embedding_dimension = 32\n",
    "\n",
    "        self.user_embeddings = tf.keras.Sequential([\n",
    "                                    tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "                                        vocabulary=unique_userIds, mask_token=None),\n",
    "                                        # add addional embedding to account for unknow tokens\n",
    "                                    tf.keras.layers.Embedding(len(unique_userIds)+1, embedding_dimension)\n",
    "                                    ])\n",
    "\n",
    "        self.product_embeddings = tf.keras.Sequential([\n",
    "                                    tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "                                        vocabulary=unique_productIds, mask_token=None),\n",
    "                                    # add addional embedding to account for unknow tokens\n",
    "                                    tf.keras.layers.Embedding(len(unique_productIds)+1, embedding_dimension)\n",
    "                                    ])\n",
    "        # Set up a retrieval task and evaluation metrics over the\n",
    "        # entire dataset of candidates.\n",
    "        self.ratings = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "                            tf.keras.layers.Dense(64,  activation=\"relu\"),\n",
    "                            tf.keras.layers.Dense(1)\n",
    "                              ])\n",
    "    def call(self, userId, productId):\n",
    "        user_embeddings  = self.user_embeddings (userId)\n",
    "        product_embeddings = self.product_embeddings(productId)\n",
    "        return self.ratings(tf.concat([user_embeddings,product_embeddings], axis=1))\n",
    "\n",
    "# Build a model.\n",
    "class amazonModel(tfrs.models.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ranking_model: tf.keras.Model = RankingModel()\n",
    "        self.task: tf.keras.layers.Layer   = tfrs.tasks.Ranking(\n",
    "                                                    loss    =  tf.keras.losses.MeanSquaredError(),\n",
    "                                                    metrics = [tf.keras.metrics.RootMeanSquaredError()])\n",
    "            \n",
    "\n",
    "    def compute_loss(self, features, training=False):\n",
    "        rating_predictions = self.ranking_model(features[\"userId\"], features[\"productId\"]  )\n",
    "\n",
    "        return self.task( labels=features[\"rating\"], predictions=rating_predictions)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cutoff_no_rat = 50    ## Only count products which received more than or equal 50\n",
    "cutoff_year   = 2011  ## Only count Rating after 2011\n",
    "recent_data   = data_by_date.loc[data_by_date[\"year\"] > cutoff_year]\n",
    "print(\"Number of Rating: {:,}\".format(recent_data.shape[0]) )\n",
    "print(\"Number of Users: {:,}\".format(len(recent_data.userId.unique()) ) )\n",
    "print(\"Number of Products: {:,}\".format(len(recent_data.productId.unique())  ) )\n",
    "del data_by_date  ### Free up memory ###\n",
    "recent_prod   = recent_data.loc[recent_data.groupby(\"productId\")[\"rating\"].transform('count').ge(cutoff_no_rat)].reset_index(\n",
    "                    drop=True).drop([\"timestamp\",\"year\",\"month\"],axis=1)\n",
    "del recent_data  ### Free up memory ###\n",
    "\n",
    "\n",
    "userIds    = recent_prod.userId.unique()\n",
    "productIds = recent_prod.productId.unique()\n",
    "total_ratings= len(recent_prod.index)\n",
    "\n",
    "\n",
    "ratings = tf.data.Dataset.from_tensor_slices( {\"userId\":tf.cast( recent_prod.userId.values  ,tf.string),\n",
    "                                \"productId\":tf.cast( recent_prod.productId.values,tf.string),\n",
    "                                \"rating\":tf.cast( recent_prod.rating.values  ,tf.int8,) } )\n",
    "\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take( int(total_ratings*0.8) )\n",
    "test = shuffled.skip(int(total_ratings*0.8)).take(int(total_ratings*0.2))\n",
    "\n",
    "unique_productIds = productIds\n",
    "unique_userIds    = userIds\n",
    "\n",
    "\n",
    "model = amazonModel()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad( learning_rate=0.1 ))\n",
    "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
    "cached_test = test.batch(4096).cache()\n",
    "model.fit(cached_train, epochs=10)\n",
    "\n",
    "\n",
    "# Evaluate.\n",
    "model.evaluate(cached_test, return_dict=True)\n",
    "\n",
    "\n",
    "user_rand = userIds[123]\n",
    "test_rating = {}\n",
    "for m in test.take(5):\n",
    "    test_rating[m[\"productId\"].numpy()]=RankingModel()(tf.convert_to_tensor([user_rand]),tf.convert_to_tensor([m[\"productId\"]]))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Top 5 recommended products for User {}: \".format(user_rand))\n",
    "for m in sorted(test_rating, key=test_rating.get, reverse=True):\n",
    "    print(m.decode())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "from scipy.sparse import csr_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from surprise import Reader, Dataset, SVD\n",
    "from surprise.model_selection import KFold\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "# Import both datasets\n",
    "\n",
    "df5=pd.read_csv('../input/amazon-cell-phones-reviews/20191226-items.csv')\n",
    "df6=pd.read_csv('../input/amazon-cell-phones-reviews/20191226-reviews.csv')\n",
    "\n",
    "df5.rename(columns={'rating':'avgRating'}, inplace=True)\n",
    "columns = ['url', 'reviewUrl', 'totalReviews', 'originalPrice']\n",
    "df5.drop(columns, inplace=True, axis=1)\n",
    "# Drop uneeded columns before merging both datasets\n",
    "\n",
    "columns = ['date', 'verified', 'title', 'body', 'helpfulVotes']\n",
    "df6.drop(columns, inplace=True, axis=1)\n",
    "\n",
    "df6\n",
    "# Merging 2 df to create training data for the SVD\n",
    "\n",
    "ratings = pd.merge(df5, df6, how='inner', on='asin')\n",
    "\n",
    "columns = ['brand', 'price', 'image', 'avgRating']\n",
    "ratings.drop(columns, inplace=True, axis=1)\n",
    "\n",
    "ratings = ratings[['name', 'asin', 'title', 'rating']]\n",
    "ratings = ratings.sort_values(by=['name'], ascending=True)\n",
    "ratings = ratings.reset_index(drop=True)\n",
    "\n",
    "ratings\n",
    "\n",
    "\n",
    "# Create a pivot table to see how the data columns correspond to one another\n",
    "\n",
    "matrix=pd.pivot_table(data=ratings[['name', 'asin', 'rating']], values='rating', index='name',columns='asin')\n",
    "matrix.head()\n",
    "\n",
    "# Initialize the SVD model and train the model on the created dataset\n",
    "\n",
    "svd = SVD()\n",
    "reader = Reader()\n",
    "data = Dataset.load_from_df(ratings[['name', 'asin', 'rating']], reader)\n",
    "cross_validate(svd, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n",
    "\n",
    "trainset = data.build_full_trainset()\n",
    "svd.fit(trainset)\n",
    "\n",
    "# Function that takes in productId and userId as input and outputs up to 5 most similar products.\n",
    "def hybrid_recommendations(userId, productId):\n",
    "    \n",
    "    # Get the Id of the top five products that are correlated with the ProductId chosen by the user.\n",
    "    top_five=matrix.corrwith(matrix[productId]).sort_values(ascending=False).head(15)\n",
    "    \n",
    "    # Predict the ratings the user might give to these top 5 most correlated products.\n",
    "    est_rating=[]\n",
    "    for x in list(top_five.index):\n",
    "        if str(top_five[x])!='nan':\n",
    "            est_rating.append(svd.predict(userId, iid=x, r_ui=None).est)\n",
    "           \n",
    "    return pd.DataFrame({'productId':list(top_five.index)[:len(est_rating)], 'estimated_rating':est_rating}).sort_values(by='estimated_rating', ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "find_product = hybrid_recommendations('John', 'B018OMP8ES')\n",
    "\n",
    "# Function that maps the productId to the productName\n",
    "def product_mapping(recommender_df): \n",
    "    rows_list = []\n",
    "    rows_list2 = []\n",
    "\n",
    "    # Append productName into finalDf\n",
    "    for x in recommender_df['productId']:\n",
    "        cut_row = ratings.loc[ratings['asin'] == x]\n",
    "        cut_row = cut_row['title'][0:1].values\n",
    "        rows_list.append(cut_row[0])\n",
    "\n",
    "    # Copy over the estimated ratings into the finalDf\n",
    "    for i in recommender_df['estimated_rating']:\n",
    "        rows_list2.append(i)\n",
    "\n",
    "    # Creating the finalDf\n",
    "    cut_dict = {\"product\": rows_list, \"estimated_rating\": rows_list2}\n",
    "    cut_df = pd.DataFrame(cut_dict)\n",
    "    \n",
    "    return cut_df\n",
    "\n",
    "# Test 1\n",
    "product_mapping(find_product)\n",
    "\n",
    "# Test 2\n",
    "product_mapping(hybrid_recommendations('Peter ', 'B077T4MVZ6'))\n",
    "\n",
    "# Test 3\n",
    "product_mapping(hybrid_recommendations('Sarah ', 'B081H6STQQ'))\n",
    "\n",
    "\n",
    "\n",
    "# Save the model into a joblib file\n",
    "from joblib import dump, load\n",
    "\n",
    "dump(svd, 'svd.joblib')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compute the accuracy of models\n",
    "from surprise import accuracy\n",
    "\n",
    "# Class is used to parse a file containing ratings, data should be in structure - user ; item ; rating\n",
    "from surprise.reader import Reader\n",
    "\n",
    "# Class for loading datasets\n",
    "from surprise.dataset import Dataset\n",
    "\n",
    "# For tuning model hyperparameters\n",
    "from surprise.model_selection import GridSearchCV\n",
    "\n",
    "# For splitting the rating data in train and test datasets\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "# For implementing similarity-based recommendation system\n",
    "from surprise.prediction_algorithms.knns import KNNBasic\n",
    "\n",
    "# For implementing matrix factorization based recommendation system\n",
    "from surprise.prediction_algorithms.matrix_factorization import SVD\n",
    "\n",
    "# for implementing K-Fold cross-validation\n",
    "from surprise.model_selection import KFold\n",
    "\n",
    "# For implementing clustering-based recommendation system\n",
    "from surprise import CoClustering\n",
    "\n",
    "\n",
    "def precision_recall_at_k(model, k = 10, threshold = 3.5):\n",
    "    \"\"\"Return precision and recall at k metrics for each user\"\"\"\n",
    "\n",
    "    # First map the predictions to each user\n",
    "    user_est_true = defaultdict(list)\n",
    "\n",
    "    # Making predictions on the test data\n",
    "    predictions = model.test(testset)\n",
    "\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key = lambda x: x[0], reverse = True)\n",
    "\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n",
    "                              for (est, true_r) in user_ratings[:k])\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        # When n_rec_k is 0, Precision is undefined. Therefore, we are setting Precision to 0 when n_rec_k is 0\n",
    "\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        # When n_rel is 0, Recall is undefined. Therefore, we are setting Recall to 0 when n_rel is 0\n",
    "\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
    "\n",
    "    # Mean of all the predicted precisions are calculated.\n",
    "    precision = round((sum(prec for prec in precisions.values()) / len(precisions)), 3)\n",
    "\n",
    "    # Mean of all the predicted recalls are calculated.\n",
    "    recall = round((sum(rec for rec in recalls.values()) / len(recalls)), 3)\n",
    "\n",
    "    accuracy.rmse(predictions)\n",
    "\n",
    "    print('Precision: ', precision) # Command to print the overall precision\n",
    "\n",
    "    print('Recall: ', recall) # Command to print the overall recall\n",
    "\n",
    "    print('F_1 score: ', round((2*precision*recall)/(precision+recall), 3)) # Formula to compute the F-1 score\n",
    "\n",
    "\n",
    "\n",
    "# Instantiating Reader scale with expected rating scale\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "# Loading the rating dataset\n",
    "df = Dataset.load_from_df(df[['user_id', 'prod_id', 'rating']], reader)\n",
    "\n",
    "# Splitting the data into train and test datasets\n",
    "trainset, testset = train_test_split(df, test_size=0.7, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Declaring the similarity options\n",
    "sim_options = {'name': 'cosine', 'user_based': True}\n",
    "\n",
    "# Initialize the KNNBasic model using sim_options declared, Verbose = False, and setting random_state = 1\n",
    "algo_knn_user = KNNBasic(sim_options=sim_options, verbose=False, random_state=1)\n",
    "\n",
    "# Fit the model on the training data\n",
    "algo_knn_user.fit(trainset)\n",
    "\n",
    "# Let us compute precision@k, recall@k, and f_1 score using the precision_recall_at_k function defined above\n",
    "precision_recall_at_k(algo_knn_user)\n",
    "\n",
    "\n",
    "# Predicting rating for a sample user with an interacted product\n",
    "algo_knn_user.predict('A3LDPF5FMB782Z', '1400501466', r_ui=5, verbose=True)\n",
    "\n",
    "\n",
    "# Unique user_id where prod_id is not equal to \"1400501466\"\n",
    "df_final.loc[df_final['prod_id'] != \"1400501466\", 'user_id'].unique()\n",
    "\n",
    "\n",
    "# Predicting rating for a sample user with a non interacted product\n",
    "algo_knn_user.predict('A34BZM6S9L7QI4', '1400501466', verbose=True)\n",
    "\n",
    "# Setting up parameter grid to tune the hyperparameters\n",
    "param_grid = {'k': [20, 30, 40], 'min_k': [3, 6, 9],\n",
    "              'sim_options': {'name': ['msd', 'cosine', 'pearson'],\n",
    "                              'user_based': [True]}\n",
    "              }\n",
    "\n",
    "# Performing 3-fold cross-validation to tune the hyperparameters\n",
    "gs = GridSearchCV(KNNBasic, param_grid, measures=['rmse'], cv=3, n_jobs=-1)\n",
    "\n",
    "# Fitting the data\n",
    "gs.fit(df)\n",
    "\n",
    "# Best RMSE score\n",
    "print(gs.best_score['rmse'])\n",
    "\n",
    "# Combination of parameters that gave the best RMSE score\n",
    "print(gs.best_params['rmse'])\n",
    "\n",
    "\n",
    "\n",
    "# Using the optimal similarity measure for user-user based collaborative filtering\n",
    "sim_options = {'name': 'cosine',\n",
    "               'user_based': True}\n",
    "\n",
    "# Creating an instance of KNNBasic with optimal hyperparameter values\n",
    "similarity_algo_optimized = KNNBasic(sim_options=sim_options, k=40, min_k=6, verbose=False, random_state=1)\n",
    "\n",
    "# Training the algorithm on the trainset\n",
    "similarity_algo_optimized.fit(trainset)\n",
    "\n",
    "# Let us compute precision@k and recall@k also with k =10\n",
    "precision_recall_at_k(similarity_algo_optimized)\n",
    "\n",
    "# sim_user_user_optimized model to recommend for userId \"A3LDPF5FMB782Z\" and productId 1400501466\n",
    "similarity_algo_optimized.predict('A3LDPF5FMB782Z', '1400501466', r_ui=5, verbose=True)\n",
    "\n",
    "\n",
    "# sim_user_user_optimized model to recommend for userId \"A34BZM6S9L7QI4\" and productId \"1400501466\"\n",
    "similarity_algo_optimized.predict('A34BZM6S9L7QI4', '1400501466', verbose=True)\n",
    "\n",
    "\n",
    "# 0 is the inner id of the above user\n",
    "similarity_algo_optimized.get_neighbors(0, k=5)\n",
    "\n",
    "\n",
    "def get_recommendations(data, user_id, top_n, algo):\n",
    "\n",
    "    # Creating an empty list to store the recommended product ids\n",
    "    recommendations = []\n",
    "\n",
    "    # Creating an user item interactions matrix\n",
    "    user_item_interactions_matrix = data.pivot(index = 'user_id', columns = 'prod_id', values = 'rating')\n",
    "\n",
    "    # Extracting those product ids which the user_id has not interacted yet\n",
    "    non_interacted_products = user_item_interactions_matrix.loc[user_id][user_item_interactions_matrix.loc[user_id].isnull()].index.tolist()\n",
    "\n",
    "    # Looping through each of the product ids which user_id has not interacted yet\n",
    "    for item_id in non_interacted_products:\n",
    "\n",
    "        # Predicting the ratings for those non interacted product ids by this user\n",
    "        est = algo.predict(user_id, item_id).est\n",
    "\n",
    "        # Appending the predicted ratings\n",
    "        recommendations.append((item_id, est))\n",
    "\n",
    "    # Sorting the predicted ratings in descending order\n",
    "    recommendations.sort(key = lambda x: x[1], reverse = True)\n",
    "\n",
    "    return recommendations[:top_n] # Returing top n highest predicted rating products for this user\n",
    "\n",
    "\n",
    "# Making top 5 recommendations for user_id \"A3LDPF5FMB782Z\" with a similarity-based recommendation engine\n",
    "recommendations = get_recommendations(df_final, 'A3LDPF5FMB782Z', 5, algo_knn_user)\n",
    "\n",
    "\n",
    "# Building the dataframe for above recommendations with columns \"prod_id\" and \"predicted_ratings\"\n",
    "pd.DataFrame(recommendations, columns=['prod_Id', 'predicted_ratings'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from learntools.time_series.utils import plot_periodogram, seasonal_plot\n",
    "from learntools.time_series.style import *\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown, display\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "# Function to display markdown\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "# Define directory and read data\n",
    "comp_dir = Path('../input/amazon-product-reviews')\n",
    "electronics_data = pd.read_csv(\n",
    "    comp_dir / \"ratings_Electronics (1).csv\",\n",
    "    dtype={'rating': 'int8'},\n",
    "    names=['userId', 'productId', 'rating', 'timestamp'],\n",
    "    index_col=None,\n",
    "    header=0\n",
    ")\n",
    "\n",
    "# Display some basic information\n",
    "printmd(f\"Number of Rating: {electronics_data.shape[0]:,}\")\n",
    "printmd(f\"Columns: {np.array2string(electronics_data.columns.values)}\")\n",
    "printmd(f\"Number of Users: {len(electronics_data.userId.unique()):,}\")\n",
    "printmd(f\"Number of Products: {len(electronics_data.productId.unique()):,}\")\n",
    "electronics_data.describe()['rating'].reset_index()\n",
    "\n",
    "# Check for missing values\n",
    "printmd('**Number of missing values**:')\n",
    "pd.DataFrame(\n",
    "    electronics_data.isnull().sum().reset_index()\n",
    ").rename(columns={0: \"Total missing\", \"index\": \"Columns\"})\n",
    "\n",
    "# Process data by date\n",
    "data_by_date = electronics_data.copy()\n",
    "data_by_date.timestamp = pd.to_datetime(electronics_data.timestamp, unit=\"s\")\n",
    "data_by_date = data_by_date.sort_values(by=\"timestamp\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "printmd(\"Number of Ratings each day:\")\n",
    "data_by_date.groupby(\"timestamp\")[\"rating\"].count().tail(10).reset_index()\n",
    "\n",
    "# Add year and month columns\n",
    "data_by_date[\"year\"] = data_by_date.timestamp.dt.year\n",
    "data_by_date[\"month\"] = data_by_date.timestamp.dt.month\n",
    "rating_by_year = data_by_date.groupby([\"year\", \"month\"])[\"rating\"].count().reset_index()\n",
    "\n",
    "# Create date column and plot data\n",
    "rating_by_year[\"date\"] = pd.to_datetime(rating_by_year[\"year\"].astype(str) + \"-\" + rating_by_year[\"month\"].astype(str) + \"-1\")\n",
    "rating_by_year.plot(x=\"date\", y=\"rating\")\n",
    "plt.title(\"Number of Ratings over Years\")\n",
    "plt.show()\n",
    "\n",
    "# Group by product and calculate statistics\n",
    "rating_by_product = electronics_data.groupby(\"productId\").agg({\n",
    "    \"userId\": \"count\",\n",
    "    \"rating\": \"mean\"\n",
    "}).rename(columns={\"userId\": \"Number of Ratings\", \"rating\": \"Average Rating\"}).reset_index()\n",
    "\n",
    "# Filter top-rated products\n",
    "cutoff = 50\n",
    "top_rated = rating_by_product.loc[\n",
    "    rating_by_product[\"Number of Ratings\"] > cutoff\n",
    "].sort_values(by=\"Average Rating\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Define TensorFlow model\n",
    "class RankingModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        embedding_dimension = 32\n",
    "\n",
    "        # Embedding layers for users and products\n",
    "        self.user_embeddings = tf.keras.Sequential([\n",
    "            tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=unique_userIds, mask_token=None),\n",
    "            tf.keras.layers.Embedding(len(unique_userIds) + 1, embedding_dimension)\n",
    "        ])\n",
    "\n",
    "        self.product_embeddings = tf.keras.Sequential([\n",
    "            tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=unique_productIds, mask_token=None),\n",
    "            tf.keras.layers.Embedding(len(unique_productIds) + 1, embedding_dimension)\n",
    "        ])\n",
    "\n",
    "        # Ratings layers\n",
    "        self.ratings = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "\n",
    "    def call(self, userId, productId):\n",
    "        user_embeddings = self.user_embeddings(userId)\n",
    "        product_embeddings = self.product_embeddings(productId)\n",
    "        return self.ratings(tf.concat([user_embeddings, product_embeddings], axis=1))\n",
    "\n",
    "class amazonModel(tfrs.models.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ranking_model = RankingModel()\n",
    "        self.task = tfrs.tasks.Ranking(\n",
    "            loss=tf.keras.losses.MeanSquaredError(),\n",
    "            metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, features, training=False):\n",
    "        rating_predictions = self.ranking_model(features[\"userId\"], features[\"productId\"])\n",
    "        return self.task(labels=features[\"rating\"], predictions=rating_predictions)\n",
    "\n",
    "# Filter recent data\n",
    "cutoff_no_rat = 50\n",
    "cutoff_year = 2011\n",
    "recent_data = data_by_date.loc[data_by_date[\"year\"] > cutoff_year]\n",
    "print(f\"Number of Rating: {recent_data.shape[0]:,}\")\n",
    "print(f\"Number of Users: {len(recent_data.userId.unique()):,}\")\n",
    "print(f\"Number of Products: {len(recent_data.productId.unique()):,}\")\n",
    "del data_by_date  # Free up memory\n",
    "\n",
    "recent_prod = recent_data.loc[\n",
    "    recent_data.groupby(\"productId\")[\"rating\"].transform('count').ge(cutoff_no_rat)\n",
    "].reset_index(drop=True).drop([\"timestamp\", \"year\", \"month\"], axis=1)\n",
    "del recent_data  # Free up memory\n",
    "\n",
    "# Prepare data for training\n",
    "userIds = recent_prod.userId.unique()\n",
    "productIds = recent_prod.productId.unique()\n",
    "total_ratings = len(recent_prod.index)\n",
    "\n",
    "ratings = tf.data.Dataset.from_tensor_slices({\n",
    "    \"userId\": tf.cast(recent_prod.userId.values, tf.string),\n",
    "    \"productId\": tf.cast(recent_prod.productId.values, tf.string),\n",
    "    \"rating\": tf.cast(recent_prod.rating.values, tf.int8)\n",
    "})\n",
    "\n",
    "# Shuffle and split data\n",
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "train = shuffled.take(int(total_ratings * 0.8))\n",
    "test = shuffled.skip(int(total_ratings * 0.8)).take(int(total_ratings * 0.2))\n",
    "\n",
    "unique_productIds = productIds\n",
    "unique_userIds = userIds\n",
    "\n",
    "# Compile and train model\n",
    "model = amazonModel()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n",
    "\n",
    "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
    "cached_test = test.batch(4096).cache()\n",
    "model.fit(cached_train, epochs=10)\n",
    "\n",
    "# Evaluate model\n",
    "model.evaluate(cached_test, return_dict=True)\n",
    "\n",
    "# Recommend products for a random user\n",
    "user_rand = userIds[123]\n",
    "test_rating = {}\n",
    "for m in test.take(5):\n",
    "    test_rating[m[\"productId\"].numpy()] = RankingModel()(\n",
    "        tf.convert_to_tensor([user_rand]), tf.convert_to_tensor([m[\"productId\"]])\n",
    "    )\n",
    "\n",
    "print(f\"Top 5 recommended products for User {user_rand}:\")\n",
    "for m in sorted(test_rating, key=test_rating.get, reverse=True):\n",
    "    print(m.decode())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogo['complexity'] = catalogo[columnas_a_transformar].apply(lambda x: int(''.join(map(str, x)), 2), axis=1)\n",
    "catalogo['complexity'] = catalogo[columnas_a_transformar].apply(lambda x: int(''.join(map(str, x)), 2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recomendadores de TensorFlow para un sistema de recomendaciones potente\n",
    "- [Articulo](https://medium.com/@pauloyc/tensorflow-recommenders-for-powerful-recommendation-system-e3dec138a07f)\n",
    "\n",
    "\n",
    "# TensorFlow Recommenders for powerful recommendation system\n",
    "- [Articulo](https://medium.com/@pauloyc/tensorflow-recommenders-for-powerful-recommendation-system-e3dec138a07f)\n",
    "\n",
    "\n",
    "# Construyendo un sistema de recomendación de películas con Surprise y Python.\n",
    "- [Articulo](https://monirah-abdulaziz.medium.com/building-movie-recommendation-system-with-surprise-and-python-e905de755c61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_series_duration = merged_df_series.groupby(‘program_name’)[[‘duration_seconds’]].max()\n",
    "\n",
    "# # average_watching\n",
    "# merged_df_series[“average_watching”] = merged_df_series.apply(lambda x: 1 if x[‘duration_seconds’] > df_series_duration.loc[x.program_name,’duration_seconds’] else x[‘duration_seconds’]/df_series_duration.loc[x.program_name,’duration_seconds’], axis=1)\n",
    "# # total_duration \n",
    "# merged_df_series[“total_duration”]= merged_df_series.apply(lambda x: df_series_duration.loc[x.program_name,’duration_seconds’],axis=1)\n",
    "\n",
    "\n",
    "# # Extract minute by using (regex) and convert to appropriate type \n",
    "# merged_df_movie[‘total_duration’] = merged_df_movie[‘duration’].str.replace(r’min’, ‘’)\n",
    "# merged_df_movie[‘duration_seconds’] = pd.to_numeric((merged_df_movie[‘duration_seconds’]) , errors=’coerce’).astype(‘Int64’)\n",
    "# merged_df_movie[‘total_duration’] = pd.to_numeric((merged_df_movie[‘total_duration’]) , errors=’coerce’).astype(‘Int64’)\n",
    "\n",
    "# # convert from min to sec\n",
    "# merged_df_movie[‘total_duration’] = (merged_df_movie[‘total_duration’]*60)\n",
    "\n",
    "# merged_df_movie[“duration_seconds”] = merged_df_movie.apply(lambda x: x[‘total_duration’] if x[‘duration_seconds’] > x[‘total_duration’] else x[‘duration_seconds’], axis=1)\n",
    "\n",
    "# merged_df_movie[‘average_watching’]=merged_df_movie[‘duration_seconds’]/merged_df_movie[‘total_duration’]\n",
    "\n",
    "# reader = Reader(rating_scale=(0.03, 1.0))\n",
    "# data = Dataset.load_from_df(df_data[[‘user_id’, ‘show_id’, ‘average_watching’]], reader)\n",
    "# benchmark = []\n",
    "\n",
    "# # Iterate over all algorithms\n",
    "# for algorithm in [SVD(), SVDpp(), SlopeOne(), NMF(), NormalPredictor(), KNNBaseline(), KNNBasic(),\n",
    "#  KNNWithMeans(), KNNWithZScore(), BaselineOnly()]:\n",
    "#  # Perform cross validation\n",
    "#  results = cross_validate(algorithm, data, measures=[‘RMSE’], cv=3, verbose=False)\n",
    " \n",
    "#  # Get results & append algorithm name\n",
    "#  tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n",
    "#  tmp = tmp.append(pd.Series([str(algorithm).split(‘ ‘)[0].split(‘.’)[-1]], index=[‘Algorithm’]))\n",
    "#  benchmark.append(tmp)\n",
    "\n",
    "\n",
    "# print(‘Using ALS’)\n",
    "# bsl_options = {‘method’: ‘als’,\n",
    "#  “random_state”:250,\n",
    "#  ‘n_epochs’: 5,\n",
    "#  ‘reg_u’: 12,\n",
    "#  ‘reg_i’: 5\n",
    "#  }\n",
    "# algo = BaselineOnly(bsl_options)\n",
    "# cross_validate(algo, data, measures=[‘RMSE’], cv=3, verbose=False)\n",
    "\n",
    "\n",
    "# trainset, testset = train_test_split(data, test_size=0.25)\n",
    "# algo = BaselineOnly(bsl_options)\n",
    "# predictions = algo.fit(trainset).test(testset)\n",
    "# accuracy.rmse(predictions)\n",
    "\n",
    "\n",
    "# # user_id is the 13618\n",
    "# ratings = newdf.loc[newdf[‘user_id’] == 13618]\n",
    "# # obtain the required data of this user\n",
    "# ratings=ratings[[‘user_id’, ‘show_id’, ‘average_watching’]]\n",
    "# ratings\n",
    "\n",
    "# # user_id is the 13618\n",
    "# ratings = newdf.loc[newdf[‘user_id’] == 13618]\n",
    "# # obtain the required data of this user\n",
    "# ratings=ratings[[‘user_id’, ‘show_id’, ‘average_watching’]]\n",
    "# ratings\n",
    "\n",
    "# # get the list of the movie ids\n",
    "# unique_ids = newdf[‘show_id’].unique()\n",
    "# # get the list of the ids that the userid 13618 has watched\n",
    "# iids1001 = newdf.loc[newdf[‘user_id’]==13618, ‘show_id’]\n",
    "# # remove the rated movies for the recommendations\n",
    "# movies_to_predict = np.setdiff1d(unique_ids,iids1001)\n",
    "\n",
    "# algo = BaselineOnly(bsl_options)\n",
    "# algo.fit(data.build_full_trainset())\n",
    "# my_recs = []\n",
    "# for iid in movies_to_predict:\n",
    "#  my_recs.append((iid, algo.predict(uid=’13618',iid=iid).est))\n",
    "# pd.DataFrame(my_recs, columns=[‘iid’, ‘predictions’]).sort_values(‘predictions’, ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Magic of Recommendation Systems: How Netflix Knows What You Want to Watch\n",
    "- [Articulo](https://www.linkedin.com/pulse/magic-recommendation-systems-how-netflix-knows-what-you-sachin-b9gzc/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sample code for user-based collaborative filtering\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # Calculate user similarity matrix\n",
    "# user_similarity = cosine_similarity(user_ratings)\n",
    "\n",
    "# # Predict a user's ratings based on similar users\n",
    "# def predict_ratings(user_ratings, user_similarity):\n",
    "#     return user_similarity.dot(user_ratings) / np.abs(user_similarity).sum(axis=1) \n",
    "\n",
    "\n",
    "# # Sample code for item-based collaborative filtering\n",
    "# item_similarity = cosine_similarity(item_ratings.T)\n",
    "\n",
    "# # Predict ratings for an item based on similar items\n",
    "# def predict_item_ratings(item_ratings, item_similarity):\n",
    "#     return item_ratings.dot(item_similarity) / np.abs(item_similarity).sum(axis=1) \n",
    "\n",
    "\n",
    "# # Sample code for content-based filtering\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# # Vectorize item descriptions using TF-IDF\n",
    "# tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "# tfidf_matrix = tfidf_vectorizer.fit_transform(item_descriptions)\n",
    "\n",
    "# # Calculate similarity between items based on descriptions\n",
    "# item_similarity_content = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "\n",
    "# # Sample code for a hybrid recommendation model\n",
    "# hybrid_similarity = alpha * user_similarity + (1 - alpha) * item_similarity_content\n",
    "\n",
    "# # Predict ratings using the hybrid similarity matrix\n",
    "# def predict_hybrid_ratings(user_ratings, hybrid_similarity):\n",
    "#     return hybrid_similarity.dot(user_ratings) / np.abs(hybrid_similarity).sum(axis=1) \n",
    "\n",
    "\n",
    "\n",
    "# # Sample code for a neural collaborative filtering model\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Dense\n",
    "\n",
    "# # Define neural collaborative filtering model\n",
    "# user_input = Input(shape=(1,))\n",
    "# item_input = Input(shape=(1,))\n",
    "# user_embedding = Embedding(num_users, embedding_size)(user_input)\n",
    "# item_embedding = Embedding(num_items, embedding_size)(item_input)\n",
    "# merged_embeddings = Concatenate()([user_embedding, item_embedding])\n",
    "# flatten = Flatten()(merged_embeddings)\n",
    "# dense_layer = Dense(128, activation='relu')(flatten)\n",
    "# output_layer = Dense(1)(dense_layer)\n",
    "\n",
    "# neural_collab_filter = Model(inputs=[user_input, item_input], outputs=output_layer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Testing Recommender Systems With Surprise, Step-By-Step\n",
    "- [Articulo](https://towardsdatascience.com/building-and-testing-recommender-systems-with-surprise-step-by-step-d4ba702ef80b)\n",
    "- [Notebook](https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Building%20Recommender%20System%20with%20Surprise.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user = pd.read_csv('BX-Users.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n",
    "# user.columns = ['userID', 'Location', 'Age']\n",
    "# rating = pd.read_csv('BX-Book-Ratings.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n",
    "# rating.columns = ['userID', 'ISBN', 'bookRating']\n",
    "# df = pd.merge(user, rating, on='userID', how='inner')\n",
    "# df.drop(['Location', 'Age'], axis=1, inplace=True)\n",
    "# df.head()\n",
    "\n",
    "\n",
    "\n",
    "# from plotly.offline import init_notebook_mode, plot, iplot\n",
    "# import plotly.graph_objs as go\n",
    "# init_notebook_mode(connected=True)\n",
    "\n",
    "# data = df['bookRating'].value_counts().sort_index(ascending=False)\n",
    "# trace = go.Bar(x = data.index,\n",
    "#                text = ['{:.1f} %'.format(val) for val in (data.values / df.shape[0] * 100)],\n",
    "#                textposition = 'auto',\n",
    "#                textfont = dict(color = '#000000'),\n",
    "#                y = data.values,\n",
    "#                )\n",
    "# # Create layout\n",
    "# layout = dict(title = 'Distribution Of {} book-ratings'.format(df.shape[0]),\n",
    "#               xaxis = dict(title = 'Rating'),\n",
    "#               yaxis = dict(title = 'Count'))\n",
    "# # Create plot\n",
    "# fig = go.Figure(data=[trace], layout=layout)\n",
    "# iplot(fig)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Number of ratings per book\n",
    "# data = df.groupby('ISBN')['bookRating'].count().clip(upper=50)\n",
    "\n",
    "# # Create trace\n",
    "# trace = go.Histogram(x = data.values,\n",
    "#                      name = 'Ratings',\n",
    "#                      xbins = dict(start = 0,\n",
    "#                                   end = 50,\n",
    "#                                   size = 2))\n",
    "# # Create layout\n",
    "# layout = go.Layout(title = 'Distribution Of Number of Ratings Per Book (Clipped at 100)',\n",
    "#                    xaxis = dict(title = 'Number of Ratings Per Book'),\n",
    "#                    yaxis = dict(title = 'Count'),\n",
    "#                    bargap = 0.2)\n",
    "\n",
    "# # Create plot\n",
    "# fig = go.Figure(data=[trace], layout=layout)\n",
    "# iplot(fig)\n",
    "\n",
    "\n",
    "\n",
    "# df.groupby('ISBN')['bookRating'].count().reset_index().sort_values('bookRating', ascending=False)[:10]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Number of ratings per user\n",
    "# data = df.groupby('userID')['bookRating'].count().clip(upper=50)\n",
    "\n",
    "# # Create trace\n",
    "# trace = go.Histogram(x = data.values,\n",
    "#                      name = 'Ratings',\n",
    "#                      xbins = dict(start = 0,\n",
    "#                                   end = 50,\n",
    "#                                   size = 2))\n",
    "# # Create layout\n",
    "# layout = go.Layout(title = 'Distribution Of Number of Ratings Per User (Clipped at 50)',\n",
    "#                    xaxis = dict(title = 'Ratings Per User'),\n",
    "#                    yaxis = dict(title = 'Count'),\n",
    "#                    bargap = 0.2)\n",
    "\n",
    "# # Create plot\n",
    "# fig = go.Figure(data=[trace], layout=layout)\n",
    "# iplot(fig)\n",
    "\n",
    "\n",
    "\n",
    "# df.groupby('userID')['bookRating'].count().reset_index().sort_values('bookRating', ascending=False)[:10]\n",
    "\n",
    "\n",
    "\n",
    "# min_book_ratings = 50\n",
    "# filter_books = df['ISBN'].value_counts() > min_book_ratings\n",
    "# filter_books = filter_books[filter_books].index.tolist()\n",
    "\n",
    "# min_user_ratings = 50\n",
    "# filter_users = df['userID'].value_counts() > min_user_ratings\n",
    "# filter_users = filter_users[filter_users].index.tolist()\n",
    "\n",
    "# df_new = df[(df['ISBN'].isin(filter_books)) & (df['userID'].isin(filter_users))]\n",
    "# print('The original data frame shape:\\t{}'.format(df.shape))\n",
    "# print('The new data frame shape:\\t{}'.format(df_new.shape))\n",
    "\n",
    "\n",
    "\n",
    "# reader = Reader(rating_scale=(0, 9))\n",
    "# data = Dataset.load_from_df(df_new[['userID', 'ISBN', 'bookRating']], reader)\n",
    "\n",
    "\n",
    "\n",
    "# benchmark = []\n",
    "# # Iterate over all algorithms\n",
    "# for algorithm in [SVD(), SVDpp(), SlopeOne(), NMF(), NormalPredictor(), KNNBaseline(), KNNBasic(), KNNWithMeans(), KNNWithZScore(), BaselineOnly(), CoClustering()]:\n",
    "#     # Perform cross validation\n",
    "#     results = cross_validate(algorithm, data, measures=['RMSE'], cv=3, verbose=False)\n",
    "    \n",
    "#     # Get results & append algorithm name\n",
    "#     tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n",
    "#     tmp = tmp.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm']))\n",
    "#     benchmark.append(tmp)\n",
    "    \n",
    "# pd.DataFrame(benchmark).set_index('Algorithm').sort_values('test_rmse')    \n",
    "\n",
    "\n",
    "\n",
    "# print('Using ALS')\n",
    "# bsl_options = {'method': 'als',\n",
    "#                'n_epochs': 5,\n",
    "#                'reg_u': 12,\n",
    "#                'reg_i': 5\n",
    "#                }\n",
    "# algo = BaselineOnly(bsl_options=bsl_options)\n",
    "# cross_validate(algo, data, measures=['RMSE'], cv=3, verbose=False)  \n",
    "\n",
    "\n",
    "\n",
    "# trainset, testset = train_test_split(data, test_size=0.25)\n",
    "# algo = BaselineOnly(bsl_options=bsl_options)\n",
    "# predictions = algo.fit(trainset).test(testset)\n",
    "# accuracy.rmse(predictions)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def get_Iu(uid):\n",
    "#     \"\"\" return the number of items rated by given user\n",
    "#     args: \n",
    "#       uid: the id of the user\n",
    "#     returns: \n",
    "#       the number of items rated by the user\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         return len(trainset.ur[trainset.to_inner_uid(uid)])\n",
    "#     except ValueError: # user was not part of the trainset\n",
    "#         return 0\n",
    "    \n",
    "# def get_Ui(iid):\n",
    "#     \"\"\" return number of users that have rated given item\n",
    "#     args:\n",
    "#       iid: the raw id of the item\n",
    "#     returns:\n",
    "#       the number of users that have rated the item.\n",
    "#     \"\"\"\n",
    "#     try: \n",
    "#         return len(trainset.ir[trainset.to_inner_iid(iid)])\n",
    "#     except ValueError:\n",
    "#         return 0\n",
    "    \n",
    "# df = pd.DataFrame(predictions, columns=['uid', 'iid', 'rui', 'est', 'details'])\n",
    "# df['Iu'] = df.uid.apply(get_Iu)\n",
    "# df['Ui'] = df.iid.apply(get_Ui)\n",
    "# df['err'] = abs(df.est - df.rui)\n",
    "# best_predictions = df.sort_values(by='err')[:10]\n",
    "# worst_predictions = df.sort_values(by='err')[-10:]\n",
    "\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib notebook\n",
    "# df_new.loc[df_new['ISBN'] == '055358264X']['bookRating'].hist()\n",
    "# plt.xlabel('rating')\n",
    "# plt.ylabel('Number of ratings')\n",
    "# plt.title('Number of ratings book ISBN 055358264X has received')\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import tensorflow as tf\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# rating = pd.read_csv('data/BX-Book-Ratings.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n",
    "# user = pd.read_csv('data/BX-Users.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n",
    "# book = pd.read_csv('data/BX-Books.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n",
    "# book_rating = pd.merge(rating, book, on='ISBN')\n",
    "# cols = ['Year-Of-Publication', 'Publisher', 'Book-Author', 'Image-URL-S', 'Image-URL-M', 'Image-URL-L']\n",
    "# book_rating.drop(cols, axis=1, inplace=True)\n",
    "\n",
    "# rating_count = (book_rating.\n",
    "#      groupby(by = ['Book-Title'])['Book-Rating'].\n",
    "#      count().\n",
    "#      reset_index().\n",
    "#      rename(columns = {'Book-Rating': 'RatingCount_book'})\n",
    "#      [['Book-Title', 'RatingCount_book']]\n",
    "#     )\n",
    "    \n",
    "# threshold = 25\n",
    "# rating_count = rating_count.query('RatingCount_book >= @threshold')\n",
    "\n",
    "# user_rating = pd.merge(rating_count, book_rating, left_on='Book-Title', right_on='Book-Title', how='left')\n",
    "\n",
    "# user_count = (user_rating.\n",
    "#      groupby(by = ['User-ID'])['Book-Rating'].\n",
    "#      count().\n",
    "#      reset_index().\n",
    "#      rename(columns = {'Book-Rating': 'RatingCount_user'})\n",
    "#      [['User-ID', 'RatingCount_user']]\n",
    "#     )\n",
    "    \n",
    "# threshold = 20\n",
    "# user_count = user_count.query('RatingCount_user >= @threshold')\n",
    "\n",
    "# combined = user_rating.merge(user_count, left_on = 'User-ID', right_on = 'User-ID', how = 'inner')\n",
    "\n",
    "# print('Number of unique books: ', combined['Book-Title'].nunique())\n",
    "# print('Number of unique users: ', combined['User-ID'].nunique())\n",
    "\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# combined['Book-Rating'] = combined['Book-Rating'].values.astype(float)\n",
    "# rating_scaled = pd.DataFrame(scaler.fit_transform(combined['Book-Rating'].values.reshape(-1,1)))\n",
    "# combined['Book-Rating'] = rating_scaled\n",
    "\n",
    "\n",
    "# combined = combined.drop_duplicates(['User-ID', 'Book-Title'])\n",
    "# user_book_matrix = combined.pivot(index='User-ID', columns='Book-Title', values='Book-Rating')\n",
    "# user_book_matrix.fillna(0, inplace=True)\n",
    "# users = user_book_matrix.index.tolist()\n",
    "# books = user_book_matrix.columns.tolist()\n",
    "# user_book_matrix = user_book_matrix.as_matrix()\n",
    "\n",
    "\n",
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "# num_input = combined['Book-Title'].nunique()\n",
    "# num_hidden_1 = 10\n",
    "# num_hidden_2 = 5\n",
    "\n",
    "# X = tf.placeholder(tf.float64, [None, num_input])\n",
    "\n",
    "# weights = {\n",
    "#     'encoder_h1': tf.Variable(tf.random_normal([num_input, num_hidden_1], dtype=tf.float64)),\n",
    "#     'encoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_hidden_2], dtype=tf.float64)),\n",
    "#     'decoder_h1': tf.Variable(tf.random_normal([num_hidden_2, num_hidden_1], dtype=tf.float64)),\n",
    "#     'decoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_input], dtype=tf.float64)),\n",
    "# }\n",
    "\n",
    "# biases = {\n",
    "#     'encoder_b1': tf.Variable(tf.random_normal([num_hidden_1], dtype=tf.float64)),\n",
    "#     'encoder_b2': tf.Variable(tf.random_normal([num_hidden_2], dtype=tf.float64)),\n",
    "#     'decoder_b1': tf.Variable(tf.random_normal([num_hidden_1], dtype=tf.float64)),\n",
    "#     'decoder_b2': tf.Variable(tf.random_normal([num_input], dtype=tf.float64)),\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "# def encoder(x):\n",
    "#     layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']), biases['encoder_b1']))\n",
    "#     layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']), biases['encoder_b2']))\n",
    "#     return layer_2\n",
    "\n",
    "# def decoder(x):\n",
    "#     layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']), biases['decoder_b1']))\n",
    "#     layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']), biases['decoder_b2']))\n",
    "#     return layer_2\n",
    "\n",
    "\n",
    "# encoder_op = encoder(X)\n",
    "# decoder_op = decoder(encoder_op)\n",
    "# y_pred = decoder_op\n",
    "# y_true = X\n",
    "\n",
    "\n",
    "# loss = tf.losses.mean_squared_error(y_true, y_pred)\n",
    "# optimizer = tf.train.RMSPropOptimizer(0.03).minimize(loss)\n",
    "# eval_x = tf.placeholder(tf.int32, )\n",
    "# eval_y = tf.placeholder(tf.int32, )\n",
    "# pre, pre_op = tf.metrics.precision(labels=eval_x, predictions=eval_y)\n",
    "\n",
    "# init = tf.global_variables_initializer()\n",
    "# local_init = tf.local_variables_initializer()\n",
    "# pred_data = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# with tf.Session() as session:\n",
    "#     epochs = 100\n",
    "#     batch_size = 35\n",
    "\n",
    "#     session.run(init)\n",
    "#     session.run(local_init)\n",
    "\n",
    "#     num_batches = int(user_book_matrix.shape[0] / batch_size)\n",
    "#     user_book_matrix = np.array_split(user_book_matrix, num_batches)\n",
    "    \n",
    "#     for i in range(epochs):\n",
    "\n",
    "#         avg_cost = 0\n",
    "#         for batch in user_book_matrix:\n",
    "#             _, l = session.run([optimizer, loss], feed_dict={X: batch})\n",
    "#             avg_cost += l\n",
    "\n",
    "#         avg_cost /= num_batches\n",
    "\n",
    "#         print(\"epoch: {} Loss: {}\".format(i + 1, avg_cost))\n",
    "\n",
    "#     user_book_matrix = np.concatenate(user_book_matrix, axis=0)\n",
    "\n",
    "#     preds = session.run(decoder_op, feed_dict={X: user_book_matrix})\n",
    "\n",
    "#     pred_data = pred_data.append(pd.DataFrame(preds))\n",
    "\n",
    "#     pred_data = pred_data.stack().reset_index(name='Book-Rating')\n",
    "#     pred_data.columns = ['User-ID', 'Book-Title', 'Book-Rating']\n",
    "#     pred_data['User-ID'] = pred_data['User-ID'].map(lambda value: users[value])\n",
    "#     pred_data['Book-Title'] = pred_data['Book-Title'].map(lambda value: books[value])\n",
    "    \n",
    "#     keys = ['User-ID', 'Book-Title']\n",
    "#     index_1 = pred_data.set_index(keys).index\n",
    "#     index_2 = combined.set_index(keys).index\n",
    "\n",
    "#     top_ten_ranked = pred_data[~index_1.isin(index_2)]\n",
    "#     top_ten_ranked = top_ten_ranked.sort_values(['User-ID', 'Book-Rating'], ascending=[True, False])\n",
    "#     top_ten_ranked = top_ten_ranked.groupby('User-ID').head(10)\n",
    "\n",
    "\n",
    "# top_ten_ranked.loc[top_ten_ranked['User-ID'] == 278582]\n",
    "\n",
    "# book_rating.loc[book_rating['User-ID'] == 278582].sort_values(by=['Book-Rating'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building A Collaborative Filtering Recommender System with TensorFlow\n",
    "- [Building A Collaborative Filtering Recommender System with TensorFlow](https://towardsdatascience.com/building-a-collaborative-filtering-recommender-system-with-tensorflow-82e63d27b420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import tensorflow as tf\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# rating = pd.read_csv('data/BX-Book-Ratings.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n",
    "# user = pd.read_csv('data/BX-Users.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n",
    "# book = pd.read_csv('data/BX-Books.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n",
    "# book_rating = pd.merge(rating, book, on='ISBN')\n",
    "# cols = ['Year-Of-Publication', 'Publisher', 'Book-Author', 'Image-URL-S', 'Image-URL-M', 'Image-URL-L']\n",
    "# book_rating.drop(cols, axis=1, inplace=True)\n",
    "\n",
    "# rating_count = (book_rating.\n",
    "#      groupby(by = ['Book-Title'])['Book-Rating'].\n",
    "#      count().\n",
    "#      reset_index().\n",
    "#      rename(columns = {'Book-Rating': 'RatingCount_book'})\n",
    "#      [['Book-Title', 'RatingCount_book']]\n",
    "#     )\n",
    "    \n",
    "# threshold = 25\n",
    "# rating_count = rating_count.query('RatingCount_book >= @threshold')\n",
    "\n",
    "# user_rating = pd.merge(rating_count, book_rating, left_on='Book-Title', right_on='Book-Title', how='left')\n",
    "\n",
    "# user_count = (user_rating.\n",
    "#      groupby(by = ['User-ID'])['Book-Rating'].\n",
    "#      count().\n",
    "#      reset_index().\n",
    "#      rename(columns = {'Book-Rating': 'RatingCount_user'})\n",
    "#      [['User-ID', 'RatingCount_user']]\n",
    "#     )\n",
    "    \n",
    "# threshold = 20\n",
    "# user_count = user_count.query('RatingCount_user >= @threshold')\n",
    "\n",
    "# combined = user_rating.merge(user_count, left_on = 'User-ID', right_on = 'User-ID', how = 'inner')\n",
    "\n",
    "# print('Number of unique books: ', combined['Book-Title'].nunique())\n",
    "# print('Number of unique users: ', combined['User-ID'].nunique())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# combined['Book-Rating'] = combined['Book-Rating'].values.astype(float)\n",
    "# rating_scaled = pd.DataFrame(scaler.fit_transform(combined['Book-Rating'].values.reshape(-1,1)))\n",
    "# combined['Book-Rating'] = rating_scaled\n",
    "\n",
    "\n",
    "# combined = combined.drop_duplicates(['User-ID', 'Book-Title'])\n",
    "# user_book_matrix = combined.pivot(index='User-ID', columns='Book-Title', values='Book-Rating')\n",
    "# user_book_matrix.fillna(0, inplace=True)\n",
    "# users = user_book_matrix.index.tolist()\n",
    "# books = user_book_matrix.columns.tolist()\n",
    "# user_book_matrix = user_book_matrix.as_matrix()\n",
    "\n",
    "\n",
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "\n",
    "# num_input = combined['Book-Title'].nunique()\n",
    "# num_hidden_1 = 10\n",
    "# num_hidden_2 = 5\n",
    "\n",
    "# X = tf.placeholder(tf.float64, [None, num_input])\n",
    "\n",
    "# weights = {\n",
    "#     'encoder_h1': tf.Variable(tf.random_normal([num_input, num_hidden_1], dtype=tf.float64)),\n",
    "#     'encoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_hidden_2], dtype=tf.float64)),\n",
    "#     'decoder_h1': tf.Variable(tf.random_normal([num_hidden_2, num_hidden_1], dtype=tf.float64)),\n",
    "#     'decoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_input], dtype=tf.float64)),\n",
    "# }\n",
    "\n",
    "# biases = {\n",
    "#     'encoder_b1': tf.Variable(tf.random_normal([num_hidden_1], dtype=tf.float64)),\n",
    "#     'encoder_b2': tf.Variable(tf.random_normal([num_hidden_2], dtype=tf.float64)),\n",
    "#     'decoder_b1': tf.Variable(tf.random_normal([num_hidden_1], dtype=tf.float64)),\n",
    "#     'decoder_b2': tf.Variable(tf.random_normal([num_input], dtype=tf.float64)),\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "# def encoder(x):\n",
    "#     layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']), biases['encoder_b1']))\n",
    "#     layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']), biases['encoder_b2']))\n",
    "#     return layer_2\n",
    "\n",
    "# def decoder(x):\n",
    "#     layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']), biases['decoder_b1']))\n",
    "#     layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']), biases['decoder_b2']))\n",
    "#     return layer_2\n",
    "\n",
    "\n",
    "# encoder_op = encoder(X)\n",
    "# decoder_op = decoder(encoder_op)\n",
    "# y_pred = decoder_op\n",
    "# y_true = X\n",
    "\n",
    "\n",
    "# loss = tf.losses.mean_squared_error(y_true, y_pred)\n",
    "# optimizer = tf.train.RMSPropOptimizer(0.03).minimize(loss)\n",
    "# eval_x = tf.placeholder(tf.int32, )\n",
    "# eval_y = tf.placeholder(tf.int32, )\n",
    "# pre, pre_op = tf.metrics.precision(labels=eval_x, predictions=eval_y)\n",
    "\n",
    "# init = tf.global_variables_initializer()\n",
    "# local_init = tf.local_variables_initializer()\n",
    "# pred_data = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# with tf.Session() as session:\n",
    "#     epochs = 100\n",
    "#     batch_size = 35\n",
    "\n",
    "#     session.run(init)\n",
    "#     session.run(local_init)\n",
    "\n",
    "#     num_batches = int(user_book_matrix.shape[0] / batch_size)\n",
    "#     user_book_matrix = np.array_split(user_book_matrix, num_batches)\n",
    "    \n",
    "#     for i in range(epochs):\n",
    "#         avg_cost = 0\n",
    "#         for batch in user_book_matrix:\n",
    "#             _, l = session.run([optimizer, loss], feed_dict={X: batch})\n",
    "#             avg_cost += l\n",
    "#         avg_cost /= num_batches\n",
    "\n",
    "#         print(\"epoch: {} Loss: {}\".format(i + 1, avg_cost))\n",
    "\n",
    "#     user_book_matrix = np.concatenate(user_book_matrix, axis=0)\n",
    "#     preds = session.run(decoder_op, feed_dict={X: user_book_matrix})\n",
    "#     pred_data = pred_data.append(pd.DataFrame(preds))\n",
    "\n",
    "#     pred_data = pred_data.stack().reset_index(name='Book-Rating')\n",
    "#     pred_data.columns = ['User-ID', 'Book-Title', 'Book-Rating']\n",
    "#     pred_data['User-ID'] = pred_data['User-ID'].map(lambda value: users[value])\n",
    "#     pred_data['Book-Title'] = pred_data['Book-Title'].map(lambda value: books[value])\n",
    "    \n",
    "#     keys = ['User-ID', 'Book-Title']\n",
    "#     index_1 = pred_data.set_index(keys).index\n",
    "#     index_2 = combined.set_index(keys).index\n",
    "\n",
    "#     top_ten_ranked = pred_data[~index_1.isin(index_2)]\n",
    "#     top_ten_ranked = top_ten_ranked.sort_values(['User-ID', 'Book-Rating'], ascending=[True, False])\n",
    "#     top_ten_ranked = top_ten_ranked.groupby('User-ID').head(10)\n",
    "\n",
    "\n",
    "\n",
    "# top_ten_ranked.loc[top_ten_ranked['User-ID'] == 278582]\n",
    "\n",
    "# book_rating.loc[book_rating['User-ID'] == 278582].sort_values(by=['Book-Rating'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Content Based Recommender System for Hotels in Seattle\n",
    "- [Articulo](https://towardsdatascience.com/building-a-content-based-recommender-system-for-hotels-in-seattle-d724f0a32070)\n",
    "> Por lo tanto, nuestro conjunto de datos final contiene 3192 usuarios para 5850 libros. Y cada usuario ha otorgado al menos 20 calificaciones y cada libro ha recibido al menos 25 calificaciones. Si no tienes una GPU, este sería un buen tamaño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from nltk.corpus import stopwords\n",
    "# from sklearn.metrics.pairwise import linear_kernel\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "# import re\n",
    "# import random\n",
    "# import plotly.graph_objs as go\n",
    "# import plotly.plotly as py\n",
    "# import cufflinks\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# import plotly.figure_factory as ff\n",
    "# from plotly.offline import iplot\n",
    "\n",
    "\n",
    "# pd.options.display.max_columns = 30\n",
    "# InteractiveShell.ast_node_interactivity = 'all'\n",
    "# cufflinks.go_offline()\n",
    "# cufflinks.set_config_file(world_readable=True, theme='solar')\n",
    "# df = pd.read_csv('Seattle_Hotels.csv', encoding=\"latin-1\")\n",
    "# df.head()\n",
    "# print('We have ', len(df), 'hotels in the data')\n",
    "\n",
    "\n",
    "\n",
    "# def print_description(index):\n",
    "#     example = df[df.index == index][['desc', 'name']].values[0]\n",
    "#     if len(example) > 0:\n",
    "#         print(example[0])\n",
    "#         print('Name:', example[1])\n",
    "\n",
    "\n",
    "# print_description(10)\n",
    "# print_description(100)\n",
    "\n",
    "\n",
    "\n",
    "# def get_top_n_words(corpus, n=None):\n",
    "#     vec = CountVectorizer().fit(corpus)\n",
    "#     bag_of_words = vec.transform(corpus)\n",
    "#     sum_words = bag_of_words.sum(axis=0) \n",
    "#     words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "#     words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "#     return words_freq[:n]\n",
    "# common_words = get_top_n_words(df['desc'], 20)\n",
    "# df1 = pd.DataFrame(common_words, columns = ['desc' , 'count'])\n",
    "# df1.groupby('desc').sum()['count'].sort_values().iplot(kind='barh', yTitle='Count', linecolor='black', title='Top 20 words in hotel description before removing stop words')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def get_top_n_words(corpus, n=None):\n",
    "#     vec = CountVectorizer(stop_words='english').fit(corpus)\n",
    "#     bag_of_words = vec.transform(corpus)\n",
    "#     sum_words = bag_of_words.sum(axis=0) \n",
    "#     words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "#     words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "#     return words_freq[:n]\n",
    "# common_words = get_top_n_words(df['desc'], 20)\n",
    "# df2 = pd.DataFrame(common_words, columns = ['desc' , 'count'])\n",
    "# df2.groupby('desc').sum()['count'].sort_values().iplot(kind='barh', yTitle='Count', linecolor='black', title='Top 20 words in hotel description after removing stop words')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def get_top_n_bigram(corpus, n=None):\n",
    "#     vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n",
    "#     bag_of_words = vec.transform(corpus)\n",
    "#     sum_words = bag_of_words.sum(axis=0) \n",
    "#     words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "#     words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "#     return words_freq[:n]\n",
    "# common_words = get_top_n_bigram(df['desc'], 20)\n",
    "# df3 = pd.DataFrame(common_words, columns = ['desc' , 'count'])\n",
    "# df3.groupby('desc').sum()['count'].sort_values(ascending=False).iplot(kind='bar', yTitle='Count', linecolor='black', title='Top 20 bigrams in hotel description before removing stop words')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def get_top_n_bigram(corpus, n=None):\n",
    "#     vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(corpus)\n",
    "#     bag_of_words = vec.transform(corpus)\n",
    "#     sum_words = bag_of_words.sum(axis=0) \n",
    "#     words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "#     words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "#     return words_freq[:n]\n",
    "# common_words = get_top_n_bigram(df['desc'], 20)\n",
    "# df4 = pd.DataFrame(common_words, columns = ['desc' , 'count'])\n",
    "# df4.groupby('desc').sum()['count'].sort_values(ascending=False).iplot(kind='bar', yTitle='Count', linecolor='black', title='Top 20 bigrams in hotel description After removing stop words')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def get_top_n_trigram(corpus, n=None):\n",
    "#     vec = CountVectorizer(ngram_range=(3, 3)).fit(corpus)\n",
    "#     bag_of_words = vec.transform(corpus)\n",
    "#     sum_words = bag_of_words.sum(axis=0) \n",
    "#     words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "#     words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "#     return words_freq[:n]\n",
    "# common_words = get_top_n_trigram(df['desc'], 20)\n",
    "# df5 = pd.DataFrame(common_words, columns = ['desc' , 'count'])\n",
    "# df5.groupby('desc').sum()['count'].sort_values(ascending=False).iplot(kind='bar', yTitle='Count', linecolor='black', title='Top 20 trigrams in hotel description before removing stop words')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def get_top_n_trigram(corpus, n=None):\n",
    "#     vec = CountVectorizer(ngram_range=(3, 3), stop_words='english').fit(corpus)\n",
    "#     bag_of_words = vec.transform(corpus)\n",
    "#     sum_words = bag_of_words.sum(axis=0) \n",
    "#     words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "#     words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "#     return words_freq[:n]\n",
    "# common_words = get_top_n_trigram(df['desc'], 20)\n",
    "# df6 = pd.DataFrame(common_words, columns = ['desc' , 'count'])\n",
    "# df6.groupby('desc').sum()['count'].sort_values(ascending=False).iplot(kind='bar', yTitle='Count', linecolor='black', title='Top 20 trigrams in hotel description after removing stop words')\n",
    "\n",
    "\n",
    "\n",
    "# df['word_count'] = df['desc'].apply(lambda x: len(str(x).split()))\n",
    "# desc_lengths = list(df['word_count'])\n",
    "# print(\"Number of descriptions:\",len(desc_lengths),\n",
    "#       \"\\nAverage word count\", np.average(desc_lengths),\n",
    "#       \"\\nMinimum word count\", min(desc_lengths),\n",
    "#       \"\\nMaximum word count\", max(desc_lengths))\n",
    "\n",
    "\n",
    "# df['word_count'].iplot(\n",
    "#     kind='hist',\n",
    "#     bins = 50,\n",
    "#     linecolor='black',\n",
    "#     xTitle='word count',\n",
    "#     yTitle='count',\n",
    "#     title='Word Count Distribution in Hotel Description')\n",
    "\n",
    "\n",
    "# REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "# BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "# STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "# def clean_text(text):\n",
    "#     \"\"\"\n",
    "#         text: a string\n",
    "        \n",
    "#         return: modified initial string\n",
    "#     \"\"\"\n",
    "#     text = text.lower() # lowercase text\n",
    "#     text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "#     text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "#     text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "#     return text\n",
    "    \n",
    "# df['desc_clean'] = df['desc'].apply(clean_text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df.set_index('name', inplace = True)\n",
    "# tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0, stop_words='english')\n",
    "# tfidf_matrix = tf.fit_transform(df['desc_clean'])\n",
    "# cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# indices = pd.Series(df.index)\n",
    "\n",
    "# def recommendations(name, cosine_similarities = cosine_similarities):\n",
    "    \n",
    "#     recommended_hotels = []\n",
    "    \n",
    "#     # gettin the index of the hotel that matches the name\n",
    "#     idx = indices[indices == name].index[0]\n",
    "\n",
    "#     # creating a Series with the similarity scores in descending order\n",
    "#     score_series = pd.Series(cosine_similarities[idx]).sort_values(ascending = False)\n",
    "\n",
    "#     # getting the indexes of the 10 most similar hotels except itself\n",
    "#     top_10_indexes = list(score_series.iloc[1:11].index)\n",
    "    \n",
    "#     # populating the list with the names of the top 10 matching hotels\n",
    "#     for i in top_10_indexes:\n",
    "#         recommended_hotels.append(list(df.index)[i])\n",
    "        \n",
    "#     return recommended_hotels\n",
    "\n",
    "\n",
    "\n",
    "# recommendations('Hilton Seattle Airport & Conference Center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import scipy.sparse as sparse\n",
    "# from scipy.sparse.linalg import spsolve\n",
    "# import random\n",
    "# from sklearn import metrics\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# import implicit\n",
    "\n",
    "# retail_df = pd.read_excel('data/Online Retail.xlsx')\n",
    "# retail_df.info()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# retail_df = retail_df[retail_df['CustomerID'].notna()]\n",
    "# grouped_df = retail_df[['CustomerID', 'StockCode', 'Description', 'Quantity']].groupby(['CustomerID', 'StockCode', 'Description']).sum().reset_index()\n",
    "# grouped_df.loc[grouped_df['Quantity'] == 0, ['Quantity']] = 1\n",
    "# grouped_df = grouped_df.loc[grouped_df['Quantity'] > 0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import plotly.express as px\n",
    "\n",
    "# fig = px.histogram(grouped_df, x='Quantity', title='Distribution of the purchase quantity', nbins=500)\n",
    "# fig.show();\n",
    "\n",
    "\n",
    "\n",
    "# print(f'Number of unique customers: {grouped_df.CustomerID.nunique()}')\n",
    "# print(f'Number of unique items: {grouped_df.StockCode.nunique()}')\n",
    "\n",
    "# print(f'Average purchase quantity per interaction: {int(grouped_df.Quantity.mean())}')\n",
    "# print(f'Minimum purchase quantity per interaction: {grouped_df.Quantity.min()}')\n",
    "# print(f'Maximum purchase quantity per interaction: {grouped_df.Quantity.max()}')\n",
    "\n",
    "\n",
    "\n",
    "# unique_customers = grouped_df.CustomerID.unique()\n",
    "# customer_ids = dict(zip(unique_customers, np.arange(unique_customers.shape[0], dtype=np.int32)))\n",
    "\n",
    "# unique_items = grouped_df.StockCode.unique()\n",
    "# item_ids = dict(zip(unique_items, np.arange(unique_items.shape[0], dtype=np.int32)))\n",
    "\n",
    "# grouped_df['customer_id'] = grouped_df.CustomerID.apply(lambda i: customer_ids[i])\n",
    "# grouped_df['item_id'] = grouped_df.StockCode.apply(lambda i: item_ids[i])\n",
    "\n",
    "# sparse_item_customer = sparse.csr_matrix((grouped_df['Quantity'].astype(float), (grouped_df['item_id'], grouped_df['customer_id'])))\n",
    "# sparse_customer_item = sparse.csr_matrix((grouped_df['Quantity'].astype(float), (grouped_df['customer_id'], grouped_df['item_id'])))\n",
    "\n",
    "# model = implicit.als.AlternatingLeastSquares(factors=20, regularization=0.1, iterations=50)\n",
    "\n",
    "# alpha = 15\n",
    "# data = (sparse_item_customer * alpha).astype('double')\n",
    "\n",
    "# model.fit(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# grouped_df.loc[grouped_df['item_id'] == 1319].head()\n",
    "\n",
    "\n",
    "# item_id = 1319\n",
    "# n_similar = 10\n",
    "\n",
    "# item_vecs = model.item_factors\n",
    "# customer_vecs = model.user_factors\n",
    "\n",
    "# item_norms = np.sqrt((item_vecs * item_vecs).sum(axis=1))\n",
    "\n",
    "# scores = item_vecs.dot(item_vecs[item_id]) / item_norms\n",
    "# top_idx = np.argpartition(scores, -n_similar)[-n_similar:]\n",
    "# similar = sorted(zip(top_idx, scores[top_idx] / item_norms[item_id]), key=lambda x: -x[1])\n",
    "\n",
    "# for item in similar:\n",
    "#     idx, score = item\n",
    "#     print(grouped_df.Description.loc[grouped_df.item_id == idx].iloc[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def recommend(customer_id, sparse_customer_item, customer_vecs, item_vecs, num_items=10):\n",
    "    \n",
    "#     customer_interactions = sparse_customer_item[customer_id,:].toarray()\n",
    "#     customer_interactions = customer_interactions.reshape(-1) + 1\n",
    "#     customer_interactions[customer_interactions > 1] = 0\n",
    "    \n",
    "#     rec_vector = customer_vecs[customer_id,:].dot(item_vecs.T).toarray()\n",
    "    \n",
    "#     min_max = MinMaxScaler()\n",
    "#     rec_vector_scaled = min_max.fit_transform(rec_vector.reshape(-1,1))[:,0]\n",
    "#     recommend_vector = customer_interactions * rec_vector_scaled\n",
    "\n",
    "#     item_idx = np.argsort(recommend_vector)[::-1][:num_items]\n",
    "    \n",
    "#     descriptions = []\n",
    "#     scores = []\n",
    "\n",
    "#     for idx in item_idx:\n",
    "#         descriptions.append(grouped_df.Description.loc[grouped_df.item_id == idx].iloc[0])\n",
    "#         scores.append(recommend_vector[idx])\n",
    "\n",
    "#     recommendations = pd.DataFrame({'description': descriptions, 'score': scores})\n",
    "\n",
    "#     return recommendations\n",
    "    \n",
    "# customer_vecs = sparse.csr_matrix(model.user_factors)\n",
    "# item_vecs = sparse.csr_matrix(model.item_factors)\n",
    "# # Create recommendations for customer with id 2\n",
    "# customer_id = 2\n",
    "# recommendations = recommend(customer_id, sparse_customer_item, customer_vecs, item_vecs)\n",
    "\n",
    "# print(recommendations)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# grouped_df.loc[grouped_df['customer_id'] == 2].sort_values('Quantity', ascending=False)[['customer_id', 'Description', 'Quantity']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Recommendation Systems: OpenAI’s Embeddings, Matrix Factorization and Deep Learning\n",
    "- [Articulo](https://medium.com/@chenycy/build-recommendation-systems-openais-embeddings-matrix-factorization-and-deep-learning-0cac62008f0c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics.pairwise import linear_kernel\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# #Import data from the clean file \n",
    "# df = pd.read_csv('metadata_clean.csv')\n",
    "# orig_df = pd.read_csv('movies_metadata.csv')\n",
    "# df['overview'], df['id'] = orig_df['overview'], orig_df['id']\n",
    "# df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from openai import OpenAI\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# load_dotenv() \n",
    "# #load_dotenv() is a function that loads variables from a .env file into environment variables in a Python script. \n",
    "# # We store OPENAI_API_KEY = xxx in .env file\n",
    "\n",
    "# client = OpenAI()\n",
    "\n",
    "# MODEL_NAME = \"text-embedding-ada-002\"\n",
    "\n",
    "# def get_embedding(text, model=MODEL_NAME):\n",
    "#     if not isinstance(text, str):\n",
    "#         text = str(text)\n",
    "#     text = text.replace(\"\\n\", \" \")\n",
    "#     return client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "\n",
    "# def cosine_similarity(a, b):\n",
    "#     return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# def search_docs(df, user_query, threshold=0.8):\n",
    "#     embedding = get_embedding(user_query, model=MODEL_NAME)\n",
    "#     df[\"similarities\"] = df.embedding.apply(lambda x: cosine_similarity(x, embedding))\n",
    "#     # Filter results based on the threshold\n",
    "#     filtered_results = df[df[\"similarities\"] > threshold]\n",
    "#     return filtered_results\n",
    "\n",
    "\n",
    "# df['embedding'] = df['overview'].apply(lambda x: get_embedding(x, model=MODEL_NAME))\n",
    "# title = \"Toy Story\"\n",
    "# description = df.loc[df[\"title\"] == title, \"overview\"].iloc[0]\n",
    "# result = search_docs(df, description, threshold=0.8)\n",
    "# #remove the search item\n",
    "# print(result[result[\"title\"] != title]['title'])\n",
    "\n",
    "\n",
    "\n",
    "# #Define a TF-IDF Vectorizer Object. Remove all english stopwords\n",
    "# tfidf = TfidfVectorizer(stop_words='english')\n",
    "# #Replace NaN with an empty string\n",
    "# df['overview'] = df['overview'].fillna('')\n",
    "# #Construct the required TF-IDF matrix by applying the fit_transform method on the overview feature\n",
    "# tfidf_matrix = tfidf.fit_transform(df['overview'])\n",
    "# # Compute the cosine similarity matrix\n",
    "# cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "# indices = pd.Series(df.index, index=df['title']).drop_duplicates()\n",
    "\n",
    "# def content_recommender(title, cosine_sim=cosine_sim, df=df, indices=indices):\n",
    "# # Obtain the index of the movie that matches the title\n",
    "#   idx = indices[title]\n",
    "#   # Get the pairwsie similarity scores of all movies with that movie\n",
    "#   # And convert it into a list of tuples as described above\n",
    "#   sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "#   # Sort the movies based on the cosine similarity scores\n",
    "#   sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "#   # Get the scores of the 10 most similar movies. Ignore the first movie.\n",
    "#   sim_scores = sim_scores[1:11]\n",
    "#   # Get the movie indices\n",
    "#   movie_indices = [i[0] for i in sim_scores]\n",
    "#   # Return the top 10 most similar movies\n",
    "#   return df['title'].iloc[movie_indices]\n",
    "\n",
    "# #Get recommendations\n",
    "# content_recommender('The Shawshank Redemption')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from surprise import Dataset, Reader, KNNBasic\n",
    "# from surprise.model_selection import cross_validate\n",
    "# import os\n",
    "\n",
    "# # path to dataset file\n",
    "# file_path = os.path.expanduser(\"ml-1m/ratings.dat\")\n",
    "\n",
    "# # As we're loading a custom dataset, we need to define a reader. In the\n",
    "# # movielens-100k dataset, each line has the following format:\n",
    "# # 'user item rating timestamp', separated by '::' characters.\n",
    "# columns = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "# reader = Reader(line_format=\"user item rating timestamp\", sep=\"::\")\n",
    "\n",
    "# data = Dataset.load_from_file(file_path, reader=reader)\n",
    "# trainset = data.build_full_trainset()\n",
    "\n",
    "# sim_options = {'name': 'cosine', 'user_based': True}\n",
    "# knn_model = KNNBasic(sim_options=sim_options)\n",
    "\n",
    "# knn_model.fit(trainset)\n",
    "# user_id = str(196)  # Replace with the desired user ID\n",
    "\n",
    "# # Get items that the user has not rated\n",
    "# items_to_predict = [(user_id, iid, 4.0) for iid in trainset.all_items() if iid not in trainset.ur[trainset.to_inner_uid(user_id)]]\n",
    "# # Get top N recommendations for the user\n",
    "# top_n = knn_model.test(items_to_predict)[0:11]\n",
    "\n",
    "# # Display the top N recommendations\n",
    "# for uid, iid, true_r, est, _ in top_n:\n",
    "#     print(f\"User {uid} -> Item {iid} (Predicted rating: {est:.2f})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from surprise import Dataset, Reader, KNNBasic\n",
    "# from surprise.model_selection import cross_validate\n",
    "# import os\n",
    "\n",
    "# # path to dataset file\n",
    "# file_path = os.path.expanduser(\"ml-1m/ratings.dat\")\n",
    "\n",
    "# # As we're loading a custom dataset, we need to define a reader. In the\n",
    "# # movielens-100k dataset, each line has the following format:\n",
    "# # 'user item rating timestamp', separated by '::' characters.\n",
    "# columns = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "# reader = Reader(line_format=\"user item rating timestamp\", sep=\"::\")\n",
    "\n",
    "# data = Dataset.load_from_file(file_path, reader=reader)\n",
    "# trainset = data.build_full_trainset()\n",
    "\n",
    "# sim_options = {'name': 'cosine', 'user_based': False}\n",
    "# knn_model = KNNBasic(sim_options=sim_options)\n",
    "\n",
    "# knn_model.fit(trainset)\n",
    "\n",
    "# user_id = str(196)  # Replace with the desired user ID\n",
    "\n",
    "# # Get items that the user has not rated\n",
    "# items_to_predict = [(user_id, iid, 4.0) for iid in trainset.all_items() if iid not in trainset.ur[trainset.to_inner_uid(user_id)]]\n",
    "\n",
    "# # Get top N recommendations for the user\n",
    "# top_n = knn_model.test(items_to_predict)[0:11]\n",
    "\n",
    "# # Display the top N recommendations\n",
    "# for uid, iid, true_r, est, _ in top_n:\n",
    "#     print(f\"User {uid} -> Item {iid} (Predicted rating: {est:.2f})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from surprise import Dataset, Reader, SVD\n",
    "# from surprise.model_selection import train_test_split\n",
    "# from surprise.accuracy import rmse\n",
    "\n",
    "# # path to dataset file\n",
    "# file_path = os.path.expanduser(\"ml-1m/ratings.dat\")\n",
    "\n",
    "# # As we're loading a custom dataset, we need to define a reader. In the\n",
    "# # movielens-100k dataset, each line has the following format:\n",
    "# # 'user item rating timestamp', separated by '::' characters.\n",
    "# columns = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "# reader = Reader(line_format=\"user item rating timestamp\", sep=\"::\")\n",
    "\n",
    "# data = Dataset.load_from_file(file_path, reader=reader)\n",
    "# full_data = data.build_full_trainset()\n",
    "\n",
    "# train_set, test_set = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Use SVD for item-based collaborative filtering\n",
    "# svd_model = SVD()  # Set user_based to False for item-based collaborative filtering\n",
    "\n",
    "# # Train the model on the training set\n",
    "# svd_model.fit(train_set)\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# predictions = svd_model.test(test_set)\n",
    "\n",
    "# # Evaluate the model using RMSE\n",
    "# accuracy = rmse(predictions)\n",
    "# print(f\"RMSE on the test set: {accuracy:.4f}\")\n",
    "\n",
    "# from collections import defaultdict\n",
    "# def get_top_n(predictions, n=10):\n",
    "#     top_n = defaultdict(list)\n",
    "#     for uid, iid, true_r, est, _ in predictions:\n",
    "#         top_n[uid].append((iid, est))\n",
    "#     for uid, user_ratings in top_n.items():\n",
    "#             user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "#             top_n[uid] = user_ratings[:n]\n",
    "#     return top_n\n",
    "\n",
    "# top_n = get_top_n(predictions, n=10)\n",
    "# for uid, user_ratings in top_n.items():\n",
    "#     print(uid, [iid for (iid, _) in user_ratings])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.layers import Input, Dense\n",
    "# from tensorflow.keras.models import Model\n",
    "\n",
    "# # Load the MovieLens dataset (download it from https://grouplens.org/datasets/movielens/)\n",
    "# file_path = 'ml-1m/ratings.dat'\n",
    "# columns = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "# df = pd.read_csv(file_path, sep='\\t', names=columns)\n",
    "\n",
    "# # Create user-item interaction matrix\n",
    "# user_item_matrix = df.pivot(index='user_id', columns='item_id', values='rating').fillna(0).values\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# train_data, test_data = train_test_split(user_item_matrix, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Build the autoencoder model\n",
    "# num_users, num_items = user_item_matrix.shape\n",
    "# latent_dim = 50\n",
    "\n",
    "# input_layer = Input(shape=(num_items,))\n",
    "# encoded = Dense(latent_dim, activation='relu')(input_layer)\n",
    "# decoded = Dense(num_items, activation='sigmoid')(encoded)\n",
    "\n",
    "# autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "# autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# # Train the autoencoder\n",
    "# autoencoder.fit(train_data, train_data, epochs=10, batch_size=64, shuffle=True, validation_data=(test_data, test_data))\n",
    "\n",
    "# # Extract user and item representations from the encoder part of the autoencoder\n",
    "# encoder = Model(inputs=input_layer, outputs=encoded)\n",
    "# user_embeddings = encoder.predict(user_item_matrix)\n",
    "\n",
    "# # Example: Recommend items for a specific user\n",
    "# user_id = 1  # Replace with the desired user ID\n",
    "# user_representation = user_embeddings[user_id - 1]\n",
    "\n",
    "# # Calculate the predicted ratings for all items\n",
    "# predicted_ratings = np.dot(user_embeddings, user_representation)\n",
    "\n",
    "# # Display top N recommendations\n",
    "# top_n = np.argsort(predicted_ratings)[::-1][:10]\n",
    "# print(f\"Top recommendations for User {user_id}: {top_n + 1}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.layers import Input, Dense\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# # Load the MovieLens dataset (download it from https://grouplens.org/datasets/movielens/)\n",
    "# file_path = 'ml-1m/ratings.dat'\n",
    "# columns = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "# df = pd.read_csv(file_path, sep='\\t', names=columns)\n",
    "\n",
    "# # Create user-item interaction matrix\n",
    "# user_item_matrix = df.pivot(index='user_id', columns='item_id', values='rating').fillna(0).values\n",
    "\n",
    "# # Binarize the ratings (0 if not rated, 1 if rated)\n",
    "# user_item_matrix_binary = (user_item_matrix > 0).astype(float)\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# train_data, test_data = train_test_split(user_item_matrix_binary, test_size=0.2, random_state=42)\n",
    "\n",
    "# # RBM parameters\n",
    "# num_visible = num_items = user_item_matrix_binary.shape[1]\n",
    "# num_hidden = 50\n",
    "# batch_size = 64\n",
    "# epochs = 10\n",
    "\n",
    "# # Build the RBM model\n",
    "# visible_layer = Input(shape=(num_visible,))\n",
    "# hidden_layer = Dense(num_hidden, activation='sigmoid')(visible_layer)\n",
    "# visible_layer_reconstruction = Dense(num_visible, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# rbm = Model(inputs=visible_layer, outputs=visible_layer_reconstruction)\n",
    "# rbm.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# # Train the RBM\n",
    "# rbm.fit(train_data, train_data, epochs=epochs, batch_size=batch_size, shuffle=True, validation_data=(test_data, test_data))\n",
    "\n",
    "# # Extract user and item representations from the hidden layer\n",
    "# user_embeddings = rbm.layers[1].get_weights()[0].T\n",
    "# item_embeddings = rbm.layers[1].get_weights()[0]\n",
    "\n",
    "# # Example: Recommend items for a specific user\n",
    "# user_id = 1  # Replace with the desired user ID\n",
    "# user_representation = user_embeddings[user_id - 1]\n",
    "\n",
    "# # Calculate the predicted ratings for all items\n",
    "# predicted_ratings = np.dot(user_embeddings, user_representation)\n",
    "\n",
    "# # Display top N recommendations\n",
    "# top_n = np.argsort(predicted_ratings)[::-1][:10]\n",
    "# print(f\"Top recommendations for User {user_id}: {top_n + 1}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering Model with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The collaborative filter approach focuses on finding users who have given similar ratings to the same books, thus creating a link between users, to whom will be suggested books that were reviewed in a positive way.\n",
    "# # In this way, we look for associations between users, not between books.\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import tensorflow as tf\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# rating = pd.read_csv('data/BX-Book-Ratings.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n",
    "# user = pd.read_csv('data/BX-Users.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n",
    "# book = pd.read_csv('data/BX-Books.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n",
    "# book_rating = pd.merge(rating, book, on='ISBN')\n",
    "# cols = ['Year-Of-Publication', 'Publisher', 'Book-Author', 'Image-URL-S', 'Image-URL-M', 'Image-URL-L']\n",
    "# book_rating.drop(cols, axis=1, inplace=True)\n",
    "# book_rating.head()\n",
    "\n",
    "\n",
    "\n",
    "# book_rating.head(3)\n",
    "\n",
    "\n",
    "\n",
    "# rating_count = (book_rating.\n",
    "#      groupby(by = ['Book-Title'])['Book-Rating'].\n",
    "#      count().\n",
    "#      reset_index().\n",
    "#      rename(columns = {'Book-Rating': 'RatingCount_book'})\n",
    "#      [['Book-Title', 'RatingCount_book']]\n",
    "#     )\n",
    "# rating_count.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# threshold = 25\n",
    "# rating_count = rating_count.query('RatingCount_book >= @threshold')\n",
    "# rating_count.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# book_rating.head(3)\n",
    "\n",
    "\n",
    "# user_rating = pd.merge(rating_count, book_rating, left_on='Book-Title', right_on='Book-Title', how='left')\n",
    "\n",
    "# user_rating.head(3)\n",
    "\n",
    "\n",
    "\n",
    "# user_count = (user_rating.\n",
    "#      groupby(by = ['User-ID'])['Book-Rating'].\n",
    "#      count().\n",
    "#      reset_index().\n",
    "#      rename(columns = {'Book-Rating': 'RatingCount_user'})\n",
    "#      [['User-ID', 'RatingCount_user']]\n",
    "#     )\n",
    "# user_count.head()\n",
    "\n",
    "\n",
    "# threshold = 20\n",
    "# user_count = user_count.query('RatingCount_user >= @threshold')\n",
    "# user_count.head()\n",
    "\n",
    "\n",
    "\n",
    "# combined = user_rating.merge(user_count, left_on = 'User-ID', right_on = 'User-ID', how = 'inner')\n",
    "\n",
    "\n",
    "\n",
    "# combined.head(3)\n",
    "\n",
    "\n",
    "\n",
    "# combined.shape\n",
    "\n",
    "\n",
    "# print('Number of unique books: ', combined['Book-Title'].nunique())\n",
    "# print('Number of unique users: ', combined['User-ID'].nunique())\n",
    "\n",
    "\n",
    "\n",
    "# # Normalize the ratings.\n",
    "# scaler = MinMaxScaler()\n",
    "# combined['Book-Rating'] = combined['Book-Rating'].values.astype(float)\n",
    "# rating_scaled = pd.DataFrame(scaler.fit_transform(combined['Book-Rating'].values.reshape(-1,1)))\n",
    "# combined['Book-Rating'] = rating_scaled\n",
    "\n",
    "\n",
    "# # Abd build the user book matrix.\n",
    "\n",
    "# combined = combined.drop_duplicates(['User-ID', 'Book-Title'])\n",
    "# user_book_matrix = combined.pivot(index='User-ID', columns='Book-Title', values='Book-Rating')\n",
    "# user_book_matrix.fillna(0, inplace=True)\n",
    "\n",
    "# users = user_book_matrix.index.tolist()\n",
    "# books = user_book_matrix.columns.tolist()\n",
    "\n",
    "# user_book_matrix = user_book_matrix.as_matrix()\n",
    "\n",
    "\n",
    "# # tf.placeholder only available in v1, so we have to work around. \n",
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "# # We will initialize the TensorFlow placeholder. Then, weights and biases are randomly initialized, the following code are taken from the book: Python Machine Learning Cook Book - Second Edition\n",
    "# num_input = combined['Book-Title'].nunique()\n",
    "# num_hidden_1 = 10\n",
    "# num_hidden_2 = 5\n",
    "\n",
    "# X = tf.placeholder(tf.float64, [None, num_input])\n",
    "\n",
    "# weights = {\n",
    "#     'encoder_h1': tf.Variable(tf.random_normal([num_input, num_hidden_1], dtype=tf.float64)),\n",
    "#     'encoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_hidden_2], dtype=tf.float64)),\n",
    "#     'decoder_h1': tf.Variable(tf.random_normal([num_hidden_2, num_hidden_1], dtype=tf.float64)),\n",
    "#     'decoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_input], dtype=tf.float64)),\n",
    "# }\n",
    "\n",
    "# biases = {\n",
    "#     'encoder_b1': tf.Variable(tf.random_normal([num_hidden_1], dtype=tf.float64)),\n",
    "#     'encoder_b2': tf.Variable(tf.random_normal([num_hidden_2], dtype=tf.float64)),\n",
    "#     'decoder_b1': tf.Variable(tf.random_normal([num_hidden_1], dtype=tf.float64)),\n",
    "#     'decoder_b2': tf.Variable(tf.random_normal([num_input], dtype=tf.float64)),\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Now, we can build the encoder and decoder model, as follows:\n",
    "# def encoder(x):\n",
    "#     layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']), biases['encoder_b1']))\n",
    "#     layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']), biases['encoder_b2']))\n",
    "#     return layer_2\n",
    "\n",
    "# def decoder(x):\n",
    "#     layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']), biases['decoder_b1']))\n",
    "#     layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']), biases['decoder_b2']))\n",
    "#     return layer_2\n",
    "\n",
    "# # We will construct the model and the predictions\n",
    "# encoder_op = encoder(X)\n",
    "# decoder_op = decoder(encoder_op)\n",
    "# y_pred = decoder_op\n",
    "# y_true = X\n",
    "\n",
    "\n",
    "\n",
    "# # define loss function and optimizer, and minimize the squared error, and define the evaluation metrics\n",
    "# loss = tf.losses.mean_squared_error(y_true, y_pred)\n",
    "# optimizer = tf.train.RMSPropOptimizer(0.03).minimize(loss)\n",
    "# eval_x = tf.placeholder(tf.int32, )\n",
    "# eval_y = tf.placeholder(tf.int32, )\n",
    "# pre, pre_op = tf.metrics.precision(labels=eval_x, predictions=eval_y)\n",
    "\n",
    "# # Initialize the variables. Because TensorFlow uses computational graphs for its operations, placeholders and variables must be initialized.\n",
    "# init = tf.global_variables_initializer()\n",
    "# local_init = tf.local_variables_initializer()\n",
    "# pred_data = pd.DataFrame()\n",
    "\n",
    "# # We can finally start to train our model.\n",
    "# # We split training data into batches, and we feed the network with them.\n",
    "# # We train our model with vectors of user ratings, each vector represents a user and each column a book, and entries are ratings that the user gave to books. \n",
    "# # After a few trials, I discovered that training model for 5 epochs with a batch size of 10 would be consum enough memory. This means that the entire training set will feed our neural network 20 times, every time using 50 users.\n",
    "\n",
    "# with tf.Session() as session:\n",
    "#     epochs = 100\n",
    "#     batch_size = 35\n",
    "\n",
    "#     session.run(init)\n",
    "#     session.run(local_init)\n",
    "\n",
    "#     num_batches = int(user_book_matrix.shape[0] / batch_size)\n",
    "#     user_book_matrix = np.array_split(user_book_matrix, num_batches)\n",
    "    \n",
    "#     for i in range(epochs):\n",
    "\n",
    "#         avg_cost = 0\n",
    "#         for batch in user_book_matrix:\n",
    "#             _, l = session.run([optimizer, loss], feed_dict={X: batch})\n",
    "#             avg_cost += l\n",
    "\n",
    "#         avg_cost /= num_batches\n",
    "\n",
    "#         print(\"epoch: {} Loss: {}\".format(i + 1, avg_cost))\n",
    "\n",
    "#     user_book_matrix = np.concatenate(user_book_matrix, axis=0)\n",
    "\n",
    "#     preds = session.run(decoder_op, feed_dict={X: user_book_matrix})\n",
    "\n",
    "#     pred_data = pred_data.append(pd.DataFrame(preds))\n",
    "\n",
    "#     pred_data = pred_data.stack().reset_index(name='Book-Rating')\n",
    "#     pred_data.columns = ['User-ID', 'Book-Title', 'Book-Rating']\n",
    "#     pred_data['User-ID'] = pred_data['User-ID'].map(lambda value: users[value])\n",
    "#     pred_data['Book-Title'] = pred_data['Book-Title'].map(lambda value: books[value])\n",
    "    \n",
    "#     keys = ['User-ID', 'Book-Title']\n",
    "#     index_1 = pred_data.set_index(keys).index\n",
    "#     index_2 = combined.set_index(keys).index\n",
    "\n",
    "#     top_ten_ranked = pred_data[~index_1.isin(index_2)]\n",
    "#     top_ten_ranked = top_ten_ranked.sort_values(['User-ID', 'Book-Rating'], ascending=[True, False])\n",
    "#     top_ten_ranked = top_ten_ranked.groupby('User-ID').head(10)\n",
    "\n",
    "\n",
    "# top_ten_ranked.loc[top_ten_ranked['User-ID'] == 278582]\n",
    "# book_rating.loc[book_rating['User-ID'] == 278582].sort_values(by=['Book-Rating'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # EJERCICIOS: Datos de los ejercicios\n",
    "# ejercicios_data = pd.DataFrame({\n",
    "#     'oid': [1, 2, 3, 4],\n",
    "#     'nombre': ['Ejercicio 1', 'Ejercicio 2', 'Ejercicio 3', 'Ejercicio 4'],\n",
    "#     'h1': [1, 0, 1, 0],\n",
    "#     'h2': [0, 1, 0, 1],\n",
    "#     'h3': [0, 0, 1, 1],\n",
    "#     'h4': [1, 1, 0, 0],\n",
    "#     's1': [1, 0, 1, 0],\n",
    "#     's2': [0, 1, 0, 1],\n",
    "#     's3': [1, 1, 0, 0],\n",
    "#     's4': [0, 0, 1, 1],\n",
    "#     'skill': [3, 2, 5, 1],\n",
    "#     'knowledge': [4, 3, 6, 2],\n",
    "#     'complexity': [7, 5, 11, 3],  # Ejemplo de complejidad combinada de skill + knowledge\n",
    "# })\n",
    "\n",
    "# # EJERCICIOS REALIZADOS: Datos de ejercicios completados por los estudiantes\n",
    "# ejercicios_realizados_data = pd.DataFrame({\n",
    "#     'rut': [101, 102, 103],\n",
    "#     'e0': [1, 0, 1],\n",
    "#     'e1': [0, 1, 0],\n",
    "#     'e2': [1, 0, 1],\n",
    "#     'e3': [0, 1, 0]\n",
    "# })\n",
    "\n",
    "# # USUARIOS: Datos de los estudiantes\n",
    "# usuarios_data = pd.DataFrame({\n",
    "#     'rut': [101, 102, 103],\n",
    "#     'programa': ['Ingeniería', 'Ciencias', 'Matemáticas'],\n",
    "#     'hito1': [3, 4, 5],\n",
    "#     'hito2': [5, 3, 2],\n",
    "#     'exitosos': [10, 12, 8],\n",
    "#     'fallidos': [2, 1, 3]\n",
    "# })\n",
    "\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers\n",
    "\n",
    "# # Dimensiones de los embeddings\n",
    "# embedding_dim = 8\n",
    "\n",
    "# # Red para los estudiantes\n",
    "# def build_student_model():\n",
    "#     # Entradas\n",
    "#     student_rut_input = layers.Input(shape=(1,), name=\"student_rut\")\n",
    "#     student_programa_input = layers.Input(shape=(1,), name=\"student_programa\")\n",
    "#     student_hito_input = layers.Input(shape=(4,), name=\"student_hitos\")\n",
    "    \n",
    "#     # Embedding para el ID del estudiante\n",
    "#     student_embedding = layers.Embedding(input_dim=1000, output_dim=embedding_dim)(student_rut_input)\n",
    "#     student_flatten = layers.Flatten()(student_embedding)\n",
    "    \n",
    "#     # Embedding para el programa académico\n",
    "#     programa_embedding = layers.Embedding(input_dim=10, output_dim=embedding_dim)(student_programa_input)\n",
    "#     programa_flatten = layers.Flatten()(programa_embedding)\n",
    "    \n",
    "#     # Concatenar todas las entradas\n",
    "#     student_concat = layers.Concatenate()([student_flatten, programa_flatten, student_hito_input])\n",
    "    \n",
    "#     # Red completamente conectada\n",
    "#     student_dense = layers.Dense(64, activation='relu')(student_concat)\n",
    "#     student_dense = layers.Dense(embedding_dim)(student_dense)\n",
    "    \n",
    "#     return tf.keras.Model([student_rut_input, student_programa_input, student_hito_input], student_dense)\n",
    "\n",
    "# student_model = build_student_model()\n",
    "# student_model.summary()\n",
    "\n",
    "\n",
    "# # Red para los ejercicios\n",
    "# def build_exercise_model():\n",
    "#     # Entradas\n",
    "#     exercise_id_input = layers.Input(shape=(1,), name=\"exercise_id\")\n",
    "#     exercise_hito_input = layers.Input(shape=(4,), name=\"exercise_hito\")\n",
    "#     exercise_skill_input = layers.Input(shape=(4,), name=\"exercise_skill\")\n",
    "    \n",
    "#     # Embedding para el ID del ejercicio\n",
    "#     exercise_embedding = layers.Embedding(input_dim=1000, output_dim=embedding_dim)(exercise_id_input)\n",
    "#     exercise_flatten = layers.Flatten()(exercise_embedding)\n",
    "    \n",
    "#     # Concatenar todas las entradas\n",
    "#     exercise_concat = layers.Concatenate()([exercise_flatten, exercise_hito_input, exercise_skill_input])\n",
    "    \n",
    "#     # Red completamente conectada\n",
    "#     exercise_dense = layers.Dense(64, activation='relu')(exercise_concat)\n",
    "#     exercise_dense = layers.Dense(embedding_dim)(exercise_dense)\n",
    "    \n",
    "#     return tf.keras.Model([exercise_id_input, exercise_hito_input, exercise_skill_input], exercise_dense)\n",
    "\n",
    "# exercise_model = build_exercise_model()\n",
    "# exercise_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "# # Cálculo de similitud entre estudiantes y ejercicios\n",
    "# student_embeddings = student_model([student_rut_input, student_programa_input, student_hito_input])\n",
    "# exercise_embeddings = exercise_model([exercise_id_input, exercise_hito_input, exercise_skill_input])\n",
    "\n",
    "# # Producto punto\n",
    "# dot_product = tf.reduce_sum(tf.multiply(student_embeddings, exercise_embeddings), axis=1)\n",
    "\n",
    "# # Función de similitud\n",
    "# similarity = layers.Activation('sigmoid')(dot_product)\n",
    "\n",
    "# # Modelo final para entrenar\n",
    "# recommendation_model = tf.keras.Model(\n",
    "#     inputs=[student_rut_input, student_programa_input, student_hito_input, \n",
    "#             exercise_id_input, exercise_hito_input, exercise_skill_input],\n",
    "#     outputs=similarity\n",
    "# )\n",
    "\n",
    "# recommendation_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# recommendation_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "# # Simulación de etiquetas de ejercicios completados (0 o 1)\n",
    "# y_train = ejercicios_realizados_data.values[:, 1:]\n",
    "\n",
    "# # Entrenamiento del modelo\n",
    "# recommendation_model.fit(\n",
    "#     [students_rut_train, students_programa_train, students_hitos_train,\n",
    "#      exercises_id_train, exercises_hito_train, exercises_skill_train],\n",
    "#     y_train,\n",
    "#     epochs=10,\n",
    "#     batch_size=32\n",
    "# )\n",
    "\n",
    "\n",
    "# # Predicción\n",
    "# predictions = recommendation_model.predict([student_input, exercise_input])\n",
    "# top_k_recommendations = tf.argsort(predictions, direction='DESCENDING')[:5]\n",
    "\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# # EJERCICIOS REALIZADOS: Datos de ejercicios completados por los estudiantes\n",
    "# ejercicios_realizados_data = pd.DataFrame({\n",
    "#     'rut': [101, 102, 103],\n",
    "#     'e0': [1, 0, 1],  # 1 si el ejercicio fue realizado, 0 si no\n",
    "#     'e1': [0, 1, 0],\n",
    "#     'e2': [1, 0, 1],\n",
    "#     'e3': [0, 1, 0]\n",
    "# })\n",
    "\n",
    "# # Convertimos los datos en una matriz numpy\n",
    "# interactions_matrix = ejercicios_realizados_data.drop(columns=['rut']).values\n",
    "# n_students, n_exercises = interactions_matrix.shape\n",
    "\n",
    "\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, Model\n",
    "\n",
    "# # Definir el número de factores latentes (dimensión de los embeddings)\n",
    "# latent_dim = 8\n",
    "\n",
    "# # Definir entradas del modelo\n",
    "# student_input = layers.Input(shape=(1,), name='student_input')\n",
    "# exercise_input = layers.Input(shape=(1,), name='exercise_input')\n",
    "\n",
    "# # Crear embeddings para estudiantes y ejercicios\n",
    "# student_embedding = layers.Embedding(input_dim=n_students, output_dim=latent_dim, name='student_embedding')(student_input)\n",
    "# exercise_embedding = layers.Embedding(input_dim=n_exercises, output_dim=latent_dim, name='exercise_embedding')(exercise_input)\n",
    "\n",
    "# # Aplanar las representaciones (embeddings)\n",
    "# student_vec = layers.Flatten()(student_embedding)\n",
    "# exercise_vec = layers.Flatten()(exercise_embedding)\n",
    "\n",
    "# # Producto punto entre los embeddings de estudiantes y ejercicios\n",
    "# dot_product = layers.Dot(axes=1)([student_vec, exercise_vec])\n",
    "\n",
    "# # Definir el modelo completo\n",
    "# model = Model(inputs=[student_input, exercise_input], outputs=dot_product)\n",
    "\n",
    "# # Compilar el modelo\n",
    "# model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "# model.summary()\n",
    "\n",
    "\n",
    "# # Crear los datos de entrada para el modelo\n",
    "# students, exercises = np.where(~np.isnan(interactions_matrix))  # IDs de estudiantes y ejercicios\n",
    "# labels = interactions_matrix[students, exercises]  # 1 si completado, 0 si no completado\n",
    "\n",
    "# # Entrenar el modelo\n",
    "# model.fit([students, exercises], labels, epochs=10, batch_size=16)\n",
    "\n",
    "\n",
    "\n",
    "# # Ejemplo: Predecir los puntajes para un estudiante\n",
    "# student_id = 0  # Estudiante con ID 0\n",
    "# exercises_not_done = np.where(interactions_matrix[student_id] == 0)[0]  # Ejercicios no realizados por el estudiante\n",
    "\n",
    "# # Predecir el puntaje para cada ejercicio no realizado\n",
    "# predicted_scores = model.predict([np.array([student_id] * len(exercises_not_done)), exercises_not_done])\n",
    "\n",
    "# # Ordenar las predicciones para recomendar los mejores ejercicios\n",
    "# top_k_recommendations = exercises_not_done[np.argsort(predicted_scores, axis=0)[::-1][:5]]\n",
    "# print(f\"Top 5 ejercicios recomendados para el estudiante {student_id}: {top_k_recommendations}\")\n",
    "\n",
    "\n",
    "\n",
    "# # Ejemplo: Predecir los puntajes para un estudiante\n",
    "# student_id = 0  # Estudiante con ID 0\n",
    "# exercises_not_done = np.where(interactions_matrix[student_id] == 0)[0]  # Ejercicios no realizados por el estudiante\n",
    "\n",
    "# # Predecir el puntaje para cada ejercicio no realizado\n",
    "# predicted_scores = model.predict([np.array([student_id] * len(exercises_not_done)), exercises_not_done])\n",
    "\n",
    "# # Ordenar las predicciones para recomendar los mejores ejercicios\n",
    "# top_k_recommendations = exercises_not_done[np.argsort(predicted_scores, axis=0)[::-1][:5]]\n",
    "# print(f\"Top 5 ejercicios recomendados para el estudiante {student_id}: {top_k_recommendations}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Agregar características adicionales al modelo\n",
    "# exercise_hito_input = layers.Input(shape=(4,), name='exercise_hito_input')  # hito binario de los ejercicios\n",
    "# exercise_skill_input = layers.Input(shape=(4,), name='exercise_skill_input')  # skill binario de los ejercicios\n",
    "\n",
    "# # Concatenar las características adicionales con los embeddings\n",
    "# exercise_concat = layers.Concatenate()([exercise_vec, exercise_hito_input, exercise_skill_input])\n",
    "\n",
    "# # Modificar el modelo para incluir las nuevas entradas\n",
    "# new_dot_product = layers.Dense(1)(exercise_concat)\n",
    "# new_model = Model(inputs=[student_input, exercise_input, exercise_hito_input, exercise_skill_input], outputs=new_dot_product)\n",
    "\n",
    "# # Compilar y entrenar el modelo\n",
    "# new_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # Datos de la prueba diagnóstica\n",
    "# diagnostico_data = pd.DataFrame({\n",
    "#     'rut': [101, 102, 103],\n",
    "#     'score': [78, 65, 89],  # Puntaje total\n",
    "#     'score_a': [23, 15, 28],  # Puntaje abstracción\n",
    "#     'core_p': [20, 18, 22],  # Puntaje reconocimiento de patrones\n",
    "#     'score_d': [19, 14, 25],  # Puntaje descomposición\n",
    "#     'score_s': [16, 18, 14]   # Puntaje algoritmos\n",
    "# })\n",
    "\n",
    "# # Datos de los ejercicios con su complejidad\n",
    "# ejercicios_data = pd.DataFrame({\n",
    "#     'oid': [0, 1, 2, 3],\n",
    "#     'complexity': [60, 80, 50, 90],  # Complejidad de los ejercicios\n",
    "#     'hito': [1, 2, 1, 3],  # Hito asociado al ejercicio\n",
    "#     'skill': [2, 3, 2, 4],  # Nivel de habilidad requerido\n",
    "#     'knowledge': [3, 4, 3, 5]  # Nivel de conocimiento requerido\n",
    "# })\n",
    "\n",
    "\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # Normalizar los puntajes de la prueba diagnóstica y las complejidades de los ejercicios\n",
    "# diagnostico_scores = diagnostico_data[['score_a', 'core_p', 'score_d', 'score_s']].values\n",
    "# ejercicio_complexity = ejercicios_data[['skill', 'knowledge']].values\n",
    "\n",
    "# # Calcular la similaridad entre estudiantes y ejercicios basándose en los puntajes\n",
    "# similarity_matrix = cosine_similarity(diagnostico_scores, ejercicio_complexity)\n",
    "\n",
    "# # Crear recomendaciones iniciales para cada estudiante\n",
    "# def recomendar_ejercicios(similarity_matrix, top_k=3):\n",
    "#     for student_idx, similarities in enumerate(similarity_matrix):\n",
    "#         recommended_exercises = np.argsort(similarities)[-top_k:][::-1]\n",
    "#         print(f\"Recomendaciones para estudiante {diagnostico_data['rut'][student_idx]}: {recommended_exercises}\")\n",
    "\n",
    "# recomendar_ejercicios(similarity_matrix)\n",
    "\n",
    "\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, Model\n",
    "\n",
    "# # Input para el estudiante (puntajes diagnósticos)\n",
    "# diagnostic_input = layers.Input(shape=(4,), name='diagnostic_input')\n",
    "\n",
    "# # Input para el ejercicio (complejidad del ejercicio: skill + knowledge)\n",
    "# exercise_input = layers.Input(shape=(2,), name='exercise_input')\n",
    "\n",
    "# # Concatenar las entradas\n",
    "# combined_input = layers.Concatenate()([diagnostic_input, exercise_input])\n",
    "\n",
    "# # Capa oculta\n",
    "# hidden = layers.Dense(16, activation='relu')(combined_input)\n",
    "# hidden = layers.Dense(8, activation='relu')(hidden)\n",
    "\n",
    "# # Capa de salida: Probabilidad de completar exitosamente el ejercicio\n",
    "# output = layers.Dense(1, activation='sigmoid')(hidden)\n",
    "\n",
    "# # Definir el modelo\n",
    "# model = Model(inputs=[diagnostic_input, exercise_input], outputs=output)\n",
    "\n",
    "# # Compilar el modelo\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Recommender System with Surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from surprise import Reader\n",
    "# from surprise import Dataset\n",
    "# from surprise.model_selection import cross_validate\n",
    "# from surprise import NormalPredictor\n",
    "# from surprise import KNNBasic\n",
    "# from surprise import KNNWithMeans\n",
    "# from surprise import KNNWithZScore\n",
    "# from surprise import KNNBaseline\n",
    "# from surprise import SVD\n",
    "# from surprise import BaselineOnly\n",
    "# from surprise import SVDpp\n",
    "# from surprise import NMF\n",
    "# from surprise import SlopeOne\n",
    "# from surprise import CoClustering\n",
    "# from surprise.accuracy import rmse\n",
    "# from surprise import accuracy\n",
    "# from surprise.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# user = pd.read_csv('BX-Users.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n",
    "# user.columns = ['userID', 'Location', 'Age']\n",
    "# rating = pd.read_csv('BX-Book-Ratings.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n",
    "# rating.columns = ['userID', 'ISBN', 'bookRating']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# user.head()\n",
    "# rating.head()\n",
    "\n",
    "\n",
    "# df = pd.merge(user, rating, on='userID', how='inner')\n",
    "# df.drop(['Location', 'Age'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# df.head()\n",
    "# df.shape\n",
    "# df.info()\n",
    "\n",
    "\n",
    "# print('Dataset shape: {}'.format(df.shape))\n",
    "# print('-Dataset examples-')\n",
    "# print(df.iloc[::200000, :])\n",
    "\n",
    "\n",
    "\n",
    "# from plotly.offline import init_notebook_mode, plot, iplot\n",
    "# import plotly.graph_objs as go\n",
    "# init_notebook_mode(connected=True)\n",
    "\n",
    "# data = df['bookRating'].value_counts().sort_index(ascending=False)\n",
    "# trace = go.Bar(x = data.index,\n",
    "#                text = ['{:.1f} %'.format(val) for val in (data.values / df.shape[0] * 100)],\n",
    "#                textposition = 'auto',\n",
    "#                textfont = dict(color = '#000000'),\n",
    "#                y = data.values,\n",
    "#                )\n",
    "# # Create layout\n",
    "# layout = dict(title = 'Distribution Of {} book-ratings'.format(df.shape[0]),\n",
    "#               xaxis = dict(title = 'Rating'),\n",
    "#               yaxis = dict(title = 'Count'))\n",
    "# # Create plot\n",
    "# fig = go.Figure(data=[trace], layout=layout)\n",
    "# iplot(fig)\n",
    "\n",
    "\n",
    "\n",
    "# # Number of ratings per book\n",
    "# data = df.groupby('ISBN')['bookRating'].count().clip(upper=50)\n",
    "\n",
    "# # Create trace\n",
    "# trace = go.Histogram(x = data.values,\n",
    "#                      name = 'Ratings',\n",
    "#                      xbins = dict(start = 0,\n",
    "#                                   end = 50,\n",
    "#                                   size = 2))\n",
    "# # Create layout\n",
    "# layout = go.Layout(title = 'Distribution Of Number of Ratings Per Book (Clipped at 50)',\n",
    "#                    xaxis = dict(title = 'Number of Ratings Per Book'),\n",
    "#                    yaxis = dict(title = 'Count'),\n",
    "#                    bargap = 0.2)\n",
    "\n",
    "# # Create plot\n",
    "# fig = go.Figure(data=[trace], layout=layout)\n",
    "# iplot(fig)\n",
    "\n",
    "\n",
    "# df.groupby('ISBN')['bookRating'].count().reset_index().sort_values('bookRating', ascending=False)[:10]\n",
    "\n",
    "\n",
    "\n",
    "# # Number of ratings per user\n",
    "# data = df.groupby('userID')['bookRating'].count().clip(upper=50)\n",
    "\n",
    "# # Create trace\n",
    "# trace = go.Histogram(x = data.values,\n",
    "#                      name = 'Ratings',\n",
    "#                      xbins = dict(start = 0,\n",
    "#                                   end = 50,\n",
    "#                                   size = 2))\n",
    "# # Create layout\n",
    "# layout = go.Layout(title = 'Distribution Of Number of Ratings Per User (Clipped at 50)',\n",
    "#                    xaxis = dict(title = 'Ratings Per User'),\n",
    "#                    yaxis = dict(title = 'Count'),\n",
    "#                    bargap = 0.2)\n",
    "\n",
    "# # Create plot\n",
    "# fig = go.Figure(data=[trace], layout=layout)\n",
    "# iplot(fig)\n",
    "\n",
    "\n",
    "\n",
    "# df.groupby('userID')['bookRating'].count().reset_index().sort_values('bookRating', ascending=False)[:10]\n",
    "\n",
    "\n",
    "# min_book_ratings = 50\n",
    "# filter_books = df['ISBN'].value_counts() > min_book_ratings\n",
    "# filter_books = filter_books[filter_books].index.tolist()\n",
    "\n",
    "# min_user_ratings = 50\n",
    "# filter_users = df['userID'].value_counts() > min_user_ratings\n",
    "# filter_users = filter_users[filter_users].index.tolist()\n",
    "\n",
    "# df_new = df[(df['ISBN'].isin(filter_books)) & (df['userID'].isin(filter_users))]\n",
    "# print('The original data frame shape:\\t{}'.format(df.shape))\n",
    "# print('The new data frame shape:\\t{}'.format(df_new.shape))\n",
    "\n",
    "\n",
    "\n",
    "# reader = Reader(rating_scale=(0, 9))\n",
    "# data = Dataset.load_from_df(df_new[['userID', 'ISBN', 'bookRating']], reader)\n",
    "\n",
    "\n",
    "# benchmark = []\n",
    "# # Iterate over all algorithms\n",
    "# for algorithm in [SVD(), SVDpp(), SlopeOne(), NMF(), NormalPredictor(), KNNBaseline(), KNNBasic(), KNNWithMeans(), KNNWithZScore(), BaselineOnly(), CoClustering()]:\n",
    "#     # Perform cross validation\n",
    "#     results = cross_validate(algorithm, data, measures=['RMSE'], cv=3, verbose=False)\n",
    "    \n",
    "#     # Get results & append algorithm name\n",
    "#     tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n",
    "#     tmp = tmp.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm']))\n",
    "#     benchmark.append(tmp)\n",
    "\n",
    "\n",
    "\n",
    "# surprise_results = pd.DataFrame(benchmark).set_index('Algorithm').sort_values('test_rmse')\n",
    "# surprise_results\n",
    "\n",
    "\n",
    "\n",
    "# print('Using ALS')\n",
    "# bsl_options = {'method': 'als',\n",
    "#                'n_epochs': 5,\n",
    "#                'reg_u': 12,\n",
    "#                'reg_i': 5\n",
    "#                }\n",
    "# algo = BaselineOnly(bsl_options=bsl_options)\n",
    "# cross_validate(algo, data, measures=['RMSE'], cv=3, verbose=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# trainset, testset = train_test_split(data, test_size=0.25)\n",
    "# algo = BaselineOnly(bsl_options=bsl_options)\n",
    "# predictions = algo.fit(trainset).test(testset)\n",
    "# accuracy.rmse(predictions)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# trainset = algo.trainset\n",
    "# print(algo.__class__.__name__)\n",
    "\n",
    "\n",
    "# def get_Iu(uid):\n",
    "#     \"\"\" return the number of items rated by given user\n",
    "#     args: \n",
    "#       uid: the id of the user\n",
    "#     returns: \n",
    "#       the number of items rated by the user\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         return len(trainset.ur[trainset.to_inner_uid(uid)])\n",
    "#     except ValueError: # user was not part of the trainset\n",
    "#         return 0\n",
    "    \n",
    "# def get_Ui(iid):\n",
    "#     \"\"\" return number of users that have rated given item\n",
    "#     args:\n",
    "#       iid: the raw id of the item\n",
    "#     returns:\n",
    "#       the number of users that have rated the item.\n",
    "#     \"\"\"\n",
    "#     try: \n",
    "#         return len(trainset.ir[trainset.to_inner_iid(iid)])\n",
    "#     except ValueError:\n",
    "#         return 0\n",
    "    \n",
    "# df = pd.DataFrame(predictions, columns=['uid', 'iid', 'rui', 'est', 'details'])\n",
    "# df['Iu'] = df.uid.apply(get_Iu)\n",
    "# df['Ui'] = df.iid.apply(get_Ui)\n",
    "# df['err'] = abs(df.est - df.rui)\n",
    "\n",
    "\n",
    "\n",
    "# df.head()\n",
    "# best_predictions = df.sort_values(by='err')[:10]\n",
    "# worst_predictions = df.sort_values(by='err')[-10:]\n",
    "# best_predictions\n",
    "# worst_predictions\n",
    "\n",
    "# df_new.loc[df_new['ISBN'] == '055358264X']['bookRating'].describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONLINE RETAIL IMPLICIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import scipy.sparse as sparse\n",
    "# from scipy.sparse.linalg import spsolve\n",
    "# import random\n",
    "# from sklearn import metrics\n",
    "\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# import implicit\n",
    "\n",
    "\n",
    "# retail_df = pd.read_excel('data/Online Retail.xlsx')\n",
    "# retail_df.head()\n",
    "# retail_df.info()\n",
    "\n",
    "\n",
    "# retail_df = retail_df[retail_df['CustomerID'].notna()]\n",
    "# retail_df.info()\n",
    "\n",
    "\n",
    "# grouped_df = retail_df[['CustomerID', 'StockCode', 'Description', 'Quantity']].groupby(['CustomerID', 'StockCode', 'Description']).sum().reset_index()\n",
    "# grouped_df.loc[grouped_df['Quantity'] == 0, ['Quantity']] = 1\n",
    "# grouped_df = grouped_df.loc[grouped_df['Quantity'] > 0]\n",
    "\n",
    "\n",
    "# grouped_df.head()\n",
    "# grouped_df.Quantity.describe()\n",
    "\n",
    "\n",
    "# import plotly.express as px\n",
    "\n",
    "# fig = px.histogram(grouped_df, x='Quantity', title='Distribution of the purchase quantity', nbins=500)\n",
    "# fig.show();\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f'Number of unique customers: {grouped_df.CustomerID.nunique()}')\n",
    "# print(f'Number of unique items: {grouped_df.StockCode.nunique()}')\n",
    "\n",
    "# print(f'Average purchase quantity per interaction: {int(grouped_df.Quantity.mean())}')\n",
    "# print(f'Minimum purchase quantity per interaction: {grouped_df.Quantity.min()}')\n",
    "# print(f'Maximum purchase quantity per interaction: {grouped_df.Quantity.max()}')\n",
    "\n",
    "\n",
    "# unique_customers = grouped_df.CustomerID.unique()\n",
    "# customer_ids = dict(zip(unique_customers, np.arange(unique_customers.shape[0], dtype=np.int32)))\n",
    "\n",
    "# unique_items = grouped_df.StockCode.unique()\n",
    "# item_ids = dict(zip(unique_items, np.arange(unique_items.shape[0], dtype=np.int32)))\n",
    "\n",
    "# grouped_df['customer_id'] = grouped_df.CustomerID.apply(lambda i: customer_ids[i])\n",
    "# grouped_df['item_id'] = grouped_df.StockCode.apply(lambda i: item_ids[i])\n",
    "\n",
    "# grouped_df.head()\n",
    "\n",
    "\n",
    "# sparse_item_customer = sparse.csr_matrix((grouped_df['Quantity'].astype(float), (grouped_df['item_id'], grouped_df['customer_id'])))\n",
    "# sparse_customer_item = sparse.csr_matrix((grouped_df['Quantity'].astype(float), (grouped_df['customer_id'], grouped_df['item_id'])))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# sparse_item_customer\n",
    "# sparse_customer_item\n",
    "# model = implicit.als.AlternatingLeastSquares(factors=20, regularization=0.1, iterations=50)\n",
    "\n",
    "# alpha = 15\n",
    "# data = (sparse_item_customer * alpha).astype('double')\n",
    "\n",
    "# model.fit(data)\n",
    "# grouped_df.loc[grouped_df['item_id'] == 1319].head()\n",
    "\n",
    "# item_id = 1319\n",
    "# n_similar = 10\n",
    "\n",
    "# item_vecs = model.item_factors\n",
    "# customer_vecs = model.user_factors\n",
    "\n",
    "# item_norms = np.sqrt((item_vecs * item_vecs).sum(axis=1))\n",
    "\n",
    "# scores = item_vecs.dot(item_vecs[item_id]) / item_norms\n",
    "# top_idx = np.argpartition(scores, -n_similar)[-n_similar:]\n",
    "# similar = sorted(zip(top_idx, scores[top_idx] / item_norms[item_id]), key=lambda x: -x[1])\n",
    "\n",
    "\n",
    "# for item in similar:\n",
    "#     idx, score = item\n",
    "#     print(grouped_df.Description.loc[grouped_df.item_id == idx].iloc[0])\n",
    "\n",
    "\n",
    "\n",
    "# def recommend(customer_id, sparse_customer_item, customer_vecs, item_vecs, num_items=10):\n",
    "    \n",
    "#     customer_interactions = sparse_customer_item[customer_id,:].toarray()\n",
    "#     customer_interactions = customer_interactions.reshape(-1) + 1\n",
    "#     customer_interactions[customer_interactions > 1] = 0\n",
    "    \n",
    "#     rec_vector = customer_vecs[customer_id,:].dot(item_vecs.T).toarray()\n",
    "    \n",
    "#     min_max = MinMaxScaler()\n",
    "#     rec_vector_scaled = min_max.fit_transform(rec_vector.reshape(-1,1))[:,0]\n",
    "#     recommend_vector = customer_interactions * rec_vector_scaled\n",
    "\n",
    "#     item_idx = np.argsort(recommend_vector)[::-1][:num_items]\n",
    "    \n",
    "#     descriptions = []\n",
    "#     scores = []\n",
    "\n",
    "#     for idx in item_idx:\n",
    "#         descriptions.append(grouped_df.Description.loc[grouped_df.item_id == idx].iloc[0])\n",
    "#         scores.append(recommend_vector[idx])\n",
    "\n",
    "#     recommendations = pd.DataFrame({'description': descriptions, 'score': scores})\n",
    "\n",
    "#     return recommendations\n",
    "\n",
    "\n",
    "# customer_vecs = sparse.csr_matrix(model.user_factors)\n",
    "# item_vecs = sparse.csr_matrix(model.item_factors)\n",
    "\n",
    "# # Create recommendations for customer with id 2\n",
    "# customer_id = 2\n",
    "\n",
    "# recommendations = recommend(customer_id, sparse_customer_item, customer_vecs, item_vecs)\n",
    "\n",
    "# print(recommendations)\n",
    "\n",
    "# grouped_df.loc[grouped_df['customer_id'] == 2].sort_values('Quantity', ascending=False)[['customer_id', 'Description', 'Quantity']].head(20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import random\n",
    "\n",
    "# def make_train(ratings, pct_test = 0.2):\n",
    "#     test_set = ratings.copy() # Make a copy of the original set to be the test set. \n",
    "#     test_set[test_set != 0] = 1 # Store the test set as a binary preference matrix\n",
    "    \n",
    "#     training_set = ratings.copy() # Make a copy of the original data we can alter as our training set. \n",
    "    \n",
    "#     nonzero_inds = training_set.nonzero() # Find the indices in the ratings data where an interaction exists\n",
    "#     nonzero_pairs = list(zip(nonzero_inds[0], nonzero_inds[1])) # Zip these pairs together of item,user index into list\n",
    "\n",
    "    \n",
    "#     random.seed(0) # Set the random seed to zero for reproducibility\n",
    "    \n",
    "#     num_samples = int(np.ceil(pct_test*len(nonzero_pairs))) # Round the number of samples needed to the nearest integer\n",
    "#     samples = random.sample(nonzero_pairs, num_samples) # Sample a random number of item-user pairs without replacement\n",
    "\n",
    "#     item_inds = [index[0] for index in samples] # Get the item row indices\n",
    "\n",
    "#     customer_inds = [index[1] for index in samples] # Get the user column indices\n",
    "\n",
    "    \n",
    "#     training_set[item_inds, customer_inds] = 0 # Assign all of the randomly chosen user-item pairs to zero\n",
    "#     training_set.eliminate_zeros() # Get rid of zeros in sparse array storage after update to save space\n",
    "    \n",
    "#     return training_set, test_set, list(set(customer_inds))\n",
    "\n",
    "\n",
    "\n",
    "# item_train, item_test, item_customers_altered = make_train(sparse_item_customer, pct_test = 0.2)\n",
    "\n",
    "\n",
    "# def auc_score(predictions, test):\n",
    "#     fpr, tpr, thresholds = metrics.roc_curve(test, predictions)\n",
    "#     return metrics.auc(fpr, tpr)\n",
    "\n",
    "\n",
    "# def calc_mean_auc(training_set, altered_customers, predictions, test_set):\n",
    "#     store_auc = [] # An empty list to store the AUC for each user that had an item removed from the training set\n",
    "#     popularity_auc = [] # To store popular AUC scores\n",
    "#     pop_items = np.array(test_set.sum(axis = 1)).reshape(-1) # Get sum of item iteractions to find most popular\n",
    "#     item_vecs = predictions[1]\n",
    "#     for customer in altered_customers: # Iterate through each user that had an item altered\n",
    "#         training_column = training_set[:,customer].toarray().reshape(-1) # Get the training set column\n",
    "#         zero_inds = np.where(training_column == 0) # Find where the interaction had not yet occurred\n",
    "        \n",
    "#         # Get the predicted values based on our user/item vectors\n",
    "#         customer_vec = predictions[0][customer,:]\n",
    "#         pred = customer_vec.dot(item_vecs).toarray()[0,zero_inds].reshape(-1)\n",
    "        \n",
    "#         # Get only the items that were originally zero\n",
    "#         # Select all ratings from the MF prediction for this user that originally had no iteraction\n",
    "#         actual = test_set[:,customer].toarray()[zero_inds,0].reshape(-1)\n",
    "        \n",
    "#         # Select the binarized yes/no interaction pairs from the original full data\n",
    "#         # that align with the same pairs in training \n",
    "#         pop = pop_items[zero_inds] # Get the item popularity for our chosen items\n",
    "        \n",
    "#         store_auc.append(auc_score(pred, actual)) # Calculate AUC for the given user and store\n",
    "        \n",
    "#         popularity_auc.append(auc_score(pop, actual)) # Calculate AUC using most popular and score\n",
    "#     # End users iteration\n",
    "    \n",
    "#     return float('%.3f'%np.mean(store_auc)), float('%.3f'%np.mean(popularity_auc))\n",
    "\n",
    "\n",
    "# calc_mean_auc(item_train, item_customers_altered,\n",
    "#               [customer_vecs, item_vecs.T], item_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".sistema-de-recomendacion-unab-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
