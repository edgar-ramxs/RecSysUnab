Los sistemas de recomendación (RSs) se han convertido en una parte inseparable de nuestra vida cotidiana. Nos ayudan a encontrar nuestros artículos favoritos para comprar, nuestros amigos en redes sociales y nuestras películas favoritas para ver. Tradicionalmente, el problema de la recomendación se consideraba un problema de clasificación o predicción, pero ahora está ampliamente aceptado que formularlo como un problema de decisión secuencial puede reflejar mejor la interacción usuario-sistema. Por lo tanto, puede formularse como un proceso de decisión de Markov (MDP) y resolverse mediante algoritmos de aprendizaje por refuerzo (RL). A diferencia de los métodos tradicionales de recomendación, como el filtrado colaborativo y el basado en contenido, RL puede manejar la interacción usuario-sistema secuencial y dinámica, y tener en cuenta el compromiso a largo plazo del usuario. Aunque la idea de utilizar RL para la recomendación no es nueva y ha existido durante aproximadamente dos décadas, no era muy práctica, principalmente debido a problemas de escalabilidad de los algoritmos de RL tradicionales. Sin embargo, desde la introducción del aprendizaje por refuerzo profundo (DRL), ha surgido una nueva tendencia en el campo que ha hecho posible aplicar RL al problema de la recomendación con espacios de estados y acciones grandes. En este artículo, se presenta una revisión de los sistemas de recomendación basados en aprendizaje por refuerzo (RLRSs). Nuestro objetivo es ofrecer una perspectiva sobre el campo y proporcionar al lector un conocimiento bastante completo de los conceptos clave del mismo. Primero reconocemos e ilustramos que los RLRSs se pueden clasificar generalmente en métodos basados en RL y DRL. Luego, proponemos un marco de trabajo para RLRS con cuatro componentes, es decir, representación del estado, optimización de la política, formulación de la recompensa y construcción del entorno, y revisamos los algoritmos RLRS en consecuencia. Destacamos temas emergentes y representamos tendencias importantes mediante diversos gráficos y tablas. Finalmente, discutimos aspectos importantes y desafíos que pueden abordarse en el futuro.

1. INTRODUCCIÓN
Estamos viviendo en la Era del Zettabyte [1]. El volumen masivo de información disponible en la web conduce al problema de la sobrecarga de información, lo cual dificulta que un tomador de decisiones pueda tomar decisiones correctas. Nos damos cuenta de esto en nuestra vida cotidiana cuando nos enfrentamos a una larga lista de artículos en una tienda en línea; cuantos más artículos en la lista, más difícil se vuelve seleccionar entre ellos. Los sistemas de recomendación (RSs) son herramientas de software y algoritmos que se han desarrollado con la idea de ayudar a los usuarios a encontrar sus artículos de interés, prediciendo sus preferencias o valoraciones sobre los artículos [2, 3]. De hecho, la idea es conocer a los usuarios hasta cierto punto, es decir, crear un perfil de usuario basado en sus retroalimentaciones sobre los artículos, y recomendar aquellos artículos que coincidan con su perfil. Hoy en día, los RSs son parte esencial de la mayoría de las grandes compañías, como Google, Facebook, Amazon y Netflix, y se emplean en una amplia gama de aplicaciones, incluyendo entretenimiento [4–6], comercio electrónico [7], noticias [8], e-learning [9] y cuidado de la salud [10].

Se han propuesto numerosas técnicas para abordar el problema de la recomendación; las técnicas tradicionales incluyen el filtrado colaborativo, el filtrado basado en contenido y métodos híbridos. A pesar de cierto éxito en proporcionar recomendaciones relevantes, especialmente después de la introducción de la factorización de matrices [11], estos métodos tienen problemas severos, como el inicio en frío (es decir, el sistema no puede proporcionar recomendaciones útiles cuando el usuario o el artículo son nuevos), falta de novedad y diversidad, escalabilidad, recomendaciones de baja calidad y gran gasto computacional [2, 3, 12]. Recientemente, el aprendizaje profundo [13] también ha ganado popularidad en el campo de los RSs debido a su capacidad para encontrar relaciones complejas y no lineales entre usuarios y artículos, y su rendimiento de vanguardia en la recomendación [14]. Sin embargo, los modelos de aprendizaje profundo suelen ser no interpretables, requieren una gran cantidad de datos (esto es especialmente problemático dado que la cantidad de datos, es decir, retroalimentación de usuarios/calificaciones, en el campo de los RSs es escasa) y son computacionalmente costosos [14].

RL es un campo de aprendizaje automático semi-supervisado en el que el agente optimiza su comportamiento mediante la interacción con el entorno. El hito en el campo de RL es la combinación del aprendizaje profundo con métodos tradicionales de RL, conocida como aprendizaje por refuerzo profundo (DRL) [15, 16]. Esto hizo posible aplicar RL en problemas con enormes espacios de estados y acciones, incluyendo autos autónomos [17, 18], robótica [19], automatización industrial [20], finanzas [21], salud [22, 23] y RSs [24]. La capacidad única de un agente de RL para aprender a partir de una recompensa del entorno sin necesidad de datos de entrenamiento hace que RL sea específicamente adecuado para el problema de la recomendación. Hoy en día, cada vez más empresas están utilizando el poder de RL para recomendar mejores artículos a sus clientes. Por ejemplo, en un estudio realizado por investigadores de Google [25], se muestra que RL puede utilizarse para recomendar mejor contenido de video a los usuarios de YouTube. De hecho, el uso de RL en la comunidad de RSs no se limita a la industria, sino que también se está convirtiendo en una tendencia en el ámbito académico. La Fig. 1(a) ilustra esta tendencia.

Esta tendencia y este tema nos motivaron a preparar este artículo de revisión, que tiene como objetivo proporcionar una visión general completa del estado del arte en sistemas de recomendación basados en aprendizaje por refuerzo (RLRSs). Nuestro propósito principal es ofrecer una imagen de alto nivel desde el progreso en el campo desde sus inicios y mostrar cómo esta tendencia ha cambiado significativamente con la llegada del DRL. Al mismo tiempo, proporcionamos información detallada sobre cada método en forma de tablas para que el lector pueda observar fácilmente las similitudes y diferencias entre los métodos.

Metodología de Recolección de Artículos. Para recolectar los artículos relevantes, hemos utilizado un proceso de búsqueda multinivel. El enfoque de este artículo de revisión se centra específicamente en RSs que utilizan un algoritmo de RL. En consecuencia, para encontrar artículos relevantes, utilizamos Google Scholar como motor de búsqueda principal y buscamos la palabra clave "reinforcement learning recommender system". Esta búsqueda arrojó alrededor de 33,000 artículos. De los primeros 1000 artículos encontrados, recolectamos 500 como resultado de nuestro primer nivel de cribado. Luego, para aumentar la fiabilidad de nuestra colección de artículos, exploramos también bibliotecas relacionadas como ACM Digital Library, IEEE Xplore, SpringerLink y ScienceDirect con la misma palabra clave, hasta el punto en que no encontramos más artículos relevantes entre los resultados. Descubrimos que todos los artículos relacionados identificados en estas bibliotecas estaban disponibles en nuestra búsqueda inicial utilizando Google Scholar. Después de estudiar cuidadosamente los artículos recolectados y excluir los que eran irrelevantes, duplicados, tesis, y artículos de revisión, seleccionamos 97 artículos para incluir en nuestro artículo de revisión. Aunque estamos seguros de que no encontramos todos los RLRSs a través del proceso de búsqueda explicado, confiamos en haber encontrado la gran mayoría de las publicaciones relevantes.

Es importante mencionar que no incluimos RSs basados en bandas multi-brazo. Los bandas son una versión simplificada de RL. En particular, en las bandas, al igual que en un problema de RL, el agente debe aprender a maximizar una recompensa numérica a través de la interacción con el entorno y resolver el dilema de exploración versus explotación [26, 27]. Sin embargo, en las bandas, a diferencia del RL completo, las acciones no afectan el estado del entorno ni la recompensa [27]. Aunque las bandas han sido populares para el problema de la recomendación [28–31], dada las aplicaciones exitosas recientes de DRL y el interés sin precedentes en RL completo por parte de la comunidad de RSs, optamos por enfocarnos únicamente en RSs que utilizan un algoritmo de RL completo. Se recomienda al lector interesado consultar una revisión reciente sobre la aplicación de bandas en RSs [32].

Trabajo Relacionado. Se ha realizado una gran cantidad de investigación en el campo de los RSs y se han publicado numerosos artículos de revisión, incluyendo RSs [12], filtrado colaborativo [33, 34], métodos híbridos [35], RSs multimedia [36, 37], recomendación explicativa [38] y RSs de artículos [39], entre otros. También existen algunos artículos de revisión publicados sobre temas estrechamente relacionados con RLRSs [14, 40–42]. Quizás los dos artículos de revisión más cercanos al nuestro sean [14, 40]. La Ref. [40] revisa técnicas basadas en DRL para la búsqueda de información, como la búsqueda, la recomendación y la publicidad en línea. Los autores discuten varios RSs que utilizan bandas multi-brazo y DRL para la optimización de políticas. Sin embargo, este trabajo omite muchos RLRSs importantes y no proporciona un análisis en profundidad de los algoritmos revisados. Zhang et al. [14] ofrecen una revisión exhaustiva de los RSs basados en aprendizaje profundo. Consideran DRL como un paradigma arquitectónico de aprendizaje profundo y revisan algunos RSs basados en DRL [43–48]. Sin embargo, esta clasificación no es correcta, ya que DRL no es una arquitectura de aprendizaje profundo, sino una extensión de los algoritmos de RL tradicionales. Otras revisiones relacionadas se centran en técnicas de recomendación sensibles a la secuencia [41] y basadas en sesiones [42]. La Ref. [41] revisa RSs sensibles a la secuencia. Los autores consideran RL como un método para el aprendizaje de secuencias y revisan algunos RLRSs [49, 50]. En otra revisión relacionada [42], RL se considera como un método para RSs basados en sesiones [46, 51, 52]. Ninguno de los artículos de revisión previamente publicados proporciona una visión general completa y un análisis en profundidad de los RLRSs publicados. Hasta donde llega nuestro conocimiento, este es el primer artículo de revisión que se centra específicamente en RLRSs.

Nuestra contribución. El objetivo es proporcionar al lector una vista hacia el campo para que puedan entender rápidamente el tema y las principales tendencias y algoritmos presentados hasta ahora. Esto ayuda a los investigadores a tener una visión general, comparar las fortalezas y debilidades de los algoritmos, y arrojar luz sobre formas de avanzar en el futuro. Nuestras principales contribuciones se pueden resumir como:

Presentación de un marco para RLRSs. Primero dividimos generalmente los RLRSs en métodos basados en RL y DRL. Luego, proponemos un marco con cuatro componentes principales, es decir, representación del estado, optimización de la política, formulación de la recompensa y construcción del entorno. Este marco puede modelar todos los RLRSs y unificar el proceso de desarrollo de los RLRSs.

El resto de este artículo está organizado de la siguiente manera. En la sección 2, para ayudar al lector a entender mejor el tema, discutimos algunos conceptos preliminares y proporcionamos un sólido contexto sobre RL. La sección 3 presenta los algoritmos de RLRSs de manera clasificada. Los temas emergentes se destacan en la sección 4. En la sección 5 se sugieren algunas direcciones de investigación abiertas para el trabajo futuro, y finalmente, el artículo concluye en la sección 6.

**2 PRELIMINARES**

En esta sección, ofrecemos un contexto sobre los conceptos importantes discutidos a lo largo de este artículo. Este trasfondo proporciona al lector información útil y concisa sobre los RSs, RL y DRL, por qué es necesario usar RL en RSs, la formulación del problema y el marco propuesto para los RLRSs.

**2.1 Sistemas de Recomendación**

En la vida cotidiana, no es raro enfrentarse a situaciones en las que debemos tomar decisiones sin tener información previa sobre las opciones. En tales casos, parece bastante necesario depender de recomendaciones de otros, que tienen experiencia en ese aspecto [53]. Esta fue la razón detrás del primer RS, Tapestry [54], y los autores lo denominaron filtrado colaborativo. Más tarde, este término se amplió a sistemas de recomendación para reflejar dos hechos [53]: 1) el método puede no basarse en una colaboración implícita entre usuarios, 2) el método puede sugerir elementos interesantes, no solo filtrarlos. Por definición, los RSs son herramientas de software y algoritmos que sugieren elementos que podrían interesar a los usuarios [3].

Otro enfoque importante hacia el problema de la recomendación es el filtrado basado en contenido, en el cual la idea es utilizar descripciones de elementos y diseñar un método para relacionarlos con el perfil del usuario, una representación estructurada de los intereses del usuario [55, 56]. El filtrado colaborativo suele sufrir de esparcimiento de datos, escalabilidad y ovejas grises (usuarios con gustos especiales cuyas opiniones no coinciden o discrepan con la mayoría de los usuarios) [33]. El filtrado basado en contenido también tiene algunas limitaciones, incluyendo análisis de contenido limitado, serendipia y nuevos usuarios [56]. Los métodos híbridos, una combinación de ambos, pueden mitigar solo parte de estos problemas [3, 33].

**2.2 Desde el Aprendizaje por Refuerzo hasta el Aprendizaje Profundo por Refuerzo**

El aprendizaje por refuerzo (RL) es un campo de aprendizaje automático que estudia problemas y sus soluciones en los cuales los agentes, a través de la interacción con su entorno, aprenden a maximizar una recompensa numérica. Según Sutton y Barto [27], tres características distinguen un problema de RL: (1) el problema es de lazo cerrado, (2) el aprendiz no tiene un tutor que le enseñe qué hacer, sino que debe descubrir qué hacer a través de prueba y error, y (3) las acciones influyen no solo en los resultados a corto plazo, sino también en los resultados a largo plazo. La interfaz más común para modelar un problema de RL es la interfaz agente-entorno, representada en la Fig. 2(a). El aprendiz o tomador de decisiones se llama agente y el entorno es todo lo que está fuera del agente. De acuerdo con esto, en el paso de tiempo 𝑡, el agente ve algunas representaciones/información sobre el entorno, llamadas estado, y basado en el estado actual toma una acción. Al tomar esta acción, recibe una recompensa numérica del entorno y se encuentra en un nuevo estado.

Más formalmente, el problema de RL se formula típicamente como un proceso de decisión de Markov (MDP) en forma de una tupla (S, A, R, P, 𝛾), donde S es el conjunto de todos los estados posibles, A es el conjunto de acciones disponibles en todos los estados, R es la función de recompensa, P es la probabilidad de transición, y 𝛾 es el factor de descuento.

Los principales elementos de un sistema de RL son [27]:
- **Política**: la política generalmente se indica por 𝜋 y da la probabilidad de tomar la acción 𝑎 cuando el agente está en el estado 𝑠. Con respecto a la política, los algoritmos de RL generalmente se dividen en métodos on-policy y off-policy. En los primeros, los métodos de RL tienen como objetivo evaluar o mejorar la política que están utilizando para tomar decisiones. En los últimos, mejoran o evalúan una política que es diferente de la que se utilizó para generar los datos.
- **Señal de recompensa**: al seleccionar acciones, el entorno proporciona una recompensa numérica para informar al agente qué tan buenas o malas son las acciones seleccionadas.
- **Función de valor**: la señal de recompensa simplemente puede decir qué es bueno inmediatamente, pero la función de valor define qué es bueno a largo plazo.
- **Modelo**: el modelo proporciona la oportunidad de hacer inferencias sobre el comportamiento del entorno. Por ejemplo, el modelo puede predecir el próximo estado y la próxima recompensa en un estado y acción dados [27].

**Algoritmos**: Se han propuesto muchos algoritmos para resolver un problema de RL; generalmente se pueden dividir en métodos tabulares y métodos aproximados [27]. En los métodos tabulares, dado que el tamaño de los espacios de acción y estado es pequeño, las funciones de valor se pueden representar como tablas y se puede encontrar una función de valor y política óptimas. Por otro lado, en los métodos aproximados, dado que el tamaño del espacio de estados es enorme, el objetivo es encontrar una buena solución aproximada con la restricción de recursos computacionales limitados. Como se mencionó anteriormente, con la base de DRL, ha surgido un cambio sustancial en el campo de RL en general. En consecuencia, aunque DRL pertenece al grupo aproximado, generalmente dividimos los algoritmos de RL utilizados por los RLRSs en algoritmos basados en RL y basados en DRL, ya que creemos que esta clasificación refleja mejor la tendencia reciente en el campo de los RLRSs. Es digno de mencionar que el factor distintivo entre los algoritmos DRL y los algoritmos tradicionales de RL es que los algoritmos DRL utilizan aprendizaje profundo para la aproximación de funciones (se presenta una explicación más detallada sobre esto en la sección de Algoritmos basados en DRL). A continuación, revisamos brevemente esos algoritmos de RL empleados por los RLRSs. La Fig. 2(b) ilustra estos algoritmos.

1) **Algoritmos basados en RL**: Como se mencionó antes, los algoritmos de RL se pueden dividir en métodos tabulares y métodos aproximados. Los métodos tabulares populares incluyen programación dinámica, Monte Carlo y diferencia temporal. Los métodos de programación dinámica asumen un modelo perfecto del entorno y utilizan una función de valor para buscar buenas políticas. Dos algoritmos importantes de esta clase son la iteración de política y la iteración de valor. El algoritmo de iteración de política consta de tres pasos: inicialización, evaluación de la política y mejora de la política. Primero, la política se inicializa al azar, es decir, se selecciona una acción aleatoria 𝑎 ∈ 𝐴(𝑠) para todos los 𝑠 ∈ S. Luego, se calcula y evalúa el valor de los estados utilizando

\[ 𝑉 (𝑠) ← \sum_{𝑠′,𝑟} 𝑝(𝑠′,𝑟|𝑠,𝜋(𝑠))[𝑟 +𝛾𝑉 (𝑠′)], \]

donde 𝑝 es la probabilidad de transición y 𝑠′ es el próximo estado. Finalmente, la política se actualiza de la siguiente manera, ∀𝑠 ∈ S:

\[ 𝜋(𝑠) ← \arg \max_{𝑎} \sum_{𝑠′,𝑟} 𝑝(𝑠′,𝑟|𝑠,𝑎)[𝑟 +𝛾𝑉 (𝑠′)]. \]

Como se señala en [27], un problema con el algoritmo de iteración de política es que necesita evaluar la política en cada iteración, lo cual puede ser computacionalmente prohibitivo. El algoritmo de iteración de valor es un caso especial del algoritmo de iteración de política en el que la evaluación de la política se detiene después de un barrido. Más precisamente, 𝑉 (𝑠) se inicializa al azar ∀𝑠 ∈ S. Luego, se actualiza en cada paso según

3.6 Algoritmos basados en DRL
Los algoritmos basados en DRL representan una interesante combinación de aprendizaje profundo con RL. De hecho, investigadores de DeepMind descubrieron que esta combinación puede lograr un rendimiento a nivel humano en juegos de Atari [67, 68]. La red neuronal convolucional profunda Q-network (DQN) [67] es una combinación creativa de redes neuronales convolucionales (CNN) [13] con Q-learning. Más precisamente, en DQN, una red Q se encarga de la aproximación del valor de acción, la cual puede entrenarse para minimizar la siguiente función de pérdida:

\[ L_i(\theta_i) = \mathbb{E}_{s,a \sim \rho(\cdot)} \left[ \left( y_i - Q(s,a;\theta_i) \right)^2 \right], \]

donde \( y_i = \mathbb{E}_{s'} \left[ r + \gamma \max_{a'} Q(s',a';\theta_{i-1}) \mid s,a \right] \) es el objetivo para la iteración \( i \) y \( \rho \) es una distribución de probabilidad sobre transiciones \( s,a,r,s' \) recolectadas del entorno. Diferenciando \( L(\theta) \) en Eq. (14) con respecto a \( \theta \) se obtiene el siguiente gradiente:

\[ \nabla_{\theta_i} L_i(\theta_i) = \mathbb{E}_{s,a \sim \rho(\cdot)} \left[ \left( r + \gamma \max_{a'} Q(s',a';\theta_{i-1}) - Q(s,a;\theta_i) \right) \nabla_{\theta_i} Q(s,a;\theta_i) \right]. \]

Es computacionalmente beneficioso optimizar el gradiente en Eq. (15) utilizando descenso de gradiente estocástico [68]. Según [27], DQN modifica el algoritmo original de Q-learning de tres maneras: 1) utiliza replay de experiencias, propuesto por primera vez en [69], que mantiene las experiencias de los agentes durante varios pasos de tiempo en una memoria de repetición y las utiliza para actualizar los pesos en la fase de entrenamiento. 2) Para reducir la complejidad en la actualización de pesos, los pesos actualizados actuales se mantienen fijos y se introducen en una segunda red (duplicada) cuyas salidas se utilizan como objetivos de Q-learning. 3) El término de error (r + \gamma \max_{a'} Q(s',a';\theta_{i-1}) - Q(s,a;\theta_i) en la Eq. (15) se recorta de modo que permanezca en el intervalo [-1, 1]. Todas estas modificaciones ayudan a mejorar la estabilidad de DQN.

Sin embargo, DQN tiene algunos problemas; primero, siguiendo el algoritmo de Q-learning, DQN sobreestima los valores de acción bajo ciertas circunstancias, lo que hace que el aprendizaje sea ineficiente y puede llevar a políticas subóptimas [70]. Se propuso Double DQN (DDQN) para aliviar este problema [71]. La diferencia entre DQN y DDQN es que la política codiciosa se evalúa utilizando la red en línea, pero la red objetivo se utiliza para estimar su valor. Por lo tanto, \( y_i \) se cambia de la siguiente manera:

\[ y_i = r + \gamma Q(s', \arg \max_{a'} Q(s',a';\theta_{i-1}); \theta_{i-1}). \]

Una extensión interesante sobre DDQN es la red dueling [72], cuya idea es tener una sola red Q con las mismas capas convolucionales que DQN, pero con dos flujos de capas completamente conectadas (FC), que proporcionan estimaciones de las funciones de valor y ventaja. Esto ayuda a generalizar mejor el aprendizaje entre acciones. Segundo, DQN selecciona experiencias de manera uniforme para repetir, independientemente de su importancia, lo que hace que el proceso de aprendizaje sea lento e ineficiente. En consecuencia, se propuso replay de experiencias priorizadas para resolver el problema [73]. La idea es repetir experiencias importantes con más frecuencia, mejorando así el entrenamiento de la red. La importancia de cada transición se mide proporcionalmente al error de diferencia temporal, y se proponen dos variantes, priorización estocástica y muestreo de importancia, para mejorarlo. Finalmente, DQN no es aplicable en espacios continuos, por lo que se propuso el gradiente de política determinista profunda (DDPG) [74], que es una combinación de DQN y el gradiente de política determinista (DPG) [75] en un enfoque actor-critic. El actor mapea determinísticamente estados a una acción específica. El crítico define el valor de la acción tomada por el actor. En cada iteración, el crítico se actualiza mediante

\[ L = \frac{1}{N} \sum_i (y_i - Q(s_i,a_i \mid \theta_Q))^2 \]

y el actor se actualiza mediante

\[ \nabla_{\theta_{\mu}} J = \frac{1}{N} \sum_i \nabla_a Q(s,a \mid \theta_Q) \bigg|_{(s=s_i,a=\mu(s_i))} \nabla_{\theta_{\mu}} \mu(s \mid \theta_{\mu}) \bigg|_{s_i}, \]

donde \( \theta_{\mu} \) y \( \theta_Q \) son los parámetros de las redes actor y crítico, respectivamente.

Finalmente, la optimización de política por proximidad (PPO) [76] es otro algoritmo actor-critic utilizado por los RSRLs. De hecho, PPO es una versión mejorada del algoritmo de optimización de política de región de confianza (TRPO) [77], que maximiza un objetivo sustituto

\[ E_t \left[ \frac{\pi_{\theta}(a_t \mid s_t)}{\pi_{\theta_{\text{old}}}(a_t \mid s_t)} A_t \right], \]

donde \( A_t \) es un estimador de la función de ventaja en \( t \). La idea central en PPO es la introducción del objetivo sustituto recortado,

\[ E_t \left[ \min \left( \frac{\pi_{\theta}(a_t \mid s_t)}{\pi_{\theta_{\text{old}}}(a_t \mid s_t)} A_t, \text{clip} \left( \frac{\pi_{\theta}(a_t \mid s_t)}{\pi_{\theta_{\text{old}}}(a_t \mid s_t)}, 1 - \epsilon, 1 + \epsilon \right) A_t \right) \right], \]

donde \( \epsilon \) es un hiperparámetro.

3.7 Desafíos de RL
Existen algunos desafíos posibles al aplicar RL a cualquier problema. Un desafío conocido como "Triple mortal" indica que existe un riesgo de inestabilidad y divergencia al combinar tres elementos en RL: aproximación de funciones, bootstrapping y entrenamiento fuera de política [27]. Otro desafío en RL es la ineficiencia de la muestra, específicamente en algoritmos de RL sin modelo [78]. Los algoritmos actuales de RL sin modelo necesitan una cantidad considerable de interacción agente-entorno para aprender estados útiles. Además, dado que DRL se basa en el aprendizaje profundo, hereda la característica famosa de las redes neuronales, es decir, ser una caja negra. No es evidente cómo cambian los pesos y las activaciones, lo que los hace no interpretables. El problema clásico de la exploración versus explotación sigue siendo un desafío en RL y la exploración efectiva es un problema de investigación abierto. Finalmente, el problema de la formulación de recompensas en RL es un desafío y diseñar una buena función de recompensa no es muy claro o directo.

2.3 ¿Por qué Aprendizaje por Refuerzo para la Recomendación?
La naturaleza de la interacción del usuario con un sistema de recomendación es secuencial [79] y el problema de recomendar los mejores ítems a un usuario no es solo un problema de predicción, sino un problema de decisión secuencial [50]. Esto sugiere que el problema de recomendación podría modelarse como un MDP y resolverse mediante algoritmos de RL. Tres características únicas de RL hacen que sea una combinación perfecta para el problema de recomendación. Primero, RL puede manejar la dinámica de la interacción secuencial usuario-sistema ajustando acciones según el feedback continuo recibido del entorno. Segundo, RL puede tener en cuenta el compromiso a largo plazo del usuario con el sistema. Finalmente, aunque tener valoraciones de usuario es beneficioso, RL, por su naturaleza, no necesita valoraciones de usuario y optimiza su política interactuando secuencialmente con el entorno. Todas estas razones sugieren que sería beneficioso utilizar RL para proporcionar mejores recomendaciones, como lo han demostrado estudios en línea [25, 80].

2.4 Formulación del Problema

En un problema de recomendación, el algoritmo del sistema de recomendación (RS), a través de la interacción con el usuario y recibiendo sus retroalimentaciones implícitas/explicitas, intenta recomendar los mejores ítems al usuario, con el fin de lograr el objetivo para el cual está diseñado, que podría ser aumentar la ganancia, la satisfacción del usuario o la fidelidad del usuario [3]. Esto es análogo a una configuración típica de RL, donde un agente tiene como objetivo maximizar una recompensa numérica mediante la interacción con un entorno [27]. Por lo tanto, el agente de RL puede desempeñar el papel del algoritmo de RS y todo lo que está fuera de este agente, incluidos los usuarios del sistema e ítems, puede considerarse como el entorno para este agente.

De manera más formal, considerando al usuario e ítems como el entorno y al algoritmo de RS como el agente de RL, la formulación de MDP puede ser la siguiente:
- Estado S: un estado 𝑠𝑡 ∈ S se define como las preferencias del usuario y su historial pasado con el sistema.
- Acción A: una acción 𝑎𝑡 ∈ A es recomendar un ítem al usuario en el paso de tiempo 𝑡.
- Recompensa R: el agente de RL recibe una recompensa 𝑟(𝑠𝑡,𝑎𝑡) ∈ R basada en la retroalimentación del usuario sobre la recomendación proporcionada.
- Probabilidad de transición P: la probabilidad de transición 𝑝(𝑠′|𝑠,𝑎) ∈ P es la probabilidad de transición de 𝑠 = 𝑠𝑡 a 𝑠′ = 𝑠𝑡+1 si el agente toma la acción 𝑎.

Factor de descuento 𝛾: el factor de descuento 𝛾 ∈ [0,1] es el factor de descuento para las recompensas futuras. Con 𝛾 = 0, el agente se vuelve miope, es decir, se enfoca solo en la recompensa inmediata. Por el contrario, si 𝛾 = 1, el agente se vuelve previsor y se enfoca más en las recompensas futuras [27].

Dado (S,A,R,P,𝛾), el objetivo del agente de RL es encontrar una política 𝜋 que maximice la recompensa acumulativa esperada y descontada. En otras palabras,
max𝜋 E[ 𝑇∑︁
𝑡=0
𝛾𝑡𝑟(𝑠𝑡,𝑎𝑡)], (21)
donde 𝑇 es el paso de tiempo máximo en un MDP finito.

2.5 Marco Propuesto RLRS

Tras estudiar cuidadosamente todos los RLRS recopilados, encontramos que hay cuatro componentes comunes en todos ellos y creemos que un buen RLRS debería diseñar y abordar estos componentes cuidadosamente. En consecuencia, para unificar el proceso de desarrollo de RLRS, proponemos un marco para RLRS con cuatro componentes clave: (1) Representación del Estado, (2) Optimización de la Política, (3) Formulación de la Recompensa y (4) Construcción del Entorno. La Figura 3(a) representa este marco. A continuación, explicamos cada componente.

Representación del Estado. En la interfaz agente-entorno de RL, el estado puede ser cualquier información disponible para el agente. La representación del estado podría ser tan abstracta como descripciones simbólicas de objetos en una habitación o tan detallada como lecturas de sensores [27]. Lo importante es que los estados definidos deben tener la propiedad de Markov. Esto significa que la señal de estado no debe transmitir toda la información sobre el entorno al agente, pero debe resumir la información pasada de manera que no se pierda ninguna información relevante. Una señal de estado con esta propiedad se llama Markoviana. En general, la selección de la representación del estado es actualmente más un arte que una ciencia [27].

En RLRS, la representación del estado debe resumir información sobre usuarios, ítems y el contexto. Dividimos la representación del estado en RLRS en tres grupos:

SR1) Tratamiento de los ítems como estados. Cuando el espacio de ítems es pequeño, por ejemplo, incluye varias páginas web en un sitio web, es posible tratar cada ítem como un estado. Sin embargo, este enfoque ciertamente no es escalable cuando el espacio de ítems crece considerablemente. Para abordar el problema de escalabilidad en espacios de ítems más grandes, los investigadores encontraron que los estados podrían indicar un conjunto de ítems previamente valorados/consumidos por el usuario. La Figura 3(b) representa esta representación.

SR2) Características de usuarios, ítems y contexto. Una manera popular de representación del estado es extraer algunas características de usuarios, ítems y contexto, como se muestra en la Figura 3(c). Las características de usuario pueden incluir información demográfica, como edad, raza y género. Las características de ítems pueden incluir precio, categoría y popularidad. Las características de contexto pueden incluir tiempo, plataforma y ubicación.

SR3) Incrustaciones codificadas. Para un entrenamiento efectivo, los modelos profundos en RS basados en DRL necesitan que los estados sean vectores densos y de baja dimensionalidad. La Figura 3(d) ilustra un marco general y popular para la representación del estado en métodos basados en DRL. Típicamente, las primeras características de usuario, ítems y contexto se traducen en vectores continuos densos y de baja dimensionalidad llamados incrustaciones. Luego, para un mejor entrenamiento, esta incrustación podría ser codificada utilizando un modelo de red neuronal recurrente (RNN), que puede ayudar al modelo a aprender las preferencias secuenciales del usuario [48]. Las unidades recurrentes con compuertas (GRU) suelen ser más populares que la memoria a corto plazo (LSTM) para el módulo RNN, ya que tienen menos parámetros y pueden lograr el mismo o mejor rendimiento [81]. Para centrarse en partes importantes de la entrada, algunos investigadores también utilizan una capa de atención en el módulo de codificación y añaden pesos a los vectores codificados. Finalmente, los vectores codificados se concatenan para obtener el estado final.

Optimización de la Política. Una vez formulados los estados, es la política la que determina qué acción tomar (es decir, qué ítems recomendar) en cada estado. Para la optimización de la política, diversos algoritmos de RL han sido utilizados por los RLRSs. Antes del surgimiento del DRL, los métodos de RL utilizados por los RLRSs podían clasificarse generalmente en métodos tabulares y métodos aproximados. Los métodos tabulares incluyen la iteración de políticas, Q-learning, Sarsa, Sarsa(𝜆), R-learning y MCTS. Los métodos aproximados incluyen Q ajustado e iteración de valor de gradiente. Por otro lado, los métodos DRL podrían dividirse generalmente en tres grupos: basados en valor (DQN), gradiente de política (REINFORCE y REINFORCE-wb) y métodos actor-crítico (DDPG y PPO). Una clasificación de estos algoritmos se muestra en la Figura 2(b).

Formulación de la Recompensa. Como se mencionó anteriormente, la señal de recompensa del entorno refleja qué tan bien o mal está desempeñándose el agente mediante la selección de acciones. Por lo tanto, el diseño de una señal de recompensa informativa es crucial para el éxito/aprendizaje del agente. De hecho, en RL, la señal de recompensa es la única forma de indicarle al agente qué hacer, no cómo hacerlo [27]. En general, definir una función de recompensa adecuada es un problema difícil y es más un proceso de prueba y error o de ingeniería. No hay una regla definitiva para diseñar una buena función de recompensa en un problema específico. En los RLRSs, hemos observado dos tendencias generales en el diseño de la función de recompensa: (R1) la función de recompensa es una recompensa numérica simple y dispersa, o (R2) la recompensa es una función de una o varias observaciones del entorno.

Construcción del Entorno. En general, evaluar los RSs es difícil [3, 82]. Como resultado, construir un entorno adecuado para entrenar y evaluar correctamente al agente en los RLRSs es un desafío. Para distinguir mejor entre diferentes métodos de construcción del entorno, generalmente los dividimos en tres grupos: offline, simulación y online. En el método offline, el entorno es un conjunto de datos estático que contiene las calificaciones de algunos usuarios sobre algunos ítems. Una práctica común en los métodos offline es entrenar al agente con los datos de entrenamiento (generalmente el 70-80% de los datos) y luego probarlo con los datos restantes. En los estudios de simulación, generalmente se construye un modelo de usuario y se evalúa el algoritmo mientras interactúa con este modelo de usuario. Este modelo de usuario puede ser tan simple como un usuario con un comportamiento predefinido o puede ser más complejo y aprenderse utilizando datos disponibles. En el método online, el algoritmo se evalúa mientras interactúa con usuarios reales y en tiempo real. Este es el mejor método, pero también el más costoso para la evaluación de los RLRSs.

3 ALGORITMOS DE SISTEMAS DE RECOMENDACIÓN BASADOS EN APRENDIZAJE POR REFUERZO

En esta sección, presentamos los algoritmos de manera clasificada. Como se discutió anteriormente, primero dividimos generalmente los RLRSs en métodos basados en RL y DRL. Luego, revisamos los algoritmos en cada categoría con respecto al marco de trabajo RLRS.

3.1 RLRSs basados en RL

En esta sección, presentamos los RLRSs basados en RL; es decir, métodos que no utilizan aprendizaje profundo para la optimización de políticas. La Tabla 2 proporciona una visión general rápida sobre los métodos basados en RL.

3.1.1 Representación del Estado

Como se ilustra en la Tabla 2, aparte de RPRMS y PHRR, todos los métodos basados en RL pertenecen a SR1 o SR2, que comparten casi la misma proporción (ver Fig. 5(a)). Como se mencionó anteriormente, los métodos en SR1 utilizan todos o un conjunto/tupla de elementos para la representación del estado. Por ejemplo, WebWatcher [83], el primer RLRS que identificamos, trata cada ítem (es decir, página web) como un estado en un escenario de recomendación web. De manera similar, las Referencias [96] y [97] tratan cada autor y objeto de aprendizaje como un estado en recomendaciones de colaboradores científicos y escenarios de e-learning, respectivamente. Como se indicó anteriormente, aunque este enfoque es posible en espacios de estados pequeños, ciertamente no es escalable cuando el espacio de ítems crece considerablemente. Los investigadores descubrieron que hacer un seguimiento de un pequeño conjunto de ítems ya valorados/consumidos por el usuario podría ser lo suficientemente informativo para la optimización de políticas. Quizás las Referencias [50, 84] sean los primeros RLRSs que utilizan esta idea, pero la idea está mejor formalizada para los RLRSs por la Referencia [85]. Específicamente, en una aplicación de recomendación web, Taghipour y Kardan [85] adoptan el modelo N-gram de la literatura de minería de uso web [102] e introducen una ventana deslizante para representar estados, representada en la Figura 4. En esta figura, los círculos son estados, las flechas hacia la derecha son acciones, y 𝑉 y 𝑅 indican páginas visitadas y previamente recomendadas, respectivamente. Al usar este modelo, los autores asumen que conocer las últimas 𝑘 páginas visitadas por el usuario proporciona suficiente información para predecir sus futuras solicitudes de página. Es importante mencionar que este conjunto o ventana deslizante en SR1 podría indicar cualquier información útil para la optimización de políticas, incluyendo un conjunto de ítems comerciales [50], conceptos en un sitio web [84, 87], clases de emoción de canciones [89], habilidades [101] y canciones de música [51]. En un entorno diferente, Choi et al. [44] formulan el problema de recomendación como un juego de mundo en cuadrícula y cada celda de la cuadrícula, con sus usuarios e ítems dentro, se considera como un estado.

Otros investigadores han propuesto extraer algunas características de usuarios, ítems y contexto y usarlas para la representación del estado (SR2). Entre los primeros intentos en SR2 se encuentran los trabajos de Mahmood et al. [86, 88, 92], en los cuales se utiliza un conjunto de variables de usuario (por ejemplo, el número de veces que el usuario ha modificado su consulta), agente (por ejemplo, acción previa del agente) y sesión de interacción (por ejemplo, número de episodios transcurridos) para la representación del estado. Un enfoque similar se utiliza en RLradio [49], donde se definen algunas variables que contienen información sobre los canales de radio de interés del usuario y su comportamiento de escucha para representar los estados. DJ-MC [93] utiliza un método de codificación para representar cada canción como un vector de descriptores de canciones y cada estado es la concatenación de 𝑘 vectores de canciones en la lista de reproducción. Se utilizan características de las condiciones climáticas y el tiempo en CAPR [99] para la representación del estado en un recomendador de puntos de interés (POI). SR2 es especialmente popular en aplicaciones de atención médica en las que la información sobre los pacientes generalmente se registra mediante varias características descriptivas [90, 91]. En un entorno diferente, POMDP-Rec [95] formula los estados como estados de creencias utilizando un modelo de factor de baja dimensión [103]. Más precisamente, con una matriz usuario-ítem parcialmente observada, las observaciones del comportamiento del usuario (O), las características latentes de los ítems (V) y los intereses latentes de los usuarios (U) pueden calcularse como se muestra en las ecuaciones (22), (23) y (24).

Los únicos trabajos en métodos basados en RL que se encuentran en SR3 son RPRMS [98] y PHRR [100], ambos en el dominio de recomendación de música. En RPRMS, los estados son una concatenación de incrustaciones de letras de canciones, generadas utilizando Word2Vec [104], y incrustaciones de audio, generadas utilizando un modelo WaveNet preentrenado [105]. PHRR utiliza una factorización matricial ponderada [106] y CNN para incrustar canciones, y al igual que DJ-MC, cada estado es la concatenación de varios vectores de canciones.

3.1.2 Optimización de Políticas

Según la Figura 5(d), los métodos de diferencia temporal, es decir, Q-learning y Sarsa, han sido los algoritmos de RL más populares entre los métodos basados en RL [44, 51, 83, 85, 87, 89, 92, 97, 98, 101]. La principal razón de esta popularidad es su simplicidad; es decir, son métodos en línea, libres de modelo, requieren una cantidad mínima de computación y pueden expresarse mediante una sola ecuación (ver Ecs. (4) y (5)) [27]. Aplicar Q-learning/Sarsa para la optimización de políticas es bastante directo y no requiere ninguna modificación específica. Los investigadores en [85] utilizan un truco simple para tener una tasa de aprendizaje decreciente 𝛼 = 1/1 + visitas(𝑠,𝑎) en la Ec. (4), lo cual ayuda a la convergencia del algoritmo. Este truco también se utiliza en [51, 89, 92]. Un problema con los métodos de diferencia temporal, al igual que cualquier método RL tabular, es que conducen a la maldición de la dimensionalidad [107]. Para abordar este problema, como se discutió anteriormente, los investigadores intentan manejar el espacio de estados y mantenerlo lo suficientemente pequeño.

Entre los métodos tabulares, los métodos de programación dinámica suelen ser imprácticos debido a su gran costo computacional y la necesidad de conocimiento perfecto sobre el entorno. Aunque estos algoritmos son polinomiales en el número de estados, realizar una sola iteración de métodos de iteración de política o de valor a menudo es inviable [108]. Para hacerlo práctico, Ref. [50] utiliza un par de características en su espacio de estados y realiza algunas aproximaciones. Por ejemplo, una característica del espacio de estados en [50] es la direccionalidad; los autores argumentan que un estado corto no puede seguir a un estado largo o que la probabilidad de ocurrencia de bucles en su MDP no es muy alta. Además, Ref. [86] mantiene el número de ejecuciones de iteración de política limitado.

MCTS es un algoritmo de planificación en tiempo de decisión que se beneficia de la estimación de valor basada en muestras incremental y online y la mejora de políticas [27], y ha sido utilizado por [93, 99, 100]. Para facilitar el aprendizaje del agente, en caso de que el espacio de canciones sea muy grande o el tiempo de búsqueda sea limitado, DJ-MC [93] agrupa canciones según tipos de canciones y luego aplica MCTS a las canciones agrupadas. PHRR [100] adopta esquemas similares en la optimización de políticas y en la agrupación de canciones. Similar a AlphaGo [62], CAPR [99] utiliza UCT (Upper Confidence Bound aplicado a Trees) [109] para resolver el compromiso entre exploración y explotación en el paso de selección de MCTS (ver sección 2.2).

Preda y Popescu [84] utilizan Sarsa(𝜆) con codificación por mosaicos [27] y aproximación lineal para la optimización de políticas. Para poder aplicar Sarsa(𝜆), el trabajo transforma la información epistémica en matrices de números reales. Moling et al. [49] definen el problema de la recomendación óptima de canales de radio como una tarea continua y luego emplean R-learning [59] para resolverlo.

Por otro lado, algunos RLRSs basados en RL han utilizado métodos aproximados para la optimización de políticas, incluidos el Q ajustado [90, 91, 94, 95] y la iteración de valor de gradiente [96]. El Q ajustado es un marco flexible que puede ajustar cualquier arquitectura de aproximación a la función Q [64]. En consecuencia, cualquier algoritmo de regresión supervisada en modo por lotes puede usarse para aproximar la función Q, lo cual puede escalar bien a espacios de alta dimensionalidad [27]. Sin embargo, un problema con este método es que podría tener un alto costo computacional y de memoria con el aumento del número de cuádruples (𝑥𝑡,𝑢𝑡,𝑟𝑡,𝑥𝑡+1), donde 𝑥𝑡 indica el estado del sistema en el tiempo 𝑡, 𝑢𝑡 la acción de control tomada, 𝑟𝑡 la recompensa inmediata, y 𝑥𝑡+1 el siguiente estado del sistema [64]. Este algoritmo ha sido utilizado por varios RLRSs [90, 91, 94, 95]. Para ajustar la función Q en estos métodos, se utilizan regresión lineal [90, 94], regresión de vector de soporte [91] y redes neuronales [95]. Finalmente, Zhang et al. [96] introducen una versión de descenso de gradiente del algoritmo de iteración de valor para la recomendación de colaboradores en un entorno de RL multiagente.

3.1.3 Formulación de Recompensas

La Figura 5(b) muestra que R2, con una proporción del 60%, ha sido más popular que R1, con un 40%, entre los métodos basados en RL. En R1, se han utilizado diferentes valores numéricos para la recompensa inmediata. Por ejemplo, Mahmood et al. utilizan +1 en estado terminal y un número negativo en otro caso [86], +5 por agregar un producto al plan de viaje, +1 por mostrar una página de resultados, y 0 en otro caso [88], y +100 por comprar un libro, -30 por abandono del usuario, y 0 en otro caso [92]. Por otro lado, en R2, los investigadores han propuesto utilizar diferentes observaciones del entorno para formular la recompensa, incluyendo beneficio neto [50], tiempo de supervivencia global [91], algunos puntajes clínicos (es decir, PANSS) [90], y la distancia de Jaccard entre dos estados [44].

3.1.4 Construcción del Entorno

Es observable en la Figura 5(c) que el método dominante de construcción del entorno en los RS basados en RL es offline. Esto tiene sentido ya que entrenar al agente y probar el rendimiento en un conjunto de datos disponible es la opción más fácil y segura. Dos conjuntos de datos populares utilizados en RS basados en RL son MovieLens [110] y el conjunto de datos Million Song [111]. Otro método de construcción del entorno es la simulación, que es un método seguro para ajustar los parámetros importantes del modelo antes del despliegue del sistema o un estudio en línea. La simulación podría ser tan simple como asumir usuarios constantes con patrones de preferencia predefinidos [89, 91], o podría ser más compleja y aprender el comportamiento del usuario a través de datos disponibles [86, 92, 93]. Por ejemplo, en una tarea de aprendizaje supervisado, Mahmood et al. [86] definen un modelo de comportamiento del usuario y lo utilizan en un RS de viaje, llamado NutKing, para aprender las probabilidades de transición. El objetivo es conocer cómo reacciona el usuario simulado ante una cierta acción del sistema.

El estudio en línea es el método más efectivo pero costoso para la construcción del entorno en los RLRSs. En un intento valioso y temprano [50], se evaluó el rendimiento del RS propuesto basado en MDP en un estudio en línea de dos años realizado en una librería en línea. El estudio tuvo una buena exposición diaria a los usuarios, con casi 5000-6000 usuarios diferentes diarios, con un número razonable de artículos para recomendar (más de 15,000), en comparación con otros estudios en línea realizados por métodos basados en RL con solo cinco [89], 13 [92], 47 [93], 469 [88], y 500 [84] usuarios.


3.2 RS basados en DRL

En esta sección, estudiamos los métodos basados en DRL; es decir, aquellos RS que utilizan un modelo de aprendizaje profundo para la optimización de políticas. La Tabla 3 proporciona una visión general rápida de estos métodos.

3.2.1 Representación del Estado

Como se muestra en la Figura 6(a), SR3 es el esquema dominante de representación del estado para los RS basados en DRL. Como se mencionó anteriormente, esto se debe a que los modelos profundos se entrenan de manera más efectiva en vectores densos y de baja dimensionalidad. No obstante, los investigadores han ido un paso más allá y han intentado hacer más efectivo el marco general de SR3 (ver Figura 3(d)). Típicamente, en los RS basados en RL, los artículos calificados positivamente por el usuario se consideran como preferencias del usuario. Sin embargo, en DEERS [48], los autores discuten que la proporción de retroalimentación negativa, como los elementos saltados, podría ser mucho mayor que la positiva, por lo que proponen tener dos estados: estados positivos y negativos. La Figura 7(a) ilustra esta modificación. En particular, la entrada se divide en elementos con retroalimentación positiva y negativa, se pasa a través de capas de incrustación y RNN, y se alimenta a la red Q donde se concatenan. Esta técnica también ha inspirado a otros investigadores [120, 165]. En lugar de la capa RNN, DRCGR [120] utiliza una capa de convolución (con núcleos horizontales y verticales) para codificar las incrustaciones de retroalimentación positiva. Por otro lado, un módulo de red generativa adversaria (GAN) se entrena para generar muestras negativas. Deep Page [46] también extiende el marco SR3 agregando un módulo CNN entre las capas de incrustación y RNN, para aprender el esquema de visualización espacial de elementos en un escenario de recomendación por página. Antes de pasar las incrustaciones de elementos a través del módulo CNN, se utiliza una capa de página para convertir las incrustaciones de elementos en una cuadrícula/matriz 2D para procesamiento CNN 2D. Además, los autores en [135] proponen utilizar un esquema de ponderación de posición para la incrustación del estado. Formalmente, si 𝑊 es una matriz con pasos históricos como filas y el peso de importancia de posiciones como columnas, la incrustación de un estado 𝑠𝑡 se puede definir como
\[ 𝑠𝑡 = ℎ(𝐹𝑡−𝑚:𝑡−1) = 𝑣𝑒𝑐[𝜎(𝐹𝑡−𝑚:𝑡−1𝑊 + 𝐵)], \]
donde 𝐹 es el vector de características del historial con 𝑚 pasos, 𝐵 es una matriz de sesgo, 𝜎(·) es una activación no lineal, y 𝑣𝑒𝑐[·] concatena las columnas de la matriz. Los autores afirman que este método para la incrustación del estado es más eficiente para la optimización que LSTM. Finalmente, en D2RLIR [162], se agrega una codificación posicional a las incrustaciones del estado para que el modelo entienda el orden cronológico de los elementos.

En DRR [144], se propone un módulo individual llamado módulo de representación del estado para la formulación del estado. Los autores proponen tres estructuras para modelar las interacciones entre el usuario y los elementos. En la primera estructura, DRR-p, simplemente se concatenan las incrustaciones de los elementos y sus productos por pares, como se muestra en la Figura 8(a). Más formalmente, si 𝐻 = {𝑣1, 𝑣2, ..., 𝑣𝑛} es el historial de interacción positiva del usuario y 𝑃 = {𝑤𝑖𝑣𝑖 ⊗ 𝑤𝑗𝑣𝑗 | 𝑖, 𝑗 = 1, 2, ..., 𝑛} es el producto por pares ponderado entre elementos, entonces el estado 𝑆 se define como la concatenación de 𝐻 y 𝑃, es decir, 𝑆 = (𝐻, 𝑃). En la segunda estructura, DRR-u, también se incorpora la incrustación del usuario (mostrado en la Figura 8(b)). Esto significa que, con 𝐾 = {𝑢 ⊗ 𝑤𝑖𝑣𝑖 | 𝑖 = 1, 2, ..., 𝑛}, 𝑆 = (𝐾, 𝑃). En la última estructura ilustrada en la Figura 8(c), DRR-ave, se introduce una capa de agrupación promedio para eliminar el sesgo de posición de los elementos en la lista recomendada. En particular, si 𝐺 = {𝑎𝑣𝑒(𝑤𝑖𝑣𝑖) | 𝑖 = 1, ..., 𝑛}, 𝑆 = (𝑢, 𝑢 ⊗ 𝐺, 𝐺). En [145], los autores amplían DRR-ave y agregan una red de atención para generar pesos dependientes del usuario para cada elemento, como se muestra en la Figura 8(d). En otro trabajo [160], los mismos autores estudian el efecto de actualizar el módulo de representación del estado utilizando una señal de aprendizaje supervisado, y a través de estudios experimentales muestran que el rendimiento de la recomendación podría mejorar.

Alrededor del 20% de los RS basados en DRL pertenecen a SR2. Por ejemplo, DRN [47] utiliza características del usuario y del contexto para la representación del estado en la recomendación de noticias. Las características del usuario extraídas en DRN incluyen las características de las noticias que el usuario ha hecho clic en diferentes marcos de tiempo, como una hora, seis horas, 24 horas, una semana y un año. Estas características de las noticias incluyen proveedor de titulares, clasificación, nombre de entidad, categoría y categoría de tema. Las características del contexto también describen el contexto temporal de la solicitud de noticias, incluyendo la hora y el día de la semana. Al igual que en los métodos basados en RL, SR2 es el método popular de representación del estado en aplicaciones de salud [113, 115]. Nemati et al. [113] utilizan una formulación de MDP parcialmente observable (POMDP) en una aplicación clínica. Formulan los estados como estados de creencia utilizando un modelo oculto de Markov discriminatorio (DHMM).

Encontramos solo dos trabajos [24, 112] que se encuentran en SR1. Tal vez la razón por la que estos trabajos utilizan un método simple de representación del estado es que son trabajos representativos no diseñados específicamente para RSs, sino desarrollados para abordar desafíos específicos en la aplicación de DRL a dominios como RSs.

3.2.2 Optimización de Política. Después de definir los estados, el rol de la política 𝜋 es mapear estados a acciones. Los algoritmos de optimización de políticas utilizados por los RS basados en DRL pueden dividirse generalmente en métodos basados en valor, gradiente de política y métodos actor-crítico.
Métodos basados en valor. Aparte de MCTS utilizado en [134], DQN y sus extensiones, es decir, DDQN, dueling DQN y dueling DDQN, son los métodos basados en valor dominantes. Básicamente, hay tres elementos principales en DQN: 1) la arquitectura de la red Q, 2) el replay de experiencia, y 3) la exploración. Revisamos los métodos basados en DQN según estos elementos y la Tabla 4 resume los métodos basados en DQN.

1) Arquitectura de la red Q. La Figura 7 muestra dos posibles arquitecturas de la red Q utilizadas por los RS basados en DQN. La arquitectura original (A1), introducida en [67, 68], recibe el estado y emite el valor Q de todas las acciones, indicado por 𝑄1, ..., 𝑄𝑛 en la Figura 7(b). Si bien A1 funciona bien cuando el espacio de acciones es pequeño, su aplicabilidad en el dominio de RS con un espacio de acciones grande, e incluso enorme (del orden de millones), es cuestionable. Otra arquitectura posible (A2) es recibir el par de estado y acción, y luego emitir el valor Q del par, es decir, 𝑄(𝑠,𝑎) (representado en la Figura 7(c)). Aunque A2 resuelve el problema de A1, un problema con A2 es que la complejidad temporal del modelo podría ser alta.

A pesar de que en el DQN original se utiliza una CNN en la red Q para procesar datos de imágenes, la red Q en los RS basados en RL típicamente está compuesta por varias capas completamente conectadas (FC), ya que la entrada, es decir, los estados o acciones, tienen la forma de vectores 1D. Por ejemplo, como se mencionó antes, DEERS [48] utiliza dos tipos de estados como entrada en la red Q: estados positivos y negativos, representados en la Figura 7(a). La red Q es una red FC de cinco capas donde las primeras tres capas son separadas para estados positivos y negativos, y luego las dos últimas capas conectan ambos estados, emitiendo el valor Q de un par dado de estado y acción. Para tener en cuenta esta arquitectura de doble estado, la función de pérdida original de DQN en la ecuación (14) se modifica como sigue:

\[ \mathcal{L}(\theta_i) = \mathbb{E}_{s,a \sim \rho(\cdot)} \left[ \left( y_i - Q(s^+, s^-, a; \theta_i) \right)^2 \right], \]

donde \( y_i = \mathbb{E}_{s'} [r + \gamma \max_{a'} Q(s'^+, s'^-, a'; \theta_{i-1}) \mid s^+, s^-, a] \).

Consecuentemente, el gradiente de la función de pérdida se convierte en:

\[ \nabla_{\theta_i} \mathcal{L}(\theta_i) = \mathbb{E}_{s,a \sim \rho(\cdot)} \left[ \left( r + \gamma \max_{a'} Q(s'^+, s'^-, a'; \theta_{i-1}) - Q(s^+, s^-, a; \theta_i) \right) \nabla_{\theta_i} Q(s^+, s^-, a; \theta_i) \right]. \]

Otros investigadores han utilizado extensiones de DQN. Por ejemplo, DRN [47] adopta dueling DDQN para la optimización de política en la recomendación de noticias. En particular, los autores argumentan que mientras la recompensa de tomar una acción se ve afectada por todas las características, es decir, usuario, noticias, contexto y características usuario-noticias, hay una recompensa que solo es impactada por las características de usuario y contexto. En consecuencia, la función Q se divide en la función de valor \( V(s) \) y la función de ventaja \( A(s, a) \). Como se muestra en la Figura 9(a), mientras \( V(s) \) recibe características de estado, la entrada a \( A(s, a) \) está compuesta por características de estado y acción.

DEAR [173] estudia el problema de la publicidad junto con la recomendación. Combina las dos arquitecturas de red Q de DQN, es decir, A1 y A2, y la arquitectura resultante genera el valor Q de una lista de anuncios candidatos si se insertan en la lista de recomendación. En otras palabras, la entrada es similar a la arquitectura A2, es decir, estado y acción, y la salida es igual a A1, que es una lista que contiene los valores Q de todos los pares estado-acción.

2) Reproducción de la Experiencia. Según la Tabla 4, la gran mayoría (22 de 28) de los RS basados en DQN utilizan muestreo uniforme original para reproducir experiencias recolectadas. Además, solo tres de ellos utilizan reproducir la experiencia con prioridad [48, 115, 170]. Los autores en [43] proponen utilizar una reproducción de muestreo estratificado en lugar de un muestreo uniforme para abordar la varianza del muestreo en entornos dinámicos. El muestreo estratificado es una técnica de muestreo de una población en la cual la población entera se divide en varios grupos (llamados estratos) y luego se seleccionan muestras aleatorias de estos estratos [177]. Proponen utilizar algunas características estables de los clientes, como género, edad y geografía, como estratos.

GoalRec [172] utiliza el replay de experiencia retrospectivo [178]. La idea principal en el replay retrospectivo es aprender tanto de un resultado no deseado como de uno deseado. Dado que el objetivo no afecta la dinámica del entorno, una trayectoria fallida se etiqueta nuevamente como exitosa, como si el estado en la trayectoria fuera el objetivo real. Esto mejora considerablemente la eficiencia de la muestra.

En contraste con los métodos existentes basados en DQN, SADQN [138] no utiliza replay de experiencia para el entrenamiento. En cambio, en cada episodio de la fase de entrenamiento, se selecciona un usuario del conjunto de usuarios y el agente se entrena en las interacciones disponibles hasta que converge. Usando experimentos, los autores afirman que el replay de experiencia de hecho disminuye el rendimiento de SADQN.

3) Exploración. Aunque la exploración es un factor importante en el aprendizaje del agente, muchos métodos basados en DQN parecen pasar por alto este aspecto, ya que no hay indicación específica al respecto en las publicaciones respectivas. Aparte de técnicas simples de exploración como 𝜖-greedy, DRN [47] propone utilizar un enfoque de exploración similar al algoritmo de descenso de gradientes de bandit duelo [179]. En particular, hay una red separada para la exploración llamada red explore y sus parámetros pueden obtenerse usando una perturbación en los parámetros de la red actual con parámetros 𝑊:

\[ \Delta 𝑊 = 𝛼 · rand(-1,1) · 𝑊, \]

donde 𝛼 es el coeficiente de exploración. Luego, el agente genera una lista combinada de recomendaciones utilizando una interlección probabilística entre los elementos encontrados por la red actual y la red explore.

En recEnergy [141], para equilibrar el compromiso entre exploración y explotación, se utiliza la exploración de Boltzmann [27]. Más precisamente, los valores Q de salida de las acciones de la red Q se pasan a través de una ecuación softmax como sigue:

\[ 𝑃(𝑎) = \frac{exp(Q(a) / \tau)}{\sum_{i=1}^{n} exp(Q(i) / \tau)}, \]

donde 𝜏 es una temperatura que decae con el tiempo. Este método garantiza que el modelo explore más frecuentemente inicialmente, y luego comienza a explotar acciones con valores Q más grandes con más frecuencia.

Métodos de Gradiente de Política. En contraste con los métodos basados en el valor, los métodos de gradiente de política aprenden una política parametrizada sin la necesidad de una función de valor. REINFORCE es un método de gradiente estocástico de Monte Carlo que actualiza directamente los pesos de la política. Los principales problemas del algoritmo REINFORCE son la alta varianza y el aprendizaje lento. Estos problemas provienen de la naturaleza de Monte Carlo de REINFORCE, ya que selecciona muestras al azar y las actualizaciones se realizan cuando se completa el episodio.

En un trabajo valioso, la Ref. [25] adapta el algoritmo REINFORCE a un generador de candidatos neural con un espacio de acción muy grande. En particular, en un entorno de RL en línea, el estimador del gradiente de política puede expresarse como:

\[ \sum_{\tau \sim \pi_{\theta}} \left[ |\tau| \sum_{t=0}^{T} R_t \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) \right], \]

donde 𝜋𝜃 es la política parametrizada, 𝜏 = (s_0, a_0, s_1, ...), y 𝑅_t es la recompensa acumulada. Dado que en el entorno de RS, a diferencia de los problemas clásicos de RL, la interacción en línea o en tiempo real entre el agente y el entorno es inviable y generalmente solo está disponible la retroalimentación registrada, aplicar el gradiente de política en la ecuación (33) está sesgado y necesita corrección. El estimador de gradiente de política corregido fuera de política es entonces:

\[ \sum_{\tau \sim \beta} \frac{\pi_{\theta}(\tau)}{\beta(\tau)} \left[ |\tau| \sum_{t=0}^{T} R_t \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) \right], \]

donde 𝛽 es la política de comportamiento y 𝜋𝜃(𝜏) / 𝛽(𝜏) es el peso de importancia.

Desde que esta corrección genera una gran varianza para el estimador debido a los productos encadenados, los autores utilizan una aproximación de primer orden para los pesos de importancia, lo que conduce al siguiente estimador sesgado con una varianza menor:

\[ \sum_{\tau \sim \beta} \left[ |\tau| \sum_{t=0}^{T} \pi_{\theta}(a_t | s_t) \beta(\tau) R_t \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) \right]. \]

La Figura 9(b) ilustra la arquitectura neural de la política parametrizada 𝜋𝜃 en la ecuación (36).

Como se discute en la sección 2.2, REINFORCE-wb añade una línea de base a la regla de actualización de REINFORCE para disminuir la varianza (ver Eq. (12)). Varios RS basados en REINFORCE han utilizado este enfoque [114, 128, 132, 150, 167]. Específicamente, la línea de base en estos métodos es una red de valor [114, 132, 167], una restricción [128] y una recompensa promedio [150]. Sin embargo, no está claro cómo otros RS basados en REINFORCE [117, 123, 136, 140, 147, 157, 159, 166] abordan el problema de la varianza.

Siguiendo a SeqGAN [180], IRecGAN [130] emplea GANs para desarrollar un recomendador basado en RL basado en modelos. En particular, el generador es responsable de generar recomendaciones y modelar el comportamiento del usuario, y el discriminador se utiliza para reescalar las recompensas generadas. Usando datos generados y datos sin conexión, REINFORCE se utiliza para optimizar la política de recomendación. Similar a SeqGAN, para reducir la varianza, IRecGAN utiliza MCTS con política de roll-out, es decir, muestreo de 𝑁 secuencias de interacción entre el recomendador y el modelo de usuario y luego promediando las estimaciones.

Métodos Actor-Critic. DDPG es el método base utilizado en casi todos los RS basados en actor-critic. DDPG utiliza una arquitectura de actor-critic para combinar DPG y DQN. El actor, también llamado red de política, es responsable de generar acciones, y el crítico, un módulo DQN, es responsable de evaluar la acción tomada. DDPG original utiliza varias capas FC o capas convolucionales más FC cuando la entrada es píxel. La capa de salida del actor es una capa tanh para limitar las acciones. Para la exploración, DDPG utiliza un ruido temporalmente correlacionado, el proceso Ornstein-Uhlenbeck (OU) [181], que es adecuado para entornos físicos con momento. Además, al igual que DQN, se utiliza replay de experiencia con muestreo uniforme.

Wolpertinger [24] es el primer método actor-critic basado en DDPG para manejar grandes espacios de acción discretos, con un estudio de caso de recomendación. La idea es proporcionar un método que tenga complejidad sublineal con respecto al espacio de acción y sea generalizable sobre acciones. Como se muestra en la Figura 9(c), Wolpertinger consta de dos partes: generación de acción y refinamiento de acción. En la primera parte, las proto-acciones son generadas por el actor en espacio continuo y luego se mapean al espacio discreto utilizando el método 𝑘-vecino más cercano (𝑘-NN). Más precisamente, la proto-acción ˆ𝑎 se genera como ˆ𝑎 = 𝑓𝜃 (𝑠). Esta proto-acción probablemente no sea una acción válida, por lo que se mapea a un elemento en A como

\[ g_k(\hat{a}) = \arg \min_{a \in A} \| a - \hat{a} \|_2^2. \]

En la segunda parte, las acciones atípicas se filtran utilizando un crítico, que selecciona la mejor acción que tiene el valor Q máximo. En otras palabras,

\[ \pi_{\theta}(s) = \arg \max_{a \in g_k} Q_{\theta}(s, a). \]

Wolpertinger se entrena utilizando DDPG. Para la exploración, para la tarea de recomendación, Wolpertinger utiliza una técnica de exploración 𝜖-greedy guiada. En particular, la exploración se restringe a un conjunto probablemente bueno de acciones proporcionadas por el simulador del entorno.

La gran mayoría de los métodos actor-critic están basados en DDPG [24, 40, 45, 46, 116, 118, 127, 143-146, 148, 149, 151, 160-164, 168, 171]. La Tabla 5 resume estos métodos. Como se muestra, solo DRR utiliza replay de experiencia priorizado; los algoritmos restantes utilizan muestreo uniforme o no hay indicios al respecto en las publicaciones respectivas. Otra observación importante de la Tabla 5 es que la gran mayoría de los algoritmos no mencionan la exploración. De los cinco algoritmos con un método de exploración descrito, tres de ellos, es decir, Wolpertinger, DRR y HRL-Recused, están basados en 𝜖-greedy. Similar a DDPG, DRGR utiliza el proceso OU para fomentar una mejor exploración para el actor. Sin embargo, como se mencionó anteriormente, el ruido OU es adecuado para procesos físicos. Finalmente, MASSA introduce un método novedoso de exploración regularizado por entropía, un método similar a soft actor-critic [182].

El método actor-critic parece ser una arquitectura popular para el aprendizaje por refuerzo multi-agente (MARL). El aprendizaje/entrenamiento centralizado con ejecución descentralizada [183, 184] es un marco adecuado para un entorno multi-agente y es adoptado por CROMA, MASSA, DeepChain y MASTER. Por ejemplo, la Figura 10(a) muestra la arquitectura de MADDPG [184], que utiliza un marco de entrenamiento centralizado y ejecución descentralizada. MASSA se basa en esta arquitectura y agrega una red de señales al MADDPG, como se muestra en la Figura 10(b), que es responsable de facilitar la cooperación entre actores descentralizados.

Hay un par de métodos basados en actor-critic que utilizan entrenamiento adversarial para un mejor aprendizaje de políticas [152, 155]. Por ejemplo, CRSAL [152] extiende el soft actor-critic [182] con aprendizaje adversarial, añadiendo un discriminador dentro del crítico para distinguir entre diálogos generados por la red de políticas y usuarios reales. En un escenario de razonamiento de caminos sobre un grafo de conocimiento, ADAC [155] utiliza aprendizaje de imitación adversarial [185] y define dos discriminadores de camino y metapath para distinguir entre caminos expertos y caminos generados por el actor.

En contraste con otros métodos actor-critic, DRESS [124] utiliza PPO y SDAC [175] propone un actor-critic estocástico discreto. Los autores de SDAC proponen un marco general fuera de línea para RSs de aprendizaje por refuerzo. Primero formulan el problema de recomendación como un modelo generativo probabilístico. Luego, proponen un algoritmo estocástico actor-critic para optimizar la política de recomendación.

3.2.3 Formulación de la Recompensa. Como se muestra en la Figura 6(b), la mayoría (60%) de los RSs basados en DRL pertenecen a R2. Un patrón comúnmente utilizado por los RSs en R2 es formular la recompensa como una función, o una combinación simple, de varios factores o métricas [47, 113, 115, 119, 121, 127, 131, 133, 141, 150, 152, 156-158, 160, 161, 169, 173, 175]. Por ejemplo, en un escenario de recomendación de noticias, la recompensa en DRN [47] es una función de clics de usuario y actividad del usuario. La razón detrás de incluir la actividad del usuario es que una buena recomendación debería motivar al usuario a usar o interactuar con el sistema nuevamente. Los autores utilizan modelos de supervivencia [186] para modelar el retorno del usuario [187] y la actividad del usuario. FeedRec [119] formula la recompensa como una suma ponderada de métricas instantáneas, incluyendo clics y compras de usuarios, y métricas retardadas, como la profundidad de navegación y el tiempo de permanencia. Los autores consideran los clics de usuario como una métrica instantánea, y la profundidad de navegación y el tiempo de retorno como métricas retardadas. Yu et al. [121] diseñan una función de ventaja compuesta por recompensas visuales, atributivas e históricas para abordar el problema de recomendación multimodal.

Robust DQN [43] propone utilizar una recompensa aproximada por arrepentimiento para mejorar la estimación de la recompensa. La idea es utilizar dos recompensas diferentes, es decir, recompensas actuales y óptimas, y luego calcular el arrepentimiento como la recompensa final. Dado que calcular la recompensa óptima en realidad no es posible, proponen utilizar una recompensa de referencia alternativa, que es la recompensa promedio lograda aplicando el modelo a un subconjunto de usuarios.

Un esquema simple pero efectivo para la optimización multiobjetivo en RSs de aprendizaje por refuerzo es formular la recompensa como una función multiobjetivo [134, 146, 159, 162]. Singh et al. [159], por ejemplo, utilizan esta idea para un RS seguro. El formato de la recompensa en su trabajo es el siguiente:

\[ R_{mo} = R_t - C_{risk}, \]

donde \( C_{risk} \) es una restricción de riesgo de salud. Esta función de recompensa equilibra la maximización de la recompensa con la preservación de la restricción de salud. Una formulación similar se utiliza para equilibrar el compromiso entre precisión y diversidad [134, 162] y equidad [146].

En el aprendizaje por refuerzo jerárquico, deben definirse dos funciones de recompensa, es decir, para agentes de nivel bajo y alto [114, 123, 143, 165, 166, 171]. Por ejemplo, en HRL-Rec [171], los tiempos de clic en el canal recomendado se consideran como la recompensa del agente de nivel bajo, mientras que la recompensa para el agente de nivel alto se compone de cuatro factores, incluidos los tiempos de clic, el tiempo de permanencia, la diversidad a nivel de lista y la novedad del elemento.

En KGRE-Rec [132], se utiliza una función de recompensa retardada. Los autores discuten que es imposible definir una recompensa binaria y dispersa cuando no hay un artículo bueno/target en su problema de recomendación. En su lugar, se alienta al agente a encontrar caminos buenos en el grafo, aquellos que conducen a un artículo de interés del usuario con alta probabilidad. Por lo tanto, el agente recibe una recompensa solo en un estado terminal. La misma idea se puede ver en MASTER [168], donde se otorga una recompensa perezosa al agente cuando se realiza con éxito una solicitud de carga. Sin embargo, la idea de recompensar al agente solo en el estado terminal no siempre es práctica. Por ejemplo, Liu et al. discuten que dado que no hay un estado terminal bien definido en AnchorKG [169], la función de recompensa debería componerse de recompensas inmediatas y terminales.


Otro método de formulación de recompensas utilizado en R2 es definir la recompensa como la distancia entre el ítem recomendado y un ítem objetivo [45, 140, 151]. En KGRL [151], por ejemplo, la recompensa se basa en la distancia entre el ítem predicho e ítem objetivo en el grafo:
\[ r = 100 \sqrt{d(v_p, v_t)} + \epsilon \cdot W_{pt}, \]
donde \( d(v_p, v_t) \) es la distancia entre el ítem predicho \( p \) e ítem objetivo \( t \), \( \epsilon \) es un regulador, y \( W_{pt} \) es la suma de los pesos del camino más corto desde \( v_p \) hasta \( v_t \). La distancia \( d \) se calcula utilizando el algoritmo de Dijkstra.

Por otro lado, quizás el método más simple de definición de recompensas para el diseñador es utilizar empíricamente varios valores reales para diferentes objetivos en el sistema, lo cual se suele usar en R1. Por ejemplo, Zhao et al. utilizan un patrón similar de recompensas numéricas en sus propuestas [40, 46, 48, 149], y recompensan tres comportamientos de usuarios, a saber, saltar, hacer clic y ordenar, con algunos números, por ejemplo, 0, 1 y 5, respectivamente. El mismo patrón se puede observar en otros RSs de RL pertenecientes a R1 [114, 116, 118, 120, 126, 138, 142].

Del mismo modo, en un escenario de RS conversacional, EAR [147] define una función de recompensa dispersa con cuatro valores predefinidos, es decir, una recompensa fuertemente positiva cuando la recomendación es exitosa (\( r_s \)), una recompensa positiva si el usuario da retroalimentación positiva sobre el atributo preguntado (\( r_a \)), una recompensa fuertemente negativa por la salida del usuario (\( r_q \)), y una ligeramente negativa por cada turno de conversación (\( r_p \)). La recompensa total es la suma de estas recompensas y en los experimentos utilizan los valores \( r_s = 1 \), \( r_a = 0.1 \), \( r_q = -0.3 \), y \( r_p = -0.1 \). Una función de recompensa similar se utiliza en otros RSs conversacionales [122, 153, 170].

La recompensa retardada también se utiliza en R1 por algunos RSs basados en grafos [136, 155, 167], donde el agente solo recibe recompensa cuando alcanza el estado terminal. Por ejemplo, en Ekar [136], el agente recibe una recompensa de +1 si alcanza un ítem en el estado terminal con el cual el usuario ha interactuado, 0 si alcanza un ítem pero el usuario no ha interactuado con él, y -1 si la entidad alcanzada no es un ítem en el grafo.

Claro, aquí tienes la continuación y finalización de la traducción del texto:

### 3.2.4 Construcción del Entorno

Como se muestra en la Figura 6(c), más de la mitad de los RS basados en DRL utilizan un método offline para la construcción del entorno. Casi el 40% de los métodos utilizan un simulador, y solo el 10% utiliza un estudio en línea. En comparación con los métodos basados en RL, aunque una proporción similar utiliza el método offline, el uso de simulación ha aumentado al doble y disminuido casi un 60% para los esquemas en línea. Este gráfico muestra que realizar un estudio en línea se ha vuelto más difícil o costoso, y la simulación se está volviendo cada vez más popular entre la comunidad de RS basados en RL.

Entre aquellos que realizan un estudio de simulación, SlateQ [80] introduce un entorno de simulación de RS basado en RL de código abierto, llamado RecSim [188], que brinda al investigador la flexibilidad para evaluar sus algoritmos en diferentes configuraciones. Cascading DQN [135] utiliza GANs para simular a un usuario real y estimar la función de recompensa a partir de datos registrados. Más precisamente, el entrenamiento de GAN se formula como

\[ \min_\theta \max_\alpha \left( \mathbb{E}_{\phi_\alpha} \left[ \sum_{t=1}^T r_\theta(s_{tt}, a_{tt}) \right] - \frac{R(\phi_\alpha)}{\eta} \right) - \sum_{t=1}^T r_\theta(s_{tt}, a_{tt}^{\text{true}}), \]

donde \( \eta \) es un término de regularización, \( tt \) significa datos reales, \( \phi \) representa al generador y genera la próxima acción del usuario, y \( r \) es el discriminador que intenta diferenciar entre acciones generadas y acciones reales.

En DEERS [48], un simulador de usuario, con la misma arquitectura que DEERS, se entrena en registros de usuario. Sin embargo, la capa de salida del simulador es una capa softmax para predecir la retroalimentación del usuario (recompensa inmediata) basada en la entrada (par de estado e ítem recomendado). Los autores afirman que el simulador tiene un 90% de precisión en predecir la retroalimentación del usuario. El mismo enfoque para el estudio de simulación ha sido utilizado por otros RSs de RL [46, 119, 121, 128, 137, 143, 145, 148, 149, 160, 161, 172]. Por ejemplo, una idea similar se utiliza en [119], pero el simulador (S Network) proporciona diferentes retroalimentaciones, incluyendo la respuesta del usuario, tiempo de permanencia, tiempo de revisión y un indicador binario si el usuario se está yendo o no. En Pseudo Dyna-Q [137], se entrena un modelo del mundo (simulador de usuario) minimizando un error entre recompensas en línea y offline. Se utiliza muestreo de importancia truncado [189] para mitigar el sesgo en los datos offline.

Espero que esta traducción sea de ayuda.

Aquí está la traducción del texto restante:

### Otro método popular de simulación es desarrollar un simulador basado en filtrado colaborativo [40, 125, 163]. Para ser específicos, LIRD [40] construye una memoria con tuplas (𝑠,𝑎,𝑟) observadas en el conjunto de datos de registros y utiliza un método de similitud, basado en la similitud del coseno, para encontrar el par de estado-acción más cercano al estado actual y la acción recomendada. DRR [125] y DRGR [163] utilizan la misma intuición pero basada en factorización matricial probabilística [103] y factorización matricial, respectivamente.

Construir un simulador para RS conversacionales es más desafiante que para los escenarios de recomendación típicos mencionados anteriormente, ya que hay un pequeño número de conjuntos de datos públicos disponibles que tengan tanto calificaciones de usuario como lenguaje natural/conversaciones de usuario para ese par de calificación de ítem. CRM [117] aborda este problema creando usuarios simulados basados en datos de Yelp [190] y un corpus de diálogo, recopilado mediante trabajadores de crowdsourcing. Los usuarios simulados tienen tres comportamientos: responder la pregunta del agente, encontrar el ítem objetivo en una lista y abandonar el diálogo. El mismo esquema se ha utilizado en otros RS conversacionales de RL [122, 147, 153, 170].

En general, realizar un buen estudio en línea se ha vuelto más desafiante en los RS modernos con grandes espacios de usuarios e ítems, ya que el riesgo de implementar un RS no óptimo es muy alto. Como se discutió antes, esta es la razón más probable de una considerable disminución en la popularidad del estudio en línea entre los métodos basados en DRL en comparación con los métodos basados en RL. Quizás dos de los mejores estudios en línea entre los RS de RL se realizan en [25] y [80] y se llevaron a cabo en YouTube.

Aquí está la traducción del texto:

### 4 TEMAS EMERGENTES
Después de revisar los RS basados en DRL, hemos reconocido que hay un par de tendencias que se están formando y que tienen el potencial de madurar con el tiempo. En esta sección, revisamos brevemente estos temas emergentes.

**Aprendizaje por Refuerzo Multi-agente (MARL)**. El Aprendizaje por Refuerzo Multi-agente (MARL) es una generalización del aprendizaje por refuerzo de un solo agente y se formula como un juego estocástico/markoviano [191, 192]. MARL permite a los RS basados en DRL abordar varias tareas complejas dividiéndolas en sub-tareas, donde cada agente puede manejar una de ellas. Por ejemplo, en lugar de optimizar una sola estrategia para todos los escenarios en una aplicación de RS de comercio electrónico (como la página de entrada, la recomendación de productos y la finalización de compras), podría haber varios agentes de RS, cada uno responsable de un escenario específico, y la política final se optimiza conjuntamente entre ellos [149]. Desde una perspectiva de teoría de juegos, los métodos de MARL pueden dividirse generalmente en tres grupos: completamente cooperativos, completamente competitivos y una mezcla de ambos [192].

Recientemente, varios RS basados en DRL han empleado MARL para abordar problemas como la recomendación de colaboradores académicos [96], la recomendación de menciones en Twitter [127], la recomendación por página [148], la recomendación de cadenas completas [149] y la recomendación de puntos de carga [168]. Como se mencionó anteriormente, el actor-crítico con entrenamiento centralizado y ejecución descentralizada ha sido un marco popular para los RS basados en DRL que utilizan MARL [127, 148, 149, 168]. En un entorno cooperativo [127, 148, 149, 168], un desafío es determinar el papel de cada jugador en el éxito general del equipo. CROMA [127], con dos actores y un crítico centralizado, aborda este problema mediante un esquema de ventaja diferenciado utilizando operación inversa. Específicamente, cada agente actor puede estimar su ventaja particular restando el valor Q general de la acción conjunta, calculado por el crítico centralizado, del valor Q de una acción inversa. Una arquitectura similar se utiliza en DeepChain [149] para optimizar conjuntamente la recompensa general de una sesión. Sin embargo, no está claro cómo DeepChain resuelve el problema mencionado anteriormente, es decir, la recompensa compartida para dos actores, lo cual es crucial para su entrenamiento efectivo. En MASSA [148], se utiliza un MARL con agentes separados de actor y crítico para abordar una recomendación por página multi-módulo. Se utiliza un concepto de teoría de juegos llamado equilibrio correlacionado [193] en formato de red de señales para manejar la comunicación entre agentes.

MASTER [168] considera cada punto de carga para vehículos eléctricos como un agente distribuido y utiliza un crítico centralizado para coordinar estos agentes. Se emplean un par de técnicas, incluidos juegos de oferta y múltiples críticos, para abordar desafíos como la cooperación entre agentes, la competencia futura entre solicitudes y la optimización multiobjetivo. En un escenario competitivo diferente, los autores en [96] utilizan MARL para recomendar colaboradores científicos. Cada autor que busca un colaborador se considera un agente y aprende una política óptima utilizando el algoritmo de iteración de valor de gradiente.


### 4 TEMAS EMERGENTES
Tras revisar los RS basados en DRL, hemos identificado un par de tendencias que están tomando forma y que tienen el potencial de madurar con el tiempo. En esta sección, revisaremos brevemente estos temas emergentes.

**Aprendizaje por Refuerzo Jerárquico y Meta-controlador (HRL)**. El Aprendizaje por Refuerzo Jerárquico (HRL) inicialmente se buscó para abordar el problema de escalabilidad en los algoritmos tradicionales de RL [194]. Sin embargo, en HRL es posible definir múltiples capas de políticas, cada una de las cuales puede entrenarse para proporcionar niveles superiores de abstracciones temporales y conductuales, lo que permite resolver tareas más complejas [195, 196]. La recomendación no es una excepción y varios investigadores han utilizado HRL en el dominio de los RS [114, 123, 143, 165, 166, 171].

En general, todos estos RS basados en RL definen una HRL con dos niveles de jerarquías donde un agente de alto nivel define un objetivo alto/abstracto y un agente de bajo nivel intenta satisfacer ese objetivo. Por ejemplo, CEI [114] construye un RS conversacional sobre un método HRL profundo [197], que utiliza ideas de un marco HRL popular y tradicional, llamado opciones [198]. CEI utiliza un meta-controlador que selecciona un objetivo (conversación informal o recomendación) en un estado dado, y un controlador realiza una acción siguiendo una política específica del objetivo para satisfacer el objetivo definido. Zhang et al. [123] emplean HRL para la recomendación de cursos en cursos masivos en línea (MOOCs). La idea clave es desarrollar un revisor de perfiles utilizando HRL, que elimina cursos ruidosos de los perfiles de los usuarios. Esto se descompone en dos tareas de alto y bajo nivel: dado un perfil de usuario y un curso objetivo, ¿debería revisarse el perfil (alto nivel) y, si es así, qué cursos en el perfil deben eliminarse (bajo nivel)? DARL [166] mejora el RS de Zhang et al. haciendo que la unidad de recomendación sea más adaptable. Es decir, equipan el módulo básico de recomendación en el trabajo de Zhang con un mecanismo de atención para tener en cuenta el interés dinámico de los usuarios en diversos cursos. HRL-Rec [171] utiliza HRL en un escenario de recomendación integrada. Un agente de bajo nivel genera una lista de canales, y un agente de alto nivel recomienda una lista de elementos con la restricción de canal seleccionada por el agente de bajo nivel. Además, MaHRL [143] aborda la métrica de conversión escasa en comercio electrónico utilizando HRL. Más precisamente, hay un agente de alto nivel responsable de rastrear el interés de conversión escasa a largo plazo estableciendo múltiples objetivos abstractos para el agente de bajo nivel, mientras que el agente de bajo nivel sigue estos objetivos e intenta captar el interés de clics a corto plazo. Finalmente, DHCRS [165] intenta abordar el gran espacio de acción en los RS utilizando un HRL de dos niveles, donde un DQN de alto nivel selecciona categorías de elementos y un DQN de bajo nivel selecciona un elemento en la categoría para recomendar.

En un tema emergente, un grupo de investigadores ha utilizado RL como módulo meta-controlador en RS conversacionales. Esto significa que, en lugar de usar RL para optimizar la política de recomendación, similar a HRL, estos métodos utilizan RL para seleccionar ya sea la recomendación de elementos o hacer preguntas a los usuarios para refinar las recomendaciones. Pero a diferencia de HRL, hay solo un nivel que utiliza RL y la unidad de recomendación utiliza otras técnicas, como aprendizaje supervisado, para generar las recomendaciones. Este es el tema común en un par de RS basados en RL [117, 122, 131, 147, 152, 153, 170]. Por ejemplo, CRM [117] se compone de tres partes principales: un rastreador de creencias, un recomendador y una red de políticas (módulo RL). La unidad de rastreo de creencias es responsable de extraer pares faceta-valor (algunas restricciones) de las expresiones de los usuarios y convertirlas en creencias utilizando una red LSTM. Se utiliza una máquina de factorización [199] en el recomendador para generar un conjunto de recomendaciones. Finalmente, se utiliza una red de políticas neurales, optimizada por REINFORCE, para gestionar el sistema conversacional, es decir, decidir si pedir más información al usuario o recomendar los elementos.

**RS basados en grafos de conocimiento.** La incorporación de grafos de conocimiento en RS puede aumentar la precisión y la explicabilidad de las recomendaciones [38]. Utilizar grafos de conocimiento proporciona a los RS basados en RL diferentes información útil, que puede abordar la ineficiencia de muestras en DRL. Recientemente, muchos investigadores comenzaron a utilizar esta idea para mejorar el rendimiento y la explicabilidad de las recomendaciones [132, 136, 150, 151, 155–158, 167, 169, 170]. Por ejemplo, la idea en KGRE-Rec [132] es no solo recomendar un conjunto de elementos, sino también los caminos en el grafo de conocimiento para mostrar la razón por la cual el método ha hecho estas recomendaciones. Un ejemplo de este razonamiento gráfico se muestra en la Fig. 10(c). Para un usuario dado 𝐴, el algoritmo debería encontrar los elementos 𝐵 y 𝐹 con sus caminos de razonamiento en el grafo, como {Usuario 𝐴 → Elemento 𝐴 → Marca 𝐴 → Elemento 𝐵} y {Usuario 𝐴 → Característica 𝐵 → Elemento 𝐹}. Obviamente, las técnicas basadas en gráficos enfrentan el problema de escalabilidad a medida que el número de nodos y enlaces puede crecer significativamente, en proporción al número de usuarios y elementos. Para abordar este problema, KGRE-Rec propone una estrategia de poda de acciones condicionales al usuario, que utiliza una función de puntuación para mantener solo los bordes importantes condicionados en el usuario inicial.

**RL Supervisado.** La característica clave que distingue RL del aprendizaje supervisado es si los datos de entrenamiento sirven como una señal de evaluación, como recompensa numérica, o como una señal de error [200]. Sin embargo, estos métodos, RL y aprendizaje supervisado, pueden combinarse para mejorar el aprendizaje de políticas cuando ambos tipos de señales están disponibles. Wang et al. [116] utilizan esta idea para recomendar dinámicamente opciones de tratamiento a pacientes. La idea es que mientras el modelo debe maximizar el retorno esperado, también debe minimizar la diferencia de las prescripciones médicas. En particular, en una arquitectura actor-crítico, el actor es responsable de recomendar la mejor prescripción optimizando la siguiente función objetivo:

\[ \mathcal{J}(\theta) = (1 - \alpha)\mathcal{J}_{RL}(\theta) + \alpha(-\mathcal{J}_{SL}(\theta)) \]

donde \( \mathcal{J}_{RL}(\theta) \) y \( \mathcal{J}_{SL}(\theta) \) son las funciones objetivo de las tareas de RL y aprendizaje supervisado, respectivamente, y \( \alpha \) es un factor de ponderación. De manera similar, Liu et al. [160, 161] aprovechan el aprendizaje supervisado para guiar el módulo RL en el aprendizaje de políticas mejores. Más precisamente, en [160], una señal de aprendizaje supervisado ayuda a generar mejores incrustaciones para la representación del estado, y en [161], se entrena un modelo de aprendizaje supervisado para guiar la política RL para centrarse en la recompensa a corto plazo y generar recomendaciones orientadas a los principales.

**Aprendizaje por Imitación y Tareas Auxiliares.** Además de los temas emergentes mencionados anteriormente, hay algunos temas que, aunque son menos populares en comparación con los discutidos anteriormente, creemos que tienen el potencial de convertirse en temas emergentes en el futuro. Estos temas incluyen RL/adversarial, RL seguro, aprendizaje auto-supervisado y aprendizaje por imitación. El entrenamiento adversarial utilizando GANs es un tema emergente interesante utilizado en [130, 152, 155]. Como se mencionó anteriormente en la sección de Métodos Actor-Crítico, CRSAL [152] y AD

AC [155] utilizan entrenamiento adversarial integrado con arquitectura actor-crítico para un mejor entrenamiento del agente. Además, como se discutió en los métodos de gradiente de política, IRecGAN [130] propone un RL basado en modelos utilizando GANs con el propósito de reducción de varianza y eficiencia de muestras.

En RL seguro, es importante que el agente respete algunas restricciones de seguridad, junto con maximizar la recompensa a largo plazo [201]. En la Ref. [159], se propone un RS basado en RL seguro multiobjetivo para mejorar el bienestar a largo plazo de los usuarios. En particular, el agente intenta simultáneamente maximizar la participación del usuario y la salud del usuario en el peor de los casos.

El aprendizaje auto-supervisado (SSL) capacita al modelo para utilizar etiquetas disponibles libremente con los datos. En [154], se introduce un marco para aumentar los RS basados en RL con SSL. Más precisamente, los autores proponen un marco con dos cabezas: RL y SSL. Mientras que la cabeza de RL se utiliza como regularizador para ajustar las recomendaciones, la cabeza de SSL proporciona muestras negativas para actualizar los parámetros.

En el aprendizaje por imitación, el agente se entrena para realizar una tarea a partir de demostraciones [202]. Zhang et al. [124] combinan la imaginación (RL basado en modelos) y el aprendizaje por imitación para recomendar historias de búsqueda personalizadas. Argumentan que el objetivo del aprendizaje por imitación es imitar la política de un agente recomendador del cual se ha recopilado datos de registro. Además, se imaginan algunas sesiones ficticias por parte del agente y se guardan en una memoria separada, que se utiliza para ajustar el entrenamiento del agente.

Estos temas emergentes indican nuevas direcciones y enfoques en el campo de los RS basados en RL, mostrando cómo la innovación continúa expandiendo las posibilidades de mejorar la precisión, la explicabilidad y la eficiencia de los sistemas de recomendación mediante técnicas avanzadas de aprendizaje automático.


5 DIRECCIONES DE INVESTIGACIÓN ABIERTAS

Recomendación de Conjuntos de Elementos. Los algoritmos de RL fueron desarrollados originalmente para seleccionar una sola acción, por ejemplo, la acción con el valor Q más alto, en cada paso de tiempo entre diferentes acciones circundantes [203]. Sin embargo, en el campo de los RS, similar a muchos sistemas de soporte para decisiones secuenciales [203], es prudente recomendar un conjunto o lista de elementos y permitir que el usuario participe en el proceso de toma de decisiones para elegir la mejor acción, ya que el objetivo final suele ser la satisfacción del usuario y la aceptación de la recomendación. A pesar de algunos esfuerzos [25, 40, 80, 112, 135], los algoritmos de RL actuales no pueden manejar este problema. Solo hay dos estudios [80, 112] en el campo de los RLRS que investigan profundamente este problema. Slate-MDP [112] intenta resolver este problema buscando el espacio de políticas para cada espacio en el conjunto individualmente. SlateQ [80] propone calcular la combinación del conjunto de acciones y considerar cada combinación como una acción. Slate-MDP no puede garantizar ninguna optimalidad, y SlateQ solo es aplicable en RSs de dos etapas y no logra escalar a RSs de una sola etapa con grandes espacios de acciones. Se necesita más atención en este aspecto y se deben realizar más estudios con bases teóricas sólidas en el futuro.

Explicabilidad. La recomendación explicativa es la capacidad de un RS no solo para proporcionar una recomendación, sino también para explicar por qué se ha hecho una recomendación específica [38]. La explicación sobre las recomendaciones realizadas podría mejorar la experiencia del usuario, aumentar su confianza en el sistema y ayudarles a tomar mejores decisiones [204–206]. Los métodos explicativos podrían dividirse en dos grupos generales: intrínsecos al modelo o agnósticos al modelo [207]. En el primero, la explicación es parte del proceso de recomendación, mientras que en el segundo, la explicación se proporciona después de que se hace la recomendación. Un método de explicación intrínseco podría ser el método que revisamos anteriormente [132]. Por otro lado, como ejemplo agnóstico al modelo [208], se utiliza RL para proporcionar explicaciones para diferentes métodos de recomendación. En particular, el método utiliza un par de agentes; uno es responsable de generar explicaciones y otro predice si la explicación generada es lo suficientemente buena para el usuario. Una aplicación interesante de la recomendación explicativa es en la depuración de RSs fallidos [208]. Es decir, a través de las explicaciones proporcionadas, podemos rastrear la fuente de problemas en nuestro sistema y ver qué partes no están funcionando correctamente. Aunque ha habido algunos esfuerzos en RLRSs para proporcionar recomendaciones explicativas [132, 136, 155, 167, 169], todavía hay una falta en este aspecto y se requiere más atención en el futuro.

Diseño. Todos los RLRSs revisados emplean algoritmos de RL/DRL que originalmente fueron desarrollados en dominios distintos de los RSs, como los juegos [62, 67, 74]. Estos métodos generalmente están diseñados basados en procesos físicos o procesos gaussianos, no basados en la naturaleza compleja y dinámica de los seres humanos. Si bien es prudente adherirse a los algoritmos de RL de vanguardia disponibles y adaptarlos para RLRSs, a veces pensar de manera innovadora podría mejorar sustancialmente el campo. Por ejemplo, en lugar de los algoritmos de RL basados en MDP usuales, la Ref. [209] utiliza estrategias de evolución [210] para optimizar la política de recomendación, o la Ref. [211] adopta ideas de una literatura diferente y las adapta al problema de recomendación. Relacionado con esto, como se encuestó en [14], hay muchos modelos de aprendizaje profundo desarrollados para RSs. Dado que el aprendizaje profundo y el DRL están estrechamente relacionados, combinar sabiamente estos modelos con algoritmos de RL tradicionales podría superar a los algoritmos DRL existentes. Por último, aunque algunos algoritmos de RL como Q-learning han sido más populares entre los RLRSs que otros algoritmos de RL, no hay pistas o justificaciones detrás del uso de un algoritmo de RL específico para una aplicación de RS. Por lo tanto, sería un gran estudio encontrar posiblemente una relación entre el algoritmo de RL y la aplicación de RS.

Ambiente y Evaluación. La Fig. 11(a) muestra las métricas más populares en los RLRSs. Como se muestra, no hay una métrica específicamente desarrollada para los RLRSs y casi todas las métricas se han tomado del campo de la Recuperación de Información. Aunque los campos de RSs y Recuperación de Información son muy cercanos, son eventualmente campos diferentes. La recompensa también está entre las métricas populares utilizadas por los RLRSs, que es una métrica común en la literatura de RL. Este análisis muestra que hay una falta de métricas específicamente diseñadas para los RLRSs. Por otro lado, la Fig. 11(b) ilustra los conjuntos de datos más populares utilizados para evaluar los RLRSs. MovieLens es el conjunto de datos más popular por mucho. Un gran número de conjuntos de datos utilizados para la evaluación no están definidos o son públicos (consulte las Tablas 2 y 3). Esto limita la aplicación y el diseño de los RLRSs a solo unas pocas aplicaciones, como el entretenimiento. Por lo tanto, es importante recopilar y compartir más conjuntos de datos en diversos dominios para evaluar mejor los RLRSs. Otro aspecto en la evaluación de los RLRSs que necesita más mejora en el futuro es tener un entorno de simulación más fuerte y unificado, algo que pueda desempeñar el papel de un punto de referencia en la evaluación de los RLRSs. Como se mencionó anteriormente, la evaluación en línea es el método natural para evaluar los RLRSs; sin embargo, es difícil y costoso realizar un estudio en línea adecuado. Por otro lado, el entorno fuera de línea, es decir, un conjunto de datos, es estático y sesgado. Por lo tanto, esto muestra claramente la importancia de desarrollar un simulador general y sólido para los RLRSs, algo similar a OpenAI Gym [212] en la literatura de RL. Aunque recientemente se han desarrollado simuladores de entorno para RLRSs [213–217], esta tendencia debe continuar y fortalecerse.

Reproducibilidad. El efecto de la reproducibilidad en el avance de un campo es innegable. Por ejemplo, el campo de la síntesis de imágenes utilizando GANs ha visto resultados asombrosos en un corto período de tiempo [218–220], y sin duda, un factor efectivo ha sido la práctica común de compartir códigos de implementación, conjuntos de datos y resultados de investigación. Como se ilustra en las Tablas 2 y 3, no podemos ver esta tendencia en la comunidad de investigación de RLRS y solo alrededor del 16% de los investigadores han compartido sus códigos de implementación. Sería útil y podría acelerar significativamente el progreso del campo si los investigadores presentaran con precisión el valor de los parámetros importantes y los hiperparámetros utilizados en sus experimentos, realizaran pruebas de significancia estadística para los resultados presentados, revelaran qué semillas aleatorias se han utilizado para repetir los experimentos y compartieran sus códigos de implementación y conjuntos de datos (si los conjuntos de datos no son públicos).

6. CONCLUSIÓN

En este artículo, presentamos una encuesta exhaustiva sobre el estado del arte en los Sistemas de Recomendación basados en RL. Destacamos el papel importante del DRL en cambiar la dirección de investigación en el campo de los RLRS, y en consecuencia, clasificamos los algoritmos en dos grupos generales, es decir, métodos basados en RL y DRL. Luego, propusimos un marco para los RLRS con cuatro componentes: representación del estado, optimización de políticas, formulación de recompensas y construcción del entorno, y revisamos los algoritmos en consecuencia. Aunque se han propuesto muchos RLRS recientemente, creemos que la investigación en RLRS todavía está en sus primeras etapas y necesita muchos avances. Tanto RL como los RS son áreas de investigación activas y de interés específico para grandes empresas y negocios, por lo que podemos esperar ver nuevos y emocionantes modelos surgir cada año. Finalmente, esperamos que esta encuesta pueda ayudar a los investigadores a comprender los conceptos clave y a avanzar en el campo en el futuro.