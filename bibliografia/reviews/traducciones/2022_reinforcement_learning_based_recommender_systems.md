Los sistemas de recomendaci√≥n (RSs) se han convertido en una parte inseparable de nuestra vida cotidiana. Nos ayudan a encontrar nuestros art√≠culos favoritos para comprar, nuestros amigos en redes sociales y nuestras pel√≠culas favoritas para ver. Tradicionalmente, el problema de la recomendaci√≥n se consideraba un problema de clasificaci√≥n o predicci√≥n, pero ahora est√° ampliamente aceptado que formularlo como un problema de decisi√≥n secuencial puede reflejar mejor la interacci√≥n usuario-sistema. Por lo tanto, puede formularse como un proceso de decisi√≥n de Markov (MDP) y resolverse mediante algoritmos de aprendizaje por refuerzo (RL). A diferencia de los m√©todos tradicionales de recomendaci√≥n, como el filtrado colaborativo y el basado en contenido, RL puede manejar la interacci√≥n usuario-sistema secuencial y din√°mica, y tener en cuenta el compromiso a largo plazo del usuario. Aunque la idea de utilizar RL para la recomendaci√≥n no es nueva y ha existido durante aproximadamente dos d√©cadas, no era muy pr√°ctica, principalmente debido a problemas de escalabilidad de los algoritmos de RL tradicionales. Sin embargo, desde la introducci√≥n del aprendizaje por refuerzo profundo (DRL), ha surgido una nueva tendencia en el campo que ha hecho posible aplicar RL al problema de la recomendaci√≥n con espacios de estados y acciones grandes. En este art√≠culo, se presenta una revisi√≥n de los sistemas de recomendaci√≥n basados en aprendizaje por refuerzo (RLRSs). Nuestro objetivo es ofrecer una perspectiva sobre el campo y proporcionar al lector un conocimiento bastante completo de los conceptos clave del mismo. Primero reconocemos e ilustramos que los RLRSs se pueden clasificar generalmente en m√©todos basados en RL y DRL. Luego, proponemos un marco de trabajo para RLRS con cuatro componentes, es decir, representaci√≥n del estado, optimizaci√≥n de la pol√≠tica, formulaci√≥n de la recompensa y construcci√≥n del entorno, y revisamos los algoritmos RLRS en consecuencia. Destacamos temas emergentes y representamos tendencias importantes mediante diversos gr√°ficos y tablas. Finalmente, discutimos aspectos importantes y desaf√≠os que pueden abordarse en el futuro.

1. INTRODUCCI√ìN
Estamos viviendo en la Era del Zettabyte [1]. El volumen masivo de informaci√≥n disponible en la web conduce al problema de la sobrecarga de informaci√≥n, lo cual dificulta que un tomador de decisiones pueda tomar decisiones correctas. Nos damos cuenta de esto en nuestra vida cotidiana cuando nos enfrentamos a una larga lista de art√≠culos en una tienda en l√≠nea; cuantos m√°s art√≠culos en la lista, m√°s dif√≠cil se vuelve seleccionar entre ellos. Los sistemas de recomendaci√≥n (RSs) son herramientas de software y algoritmos que se han desarrollado con la idea de ayudar a los usuarios a encontrar sus art√≠culos de inter√©s, prediciendo sus preferencias o valoraciones sobre los art√≠culos [2, 3]. De hecho, la idea es conocer a los usuarios hasta cierto punto, es decir, crear un perfil de usuario basado en sus retroalimentaciones sobre los art√≠culos, y recomendar aquellos art√≠culos que coincidan con su perfil. Hoy en d√≠a, los RSs son parte esencial de la mayor√≠a de las grandes compa√±√≠as, como Google, Facebook, Amazon y Netflix, y se emplean en una amplia gama de aplicaciones, incluyendo entretenimiento [4‚Äì6], comercio electr√≥nico [7], noticias [8], e-learning [9] y cuidado de la salud [10].

Se han propuesto numerosas t√©cnicas para abordar el problema de la recomendaci√≥n; las t√©cnicas tradicionales incluyen el filtrado colaborativo, el filtrado basado en contenido y m√©todos h√≠bridos. A pesar de cierto √©xito en proporcionar recomendaciones relevantes, especialmente despu√©s de la introducci√≥n de la factorizaci√≥n de matrices [11], estos m√©todos tienen problemas severos, como el inicio en fr√≠o (es decir, el sistema no puede proporcionar recomendaciones √∫tiles cuando el usuario o el art√≠culo son nuevos), falta de novedad y diversidad, escalabilidad, recomendaciones de baja calidad y gran gasto computacional [2, 3, 12]. Recientemente, el aprendizaje profundo [13] tambi√©n ha ganado popularidad en el campo de los RSs debido a su capacidad para encontrar relaciones complejas y no lineales entre usuarios y art√≠culos, y su rendimiento de vanguardia en la recomendaci√≥n [14]. Sin embargo, los modelos de aprendizaje profundo suelen ser no interpretables, requieren una gran cantidad de datos (esto es especialmente problem√°tico dado que la cantidad de datos, es decir, retroalimentaci√≥n de usuarios/calificaciones, en el campo de los RSs es escasa) y son computacionalmente costosos [14].

RL es un campo de aprendizaje autom√°tico semi-supervisado en el que el agente optimiza su comportamiento mediante la interacci√≥n con el entorno. El hito en el campo de RL es la combinaci√≥n del aprendizaje profundo con m√©todos tradicionales de RL, conocida como aprendizaje por refuerzo profundo (DRL) [15, 16]. Esto hizo posible aplicar RL en problemas con enormes espacios de estados y acciones, incluyendo autos aut√≥nomos [17, 18], rob√≥tica [19], automatizaci√≥n industrial [20], finanzas [21], salud [22, 23] y RSs [24]. La capacidad √∫nica de un agente de RL para aprender a partir de una recompensa del entorno sin necesidad de datos de entrenamiento hace que RL sea espec√≠ficamente adecuado para el problema de la recomendaci√≥n. Hoy en d√≠a, cada vez m√°s empresas est√°n utilizando el poder de RL para recomendar mejores art√≠culos a sus clientes. Por ejemplo, en un estudio realizado por investigadores de Google [25], se muestra que RL puede utilizarse para recomendar mejor contenido de video a los usuarios de YouTube. De hecho, el uso de RL en la comunidad de RSs no se limita a la industria, sino que tambi√©n se est√° convirtiendo en una tendencia en el √°mbito acad√©mico. La Fig. 1(a) ilustra esta tendencia.

Esta tendencia y este tema nos motivaron a preparar este art√≠culo de revisi√≥n, que tiene como objetivo proporcionar una visi√≥n general completa del estado del arte en sistemas de recomendaci√≥n basados en aprendizaje por refuerzo (RLRSs). Nuestro prop√≥sito principal es ofrecer una imagen de alto nivel desde el progreso en el campo desde sus inicios y mostrar c√≥mo esta tendencia ha cambiado significativamente con la llegada del DRL. Al mismo tiempo, proporcionamos informaci√≥n detallada sobre cada m√©todo en forma de tablas para que el lector pueda observar f√°cilmente las similitudes y diferencias entre los m√©todos.

Metodolog√≠a de Recolecci√≥n de Art√≠culos. Para recolectar los art√≠culos relevantes, hemos utilizado un proceso de b√∫squeda multinivel. El enfoque de este art√≠culo de revisi√≥n se centra espec√≠ficamente en RSs que utilizan un algoritmo de RL. En consecuencia, para encontrar art√≠culos relevantes, utilizamos Google Scholar como motor de b√∫squeda principal y buscamos la palabra clave "reinforcement learning recommender system". Esta b√∫squeda arroj√≥ alrededor de 33,000 art√≠culos. De los primeros 1000 art√≠culos encontrados, recolectamos 500 como resultado de nuestro primer nivel de cribado. Luego, para aumentar la fiabilidad de nuestra colecci√≥n de art√≠culos, exploramos tambi√©n bibliotecas relacionadas como ACM Digital Library, IEEE Xplore, SpringerLink y ScienceDirect con la misma palabra clave, hasta el punto en que no encontramos m√°s art√≠culos relevantes entre los resultados. Descubrimos que todos los art√≠culos relacionados identificados en estas bibliotecas estaban disponibles en nuestra b√∫squeda inicial utilizando Google Scholar. Despu√©s de estudiar cuidadosamente los art√≠culos recolectados y excluir los que eran irrelevantes, duplicados, tesis, y art√≠culos de revisi√≥n, seleccionamos 97 art√≠culos para incluir en nuestro art√≠culo de revisi√≥n. Aunque estamos seguros de que no encontramos todos los RLRSs a trav√©s del proceso de b√∫squeda explicado, confiamos en haber encontrado la gran mayor√≠a de las publicaciones relevantes.

Es importante mencionar que no incluimos RSs basados en bandas multi-brazo. Los bandas son una versi√≥n simplificada de RL. En particular, en las bandas, al igual que en un problema de RL, el agente debe aprender a maximizar una recompensa num√©rica a trav√©s de la interacci√≥n con el entorno y resolver el dilema de exploraci√≥n versus explotaci√≥n [26, 27]. Sin embargo, en las bandas, a diferencia del RL completo, las acciones no afectan el estado del entorno ni la recompensa [27]. Aunque las bandas han sido populares para el problema de la recomendaci√≥n [28‚Äì31], dada las aplicaciones exitosas recientes de DRL y el inter√©s sin precedentes en RL completo por parte de la comunidad de RSs, optamos por enfocarnos √∫nicamente en RSs que utilizan un algoritmo de RL completo. Se recomienda al lector interesado consultar una revisi√≥n reciente sobre la aplicaci√≥n de bandas en RSs [32].

Trabajo Relacionado. Se ha realizado una gran cantidad de investigaci√≥n en el campo de los RSs y se han publicado numerosos art√≠culos de revisi√≥n, incluyendo RSs [12], filtrado colaborativo [33, 34], m√©todos h√≠bridos [35], RSs multimedia [36, 37], recomendaci√≥n explicativa [38] y RSs de art√≠culos [39], entre otros. Tambi√©n existen algunos art√≠culos de revisi√≥n publicados sobre temas estrechamente relacionados con RLRSs [14, 40‚Äì42]. Quiz√°s los dos art√≠culos de revisi√≥n m√°s cercanos al nuestro sean [14, 40]. La Ref. [40] revisa t√©cnicas basadas en DRL para la b√∫squeda de informaci√≥n, como la b√∫squeda, la recomendaci√≥n y la publicidad en l√≠nea. Los autores discuten varios RSs que utilizan bandas multi-brazo y DRL para la optimizaci√≥n de pol√≠ticas. Sin embargo, este trabajo omite muchos RLRSs importantes y no proporciona un an√°lisis en profundidad de los algoritmos revisados. Zhang et al. [14] ofrecen una revisi√≥n exhaustiva de los RSs basados en aprendizaje profundo. Consideran DRL como un paradigma arquitect√≥nico de aprendizaje profundo y revisan algunos RSs basados en DRL [43‚Äì48]. Sin embargo, esta clasificaci√≥n no es correcta, ya que DRL no es una arquitectura de aprendizaje profundo, sino una extensi√≥n de los algoritmos de RL tradicionales. Otras revisiones relacionadas se centran en t√©cnicas de recomendaci√≥n sensibles a la secuencia [41] y basadas en sesiones [42]. La Ref. [41] revisa RSs sensibles a la secuencia. Los autores consideran RL como un m√©todo para el aprendizaje de secuencias y revisan algunos RLRSs [49, 50]. En otra revisi√≥n relacionada [42], RL se considera como un m√©todo para RSs basados en sesiones [46, 51, 52]. Ninguno de los art√≠culos de revisi√≥n previamente publicados proporciona una visi√≥n general completa y un an√°lisis en profundidad de los RLRSs publicados. Hasta donde llega nuestro conocimiento, este es el primer art√≠culo de revisi√≥n que se centra espec√≠ficamente en RLRSs.

Nuestra contribuci√≥n. El objetivo es proporcionar al lector una vista hacia el campo para que puedan entender r√°pidamente el tema y las principales tendencias y algoritmos presentados hasta ahora. Esto ayuda a los investigadores a tener una visi√≥n general, comparar las fortalezas y debilidades de los algoritmos, y arrojar luz sobre formas de avanzar en el futuro. Nuestras principales contribuciones se pueden resumir como:

Presentaci√≥n de un marco para RLRSs. Primero dividimos generalmente los RLRSs en m√©todos basados en RL y DRL. Luego, proponemos un marco con cuatro componentes principales, es decir, representaci√≥n del estado, optimizaci√≥n de la pol√≠tica, formulaci√≥n de la recompensa y construcci√≥n del entorno. Este marco puede modelar todos los RLRSs y unificar el proceso de desarrollo de los RLRSs.

El resto de este art√≠culo est√° organizado de la siguiente manera. En la secci√≥n 2, para ayudar al lector a entender mejor el tema, discutimos algunos conceptos preliminares y proporcionamos un s√≥lido contexto sobre RL. La secci√≥n 3 presenta los algoritmos de RLRSs de manera clasificada. Los temas emergentes se destacan en la secci√≥n 4. En la secci√≥n 5 se sugieren algunas direcciones de investigaci√≥n abiertas para el trabajo futuro, y finalmente, el art√≠culo concluye en la secci√≥n 6.

**2 PRELIMINARES**

En esta secci√≥n, ofrecemos un contexto sobre los conceptos importantes discutidos a lo largo de este art√≠culo. Este trasfondo proporciona al lector informaci√≥n √∫til y concisa sobre los RSs, RL y DRL, por qu√© es necesario usar RL en RSs, la formulaci√≥n del problema y el marco propuesto para los RLRSs.

**2.1 Sistemas de Recomendaci√≥n**

En la vida cotidiana, no es raro enfrentarse a situaciones en las que debemos tomar decisiones sin tener informaci√≥n previa sobre las opciones. En tales casos, parece bastante necesario depender de recomendaciones de otros, que tienen experiencia en ese aspecto [53]. Esta fue la raz√≥n detr√°s del primer RS, Tapestry [54], y los autores lo denominaron filtrado colaborativo. M√°s tarde, este t√©rmino se ampli√≥ a sistemas de recomendaci√≥n para reflejar dos hechos [53]: 1) el m√©todo puede no basarse en una colaboraci√≥n impl√≠cita entre usuarios, 2) el m√©todo puede sugerir elementos interesantes, no solo filtrarlos. Por definici√≥n, los RSs son herramientas de software y algoritmos que sugieren elementos que podr√≠an interesar a los usuarios [3].

Otro enfoque importante hacia el problema de la recomendaci√≥n es el filtrado basado en contenido, en el cual la idea es utilizar descripciones de elementos y dise√±ar un m√©todo para relacionarlos con el perfil del usuario, una representaci√≥n estructurada de los intereses del usuario [55, 56]. El filtrado colaborativo suele sufrir de esparcimiento de datos, escalabilidad y ovejas grises (usuarios con gustos especiales cuyas opiniones no coinciden o discrepan con la mayor√≠a de los usuarios) [33]. El filtrado basado en contenido tambi√©n tiene algunas limitaciones, incluyendo an√°lisis de contenido limitado, serendipia y nuevos usuarios [56]. Los m√©todos h√≠bridos, una combinaci√≥n de ambos, pueden mitigar solo parte de estos problemas [3, 33].

**2.2 Desde el Aprendizaje por Refuerzo hasta el Aprendizaje Profundo por Refuerzo**

El aprendizaje por refuerzo (RL) es un campo de aprendizaje autom√°tico que estudia problemas y sus soluciones en los cuales los agentes, a trav√©s de la interacci√≥n con su entorno, aprenden a maximizar una recompensa num√©rica. Seg√∫n Sutton y Barto [27], tres caracter√≠sticas distinguen un problema de RL: (1) el problema es de lazo cerrado, (2) el aprendiz no tiene un tutor que le ense√±e qu√© hacer, sino que debe descubrir qu√© hacer a trav√©s de prueba y error, y (3) las acciones influyen no solo en los resultados a corto plazo, sino tambi√©n en los resultados a largo plazo. La interfaz m√°s com√∫n para modelar un problema de RL es la interfaz agente-entorno, representada en la Fig. 2(a). El aprendiz o tomador de decisiones se llama agente y el entorno es todo lo que est√° fuera del agente. De acuerdo con esto, en el paso de tiempo ùë°, el agente ve algunas representaciones/informaci√≥n sobre el entorno, llamadas estado, y basado en el estado actual toma una acci√≥n. Al tomar esta acci√≥n, recibe una recompensa num√©rica del entorno y se encuentra en un nuevo estado.

M√°s formalmente, el problema de RL se formula t√≠picamente como un proceso de decisi√≥n de Markov (MDP) en forma de una tupla (S, A, R, P, ùõæ), donde S es el conjunto de todos los estados posibles, A es el conjunto de acciones disponibles en todos los estados, R es la funci√≥n de recompensa, P es la probabilidad de transici√≥n, y ùõæ es el factor de descuento.

Los principales elementos de un sistema de RL son [27]:
- **Pol√≠tica**: la pol√≠tica generalmente se indica por ùúã y da la probabilidad de tomar la acci√≥n ùëé cuando el agente est√° en el estado ùë†. Con respecto a la pol√≠tica, los algoritmos de RL generalmente se dividen en m√©todos on-policy y off-policy. En los primeros, los m√©todos de RL tienen como objetivo evaluar o mejorar la pol√≠tica que est√°n utilizando para tomar decisiones. En los √∫ltimos, mejoran o eval√∫an una pol√≠tica que es diferente de la que se utiliz√≥ para generar los datos.
- **Se√±al de recompensa**: al seleccionar acciones, el entorno proporciona una recompensa num√©rica para informar al agente qu√© tan buenas o malas son las acciones seleccionadas.
- **Funci√≥n de valor**: la se√±al de recompensa simplemente puede decir qu√© es bueno inmediatamente, pero la funci√≥n de valor define qu√© es bueno a largo plazo.
- **Modelo**: el modelo proporciona la oportunidad de hacer inferencias sobre el comportamiento del entorno. Por ejemplo, el modelo puede predecir el pr√≥ximo estado y la pr√≥xima recompensa en un estado y acci√≥n dados [27].

**Algoritmos**: Se han propuesto muchos algoritmos para resolver un problema de RL; generalmente se pueden dividir en m√©todos tabulares y m√©todos aproximados [27]. En los m√©todos tabulares, dado que el tama√±o de los espacios de acci√≥n y estado es peque√±o, las funciones de valor se pueden representar como tablas y se puede encontrar una funci√≥n de valor y pol√≠tica √≥ptimas. Por otro lado, en los m√©todos aproximados, dado que el tama√±o del espacio de estados es enorme, el objetivo es encontrar una buena soluci√≥n aproximada con la restricci√≥n de recursos computacionales limitados. Como se mencion√≥ anteriormente, con la base de DRL, ha surgido un cambio sustancial en el campo de RL en general. En consecuencia, aunque DRL pertenece al grupo aproximado, generalmente dividimos los algoritmos de RL utilizados por los RLRSs en algoritmos basados en RL y basados en DRL, ya que creemos que esta clasificaci√≥n refleja mejor la tendencia reciente en el campo de los RLRSs. Es digno de mencionar que el factor distintivo entre los algoritmos DRL y los algoritmos tradicionales de RL es que los algoritmos DRL utilizan aprendizaje profundo para la aproximaci√≥n de funciones (se presenta una explicaci√≥n m√°s detallada sobre esto en la secci√≥n de Algoritmos basados en DRL). A continuaci√≥n, revisamos brevemente esos algoritmos de RL empleados por los RLRSs. La Fig. 2(b) ilustra estos algoritmos.

1) **Algoritmos basados en RL**: Como se mencion√≥ antes, los algoritmos de RL se pueden dividir en m√©todos tabulares y m√©todos aproximados. Los m√©todos tabulares populares incluyen programaci√≥n din√°mica, Monte Carlo y diferencia temporal. Los m√©todos de programaci√≥n din√°mica asumen un modelo perfecto del entorno y utilizan una funci√≥n de valor para buscar buenas pol√≠ticas. Dos algoritmos importantes de esta clase son la iteraci√≥n de pol√≠tica y la iteraci√≥n de valor. El algoritmo de iteraci√≥n de pol√≠tica consta de tres pasos: inicializaci√≥n, evaluaci√≥n de la pol√≠tica y mejora de la pol√≠tica. Primero, la pol√≠tica se inicializa al azar, es decir, se selecciona una acci√≥n aleatoria ùëé ‚àà ùê¥(ùë†) para todos los ùë† ‚àà S. Luego, se calcula y eval√∫a el valor de los estados utilizando

\[ ùëâ (ùë†) ‚Üê \sum_{ùë†‚Ä≤,ùëü} ùëù(ùë†‚Ä≤,ùëü|ùë†,ùúã(ùë†))[ùëü +ùõæùëâ (ùë†‚Ä≤)], \]

donde ùëù es la probabilidad de transici√≥n y ùë†‚Ä≤ es el pr√≥ximo estado. Finalmente, la pol√≠tica se actualiza de la siguiente manera, ‚àÄùë† ‚àà S:

\[ ùúã(ùë†) ‚Üê \arg \max_{ùëé} \sum_{ùë†‚Ä≤,ùëü} ùëù(ùë†‚Ä≤,ùëü|ùë†,ùëé)[ùëü +ùõæùëâ (ùë†‚Ä≤)]. \]

Como se se√±ala en [27], un problema con el algoritmo de iteraci√≥n de pol√≠tica es que necesita evaluar la pol√≠tica en cada iteraci√≥n, lo cual puede ser computacionalmente prohibitivo. El algoritmo de iteraci√≥n de valor es un caso especial del algoritmo de iteraci√≥n de pol√≠tica en el que la evaluaci√≥n de la pol√≠tica se detiene despu√©s de un barrido. M√°s precisamente, ùëâ (ùë†) se inicializa al azar ‚àÄùë† ‚àà S. Luego, se actualiza en cada paso seg√∫n

3.6 Algoritmos basados en DRL
Los algoritmos basados en DRL representan una interesante combinaci√≥n de aprendizaje profundo con RL. De hecho, investigadores de DeepMind descubrieron que esta combinaci√≥n puede lograr un rendimiento a nivel humano en juegos de Atari [67, 68]. La red neuronal convolucional profunda Q-network (DQN) [67] es una combinaci√≥n creativa de redes neuronales convolucionales (CNN) [13] con Q-learning. M√°s precisamente, en DQN, una red Q se encarga de la aproximaci√≥n del valor de acci√≥n, la cual puede entrenarse para minimizar la siguiente funci√≥n de p√©rdida:

\[ L_i(\theta_i) = \mathbb{E}_{s,a \sim \rho(\cdot)} \left[ \left( y_i - Q(s,a;\theta_i) \right)^2 \right], \]

donde \( y_i = \mathbb{E}_{s'} \left[ r + \gamma \max_{a'} Q(s',a';\theta_{i-1}) \mid s,a \right] \) es el objetivo para la iteraci√≥n \( i \) y \( \rho \) es una distribuci√≥n de probabilidad sobre transiciones \( s,a,r,s' \) recolectadas del entorno. Diferenciando \( L(\theta) \) en Eq. (14) con respecto a \( \theta \) se obtiene el siguiente gradiente:

\[ \nabla_{\theta_i} L_i(\theta_i) = \mathbb{E}_{s,a \sim \rho(\cdot)} \left[ \left( r + \gamma \max_{a'} Q(s',a';\theta_{i-1}) - Q(s,a;\theta_i) \right) \nabla_{\theta_i} Q(s,a;\theta_i) \right]. \]

Es computacionalmente beneficioso optimizar el gradiente en Eq. (15) utilizando descenso de gradiente estoc√°stico [68]. Seg√∫n [27], DQN modifica el algoritmo original de Q-learning de tres maneras: 1) utiliza replay de experiencias, propuesto por primera vez en [69], que mantiene las experiencias de los agentes durante varios pasos de tiempo en una memoria de repetici√≥n y las utiliza para actualizar los pesos en la fase de entrenamiento. 2) Para reducir la complejidad en la actualizaci√≥n de pesos, los pesos actualizados actuales se mantienen fijos y se introducen en una segunda red (duplicada) cuyas salidas se utilizan como objetivos de Q-learning. 3) El t√©rmino de error (r + \gamma \max_{a'} Q(s',a';\theta_{i-1}) - Q(s,a;\theta_i) en la Eq. (15) se recorta de modo que permanezca en el intervalo [-1, 1]. Todas estas modificaciones ayudan a mejorar la estabilidad de DQN.

Sin embargo, DQN tiene algunos problemas; primero, siguiendo el algoritmo de Q-learning, DQN sobreestima los valores de acci√≥n bajo ciertas circunstancias, lo que hace que el aprendizaje sea ineficiente y puede llevar a pol√≠ticas sub√≥ptimas [70]. Se propuso Double DQN (DDQN) para aliviar este problema [71]. La diferencia entre DQN y DDQN es que la pol√≠tica codiciosa se eval√∫a utilizando la red en l√≠nea, pero la red objetivo se utiliza para estimar su valor. Por lo tanto, \( y_i \) se cambia de la siguiente manera:

\[ y_i = r + \gamma Q(s', \arg \max_{a'} Q(s',a';\theta_{i-1}); \theta_{i-1}). \]

Una extensi√≥n interesante sobre DDQN es la red dueling [72], cuya idea es tener una sola red Q con las mismas capas convolucionales que DQN, pero con dos flujos de capas completamente conectadas (FC), que proporcionan estimaciones de las funciones de valor y ventaja. Esto ayuda a generalizar mejor el aprendizaje entre acciones. Segundo, DQN selecciona experiencias de manera uniforme para repetir, independientemente de su importancia, lo que hace que el proceso de aprendizaje sea lento e ineficiente. En consecuencia, se propuso replay de experiencias priorizadas para resolver el problema [73]. La idea es repetir experiencias importantes con m√°s frecuencia, mejorando as√≠ el entrenamiento de la red. La importancia de cada transici√≥n se mide proporcionalmente al error de diferencia temporal, y se proponen dos variantes, priorizaci√≥n estoc√°stica y muestreo de importancia, para mejorarlo. Finalmente, DQN no es aplicable en espacios continuos, por lo que se propuso el gradiente de pol√≠tica determinista profunda (DDPG) [74], que es una combinaci√≥n de DQN y el gradiente de pol√≠tica determinista (DPG) [75] en un enfoque actor-critic. El actor mapea determin√≠sticamente estados a una acci√≥n espec√≠fica. El cr√≠tico define el valor de la acci√≥n tomada por el actor. En cada iteraci√≥n, el cr√≠tico se actualiza mediante

\[ L = \frac{1}{N} \sum_i (y_i - Q(s_i,a_i \mid \theta_Q))^2 \]

y el actor se actualiza mediante

\[ \nabla_{\theta_{\mu}} J = \frac{1}{N} \sum_i \nabla_a Q(s,a \mid \theta_Q) \bigg|_{(s=s_i,a=\mu(s_i))} \nabla_{\theta_{\mu}} \mu(s \mid \theta_{\mu}) \bigg|_{s_i}, \]

donde \( \theta_{\mu} \) y \( \theta_Q \) son los par√°metros de las redes actor y cr√≠tico, respectivamente.

Finalmente, la optimizaci√≥n de pol√≠tica por proximidad (PPO) [76] es otro algoritmo actor-critic utilizado por los RSRLs. De hecho, PPO es una versi√≥n mejorada del algoritmo de optimizaci√≥n de pol√≠tica de regi√≥n de confianza (TRPO) [77], que maximiza un objetivo sustituto

\[ E_t \left[ \frac{\pi_{\theta}(a_t \mid s_t)}{\pi_{\theta_{\text{old}}}(a_t \mid s_t)} A_t \right], \]

donde \( A_t \) es un estimador de la funci√≥n de ventaja en \( t \). La idea central en PPO es la introducci√≥n del objetivo sustituto recortado,

\[ E_t \left[ \min \left( \frac{\pi_{\theta}(a_t \mid s_t)}{\pi_{\theta_{\text{old}}}(a_t \mid s_t)} A_t, \text{clip} \left( \frac{\pi_{\theta}(a_t \mid s_t)}{\pi_{\theta_{\text{old}}}(a_t \mid s_t)}, 1 - \epsilon, 1 + \epsilon \right) A_t \right) \right], \]

donde \( \epsilon \) es un hiperpar√°metro.

3.7 Desaf√≠os de RL
Existen algunos desaf√≠os posibles al aplicar RL a cualquier problema. Un desaf√≠o conocido como "Triple mortal" indica que existe un riesgo de inestabilidad y divergencia al combinar tres elementos en RL: aproximaci√≥n de funciones, bootstrapping y entrenamiento fuera de pol√≠tica [27]. Otro desaf√≠o en RL es la ineficiencia de la muestra, espec√≠ficamente en algoritmos de RL sin modelo [78]. Los algoritmos actuales de RL sin modelo necesitan una cantidad considerable de interacci√≥n agente-entorno para aprender estados √∫tiles. Adem√°s, dado que DRL se basa en el aprendizaje profundo, hereda la caracter√≠stica famosa de las redes neuronales, es decir, ser una caja negra. No es evidente c√≥mo cambian los pesos y las activaciones, lo que los hace no interpretables. El problema cl√°sico de la exploraci√≥n versus explotaci√≥n sigue siendo un desaf√≠o en RL y la exploraci√≥n efectiva es un problema de investigaci√≥n abierto. Finalmente, el problema de la formulaci√≥n de recompensas en RL es un desaf√≠o y dise√±ar una buena funci√≥n de recompensa no es muy claro o directo.

2.3 ¬øPor qu√© Aprendizaje por Refuerzo para la Recomendaci√≥n?
La naturaleza de la interacci√≥n del usuario con un sistema de recomendaci√≥n es secuencial [79] y el problema de recomendar los mejores √≠tems a un usuario no es solo un problema de predicci√≥n, sino un problema de decisi√≥n secuencial [50]. Esto sugiere que el problema de recomendaci√≥n podr√≠a modelarse como un MDP y resolverse mediante algoritmos de RL. Tres caracter√≠sticas √∫nicas de RL hacen que sea una combinaci√≥n perfecta para el problema de recomendaci√≥n. Primero, RL puede manejar la din√°mica de la interacci√≥n secuencial usuario-sistema ajustando acciones seg√∫n el feedback continuo recibido del entorno. Segundo, RL puede tener en cuenta el compromiso a largo plazo del usuario con el sistema. Finalmente, aunque tener valoraciones de usuario es beneficioso, RL, por su naturaleza, no necesita valoraciones de usuario y optimiza su pol√≠tica interactuando secuencialmente con el entorno. Todas estas razones sugieren que ser√≠a beneficioso utilizar RL para proporcionar mejores recomendaciones, como lo han demostrado estudios en l√≠nea [25, 80].

2.4 Formulaci√≥n del Problema

En un problema de recomendaci√≥n, el algoritmo del sistema de recomendaci√≥n (RS), a trav√©s de la interacci√≥n con el usuario y recibiendo sus retroalimentaciones impl√≠citas/explicitas, intenta recomendar los mejores √≠tems al usuario, con el fin de lograr el objetivo para el cual est√° dise√±ado, que podr√≠a ser aumentar la ganancia, la satisfacci√≥n del usuario o la fidelidad del usuario [3]. Esto es an√°logo a una configuraci√≥n t√≠pica de RL, donde un agente tiene como objetivo maximizar una recompensa num√©rica mediante la interacci√≥n con un entorno [27]. Por lo tanto, el agente de RL puede desempe√±ar el papel del algoritmo de RS y todo lo que est√° fuera de este agente, incluidos los usuarios del sistema e √≠tems, puede considerarse como el entorno para este agente.

De manera m√°s formal, considerando al usuario e √≠tems como el entorno y al algoritmo de RS como el agente de RL, la formulaci√≥n de MDP puede ser la siguiente:
- Estado S: un estado ùë†ùë° ‚àà S se define como las preferencias del usuario y su historial pasado con el sistema.
- Acci√≥n A: una acci√≥n ùëéùë° ‚àà A es recomendar un √≠tem al usuario en el paso de tiempo ùë°.
- Recompensa R: el agente de RL recibe una recompensa ùëü(ùë†ùë°,ùëéùë°) ‚àà R basada en la retroalimentaci√≥n del usuario sobre la recomendaci√≥n proporcionada.
- Probabilidad de transici√≥n P: la probabilidad de transici√≥n ùëù(ùë†‚Ä≤|ùë†,ùëé) ‚àà P es la probabilidad de transici√≥n de ùë† = ùë†ùë° a ùë†‚Ä≤ = ùë†ùë°+1 si el agente toma la acci√≥n ùëé.

Factor de descuento ùõæ: el factor de descuento ùõæ ‚àà [0,1] es el factor de descuento para las recompensas futuras. Con ùõæ = 0, el agente se vuelve miope, es decir, se enfoca solo en la recompensa inmediata. Por el contrario, si ùõæ = 1, el agente se vuelve previsor y se enfoca m√°s en las recompensas futuras [27].

Dado (S,A,R,P,ùõæ), el objetivo del agente de RL es encontrar una pol√≠tica ùúã que maximice la recompensa acumulativa esperada y descontada. En otras palabras,
maxùúã E[ ùëá‚àëÔ∏Å
ùë°=0
ùõæùë°ùëü(ùë†ùë°,ùëéùë°)], (21)
donde ùëá es el paso de tiempo m√°ximo en un MDP finito.

2.5 Marco Propuesto RLRS

Tras estudiar cuidadosamente todos los RLRS recopilados, encontramos que hay cuatro componentes comunes en todos ellos y creemos que un buen RLRS deber√≠a dise√±ar y abordar estos componentes cuidadosamente. En consecuencia, para unificar el proceso de desarrollo de RLRS, proponemos un marco para RLRS con cuatro componentes clave: (1) Representaci√≥n del Estado, (2) Optimizaci√≥n de la Pol√≠tica, (3) Formulaci√≥n de la Recompensa y (4) Construcci√≥n del Entorno. La Figura 3(a) representa este marco. A continuaci√≥n, explicamos cada componente.

Representaci√≥n del Estado. En la interfaz agente-entorno de RL, el estado puede ser cualquier informaci√≥n disponible para el agente. La representaci√≥n del estado podr√≠a ser tan abstracta como descripciones simb√≥licas de objetos en una habitaci√≥n o tan detallada como lecturas de sensores [27]. Lo importante es que los estados definidos deben tener la propiedad de Markov. Esto significa que la se√±al de estado no debe transmitir toda la informaci√≥n sobre el entorno al agente, pero debe resumir la informaci√≥n pasada de manera que no se pierda ninguna informaci√≥n relevante. Una se√±al de estado con esta propiedad se llama Markoviana. En general, la selecci√≥n de la representaci√≥n del estado es actualmente m√°s un arte que una ciencia [27].

En RLRS, la representaci√≥n del estado debe resumir informaci√≥n sobre usuarios, √≠tems y el contexto. Dividimos la representaci√≥n del estado en RLRS en tres grupos:

SR1) Tratamiento de los √≠tems como estados. Cuando el espacio de √≠tems es peque√±o, por ejemplo, incluye varias p√°ginas web en un sitio web, es posible tratar cada √≠tem como un estado. Sin embargo, este enfoque ciertamente no es escalable cuando el espacio de √≠tems crece considerablemente. Para abordar el problema de escalabilidad en espacios de √≠tems m√°s grandes, los investigadores encontraron que los estados podr√≠an indicar un conjunto de √≠tems previamente valorados/consumidos por el usuario. La Figura 3(b) representa esta representaci√≥n.

SR2) Caracter√≠sticas de usuarios, √≠tems y contexto. Una manera popular de representaci√≥n del estado es extraer algunas caracter√≠sticas de usuarios, √≠tems y contexto, como se muestra en la Figura 3(c). Las caracter√≠sticas de usuario pueden incluir informaci√≥n demogr√°fica, como edad, raza y g√©nero. Las caracter√≠sticas de √≠tems pueden incluir precio, categor√≠a y popularidad. Las caracter√≠sticas de contexto pueden incluir tiempo, plataforma y ubicaci√≥n.

SR3) Incrustaciones codificadas. Para un entrenamiento efectivo, los modelos profundos en RS basados en DRL necesitan que los estados sean vectores densos y de baja dimensionalidad. La Figura 3(d) ilustra un marco general y popular para la representaci√≥n del estado en m√©todos basados en DRL. T√≠picamente, las primeras caracter√≠sticas de usuario, √≠tems y contexto se traducen en vectores continuos densos y de baja dimensionalidad llamados incrustaciones. Luego, para un mejor entrenamiento, esta incrustaci√≥n podr√≠a ser codificada utilizando un modelo de red neuronal recurrente (RNN), que puede ayudar al modelo a aprender las preferencias secuenciales del usuario [48]. Las unidades recurrentes con compuertas (GRU) suelen ser m√°s populares que la memoria a corto plazo (LSTM) para el m√≥dulo RNN, ya que tienen menos par√°metros y pueden lograr el mismo o mejor rendimiento [81]. Para centrarse en partes importantes de la entrada, algunos investigadores tambi√©n utilizan una capa de atenci√≥n en el m√≥dulo de codificaci√≥n y a√±aden pesos a los vectores codificados. Finalmente, los vectores codificados se concatenan para obtener el estado final.

Optimizaci√≥n de la Pol√≠tica. Una vez formulados los estados, es la pol√≠tica la que determina qu√© acci√≥n tomar (es decir, qu√© √≠tems recomendar) en cada estado. Para la optimizaci√≥n de la pol√≠tica, diversos algoritmos de RL han sido utilizados por los RLRSs. Antes del surgimiento del DRL, los m√©todos de RL utilizados por los RLRSs pod√≠an clasificarse generalmente en m√©todos tabulares y m√©todos aproximados. Los m√©todos tabulares incluyen la iteraci√≥n de pol√≠ticas, Q-learning, Sarsa, Sarsa(ùúÜ), R-learning y MCTS. Los m√©todos aproximados incluyen Q ajustado e iteraci√≥n de valor de gradiente. Por otro lado, los m√©todos DRL podr√≠an dividirse generalmente en tres grupos: basados en valor (DQN), gradiente de pol√≠tica (REINFORCE y REINFORCE-wb) y m√©todos actor-cr√≠tico (DDPG y PPO). Una clasificaci√≥n de estos algoritmos se muestra en la Figura 2(b).

Formulaci√≥n de la Recompensa. Como se mencion√≥ anteriormente, la se√±al de recompensa del entorno refleja qu√© tan bien o mal est√° desempe√±√°ndose el agente mediante la selecci√≥n de acciones. Por lo tanto, el dise√±o de una se√±al de recompensa informativa es crucial para el √©xito/aprendizaje del agente. De hecho, en RL, la se√±al de recompensa es la √∫nica forma de indicarle al agente qu√© hacer, no c√≥mo hacerlo [27]. En general, definir una funci√≥n de recompensa adecuada es un problema dif√≠cil y es m√°s un proceso de prueba y error o de ingenier√≠a. No hay una regla definitiva para dise√±ar una buena funci√≥n de recompensa en un problema espec√≠fico. En los RLRSs, hemos observado dos tendencias generales en el dise√±o de la funci√≥n de recompensa: (R1) la funci√≥n de recompensa es una recompensa num√©rica simple y dispersa, o (R2) la recompensa es una funci√≥n de una o varias observaciones del entorno.

Construcci√≥n del Entorno. En general, evaluar los RSs es dif√≠cil [3, 82]. Como resultado, construir un entorno adecuado para entrenar y evaluar correctamente al agente en los RLRSs es un desaf√≠o. Para distinguir mejor entre diferentes m√©todos de construcci√≥n del entorno, generalmente los dividimos en tres grupos: offline, simulaci√≥n y online. En el m√©todo offline, el entorno es un conjunto de datos est√°tico que contiene las calificaciones de algunos usuarios sobre algunos √≠tems. Una pr√°ctica com√∫n en los m√©todos offline es entrenar al agente con los datos de entrenamiento (generalmente el 70-80% de los datos) y luego probarlo con los datos restantes. En los estudios de simulaci√≥n, generalmente se construye un modelo de usuario y se eval√∫a el algoritmo mientras interact√∫a con este modelo de usuario. Este modelo de usuario puede ser tan simple como un usuario con un comportamiento predefinido o puede ser m√°s complejo y aprenderse utilizando datos disponibles. En el m√©todo online, el algoritmo se eval√∫a mientras interact√∫a con usuarios reales y en tiempo real. Este es el mejor m√©todo, pero tambi√©n el m√°s costoso para la evaluaci√≥n de los RLRSs.

3 ALGORITMOS DE SISTEMAS DE RECOMENDACI√ìN BASADOS EN APRENDIZAJE POR REFUERZO

En esta secci√≥n, presentamos los algoritmos de manera clasificada. Como se discuti√≥ anteriormente, primero dividimos generalmente los RLRSs en m√©todos basados en RL y DRL. Luego, revisamos los algoritmos en cada categor√≠a con respecto al marco de trabajo RLRS.

3.1 RLRSs basados en RL

En esta secci√≥n, presentamos los RLRSs basados en RL; es decir, m√©todos que no utilizan aprendizaje profundo para la optimizaci√≥n de pol√≠ticas. La Tabla 2 proporciona una visi√≥n general r√°pida sobre los m√©todos basados en RL.

3.1.1 Representaci√≥n del Estado

Como se ilustra en la Tabla 2, aparte de RPRMS y PHRR, todos los m√©todos basados en RL pertenecen a SR1 o SR2, que comparten casi la misma proporci√≥n (ver Fig. 5(a)). Como se mencion√≥ anteriormente, los m√©todos en SR1 utilizan todos o un conjunto/tupla de elementos para la representaci√≥n del estado. Por ejemplo, WebWatcher [83], el primer RLRS que identificamos, trata cada √≠tem (es decir, p√°gina web) como un estado en un escenario de recomendaci√≥n web. De manera similar, las Referencias [96] y [97] tratan cada autor y objeto de aprendizaje como un estado en recomendaciones de colaboradores cient√≠ficos y escenarios de e-learning, respectivamente. Como se indic√≥ anteriormente, aunque este enfoque es posible en espacios de estados peque√±os, ciertamente no es escalable cuando el espacio de √≠tems crece considerablemente. Los investigadores descubrieron que hacer un seguimiento de un peque√±o conjunto de √≠tems ya valorados/consumidos por el usuario podr√≠a ser lo suficientemente informativo para la optimizaci√≥n de pol√≠ticas. Quiz√°s las Referencias [50, 84] sean los primeros RLRSs que utilizan esta idea, pero la idea est√° mejor formalizada para los RLRSs por la Referencia [85]. Espec√≠ficamente, en una aplicaci√≥n de recomendaci√≥n web, Taghipour y Kardan [85] adoptan el modelo N-gram de la literatura de miner√≠a de uso web [102] e introducen una ventana deslizante para representar estados, representada en la Figura 4. En esta figura, los c√≠rculos son estados, las flechas hacia la derecha son acciones, y ùëâ y ùëÖ indican p√°ginas visitadas y previamente recomendadas, respectivamente. Al usar este modelo, los autores asumen que conocer las √∫ltimas ùëò p√°ginas visitadas por el usuario proporciona suficiente informaci√≥n para predecir sus futuras solicitudes de p√°gina. Es importante mencionar que este conjunto o ventana deslizante en SR1 podr√≠a indicar cualquier informaci√≥n √∫til para la optimizaci√≥n de pol√≠ticas, incluyendo un conjunto de √≠tems comerciales [50], conceptos en un sitio web [84, 87], clases de emoci√≥n de canciones [89], habilidades [101] y canciones de m√∫sica [51]. En un entorno diferente, Choi et al. [44] formulan el problema de recomendaci√≥n como un juego de mundo en cuadr√≠cula y cada celda de la cuadr√≠cula, con sus usuarios e √≠tems dentro, se considera como un estado.

Otros investigadores han propuesto extraer algunas caracter√≠sticas de usuarios, √≠tems y contexto y usarlas para la representaci√≥n del estado (SR2). Entre los primeros intentos en SR2 se encuentran los trabajos de Mahmood et al. [86, 88, 92], en los cuales se utiliza un conjunto de variables de usuario (por ejemplo, el n√∫mero de veces que el usuario ha modificado su consulta), agente (por ejemplo, acci√≥n previa del agente) y sesi√≥n de interacci√≥n (por ejemplo, n√∫mero de episodios transcurridos) para la representaci√≥n del estado. Un enfoque similar se utiliza en RLradio [49], donde se definen algunas variables que contienen informaci√≥n sobre los canales de radio de inter√©s del usuario y su comportamiento de escucha para representar los estados. DJ-MC [93] utiliza un m√©todo de codificaci√≥n para representar cada canci√≥n como un vector de descriptores de canciones y cada estado es la concatenaci√≥n de ùëò vectores de canciones en la lista de reproducci√≥n. Se utilizan caracter√≠sticas de las condiciones clim√°ticas y el tiempo en CAPR [99] para la representaci√≥n del estado en un recomendador de puntos de inter√©s (POI). SR2 es especialmente popular en aplicaciones de atenci√≥n m√©dica en las que la informaci√≥n sobre los pacientes generalmente se registra mediante varias caracter√≠sticas descriptivas [90, 91]. En un entorno diferente, POMDP-Rec [95] formula los estados como estados de creencias utilizando un modelo de factor de baja dimensi√≥n [103]. M√°s precisamente, con una matriz usuario-√≠tem parcialmente observada, las observaciones del comportamiento del usuario (O), las caracter√≠sticas latentes de los √≠tems (V) y los intereses latentes de los usuarios (U) pueden calcularse como se muestra en las ecuaciones (22), (23) y (24).

Los √∫nicos trabajos en m√©todos basados en RL que se encuentran en SR3 son RPRMS [98] y PHRR [100], ambos en el dominio de recomendaci√≥n de m√∫sica. En RPRMS, los estados son una concatenaci√≥n de incrustaciones de letras de canciones, generadas utilizando Word2Vec [104], y incrustaciones de audio, generadas utilizando un modelo WaveNet preentrenado [105]. PHRR utiliza una factorizaci√≥n matricial ponderada [106] y CNN para incrustar canciones, y al igual que DJ-MC, cada estado es la concatenaci√≥n de varios vectores de canciones.

3.1.2 Optimizaci√≥n de Pol√≠ticas

Seg√∫n la Figura 5(d), los m√©todos de diferencia temporal, es decir, Q-learning y Sarsa, han sido los algoritmos de RL m√°s populares entre los m√©todos basados en RL [44, 51, 83, 85, 87, 89, 92, 97, 98, 101]. La principal raz√≥n de esta popularidad es su simplicidad; es decir, son m√©todos en l√≠nea, libres de modelo, requieren una cantidad m√≠nima de computaci√≥n y pueden expresarse mediante una sola ecuaci√≥n (ver Ecs. (4) y (5)) [27]. Aplicar Q-learning/Sarsa para la optimizaci√≥n de pol√≠ticas es bastante directo y no requiere ninguna modificaci√≥n espec√≠fica. Los investigadores en [85] utilizan un truco simple para tener una tasa de aprendizaje decreciente ùõº = 1/1 + visitas(ùë†,ùëé) en la Ec. (4), lo cual ayuda a la convergencia del algoritmo. Este truco tambi√©n se utiliza en [51, 89, 92]. Un problema con los m√©todos de diferencia temporal, al igual que cualquier m√©todo RL tabular, es que conducen a la maldici√≥n de la dimensionalidad [107]. Para abordar este problema, como se discuti√≥ anteriormente, los investigadores intentan manejar el espacio de estados y mantenerlo lo suficientemente peque√±o.

Entre los m√©todos tabulares, los m√©todos de programaci√≥n din√°mica suelen ser impr√°cticos debido a su gran costo computacional y la necesidad de conocimiento perfecto sobre el entorno. Aunque estos algoritmos son polinomiales en el n√∫mero de estados, realizar una sola iteraci√≥n de m√©todos de iteraci√≥n de pol√≠tica o de valor a menudo es inviable [108]. Para hacerlo pr√°ctico, Ref. [50] utiliza un par de caracter√≠sticas en su espacio de estados y realiza algunas aproximaciones. Por ejemplo, una caracter√≠stica del espacio de estados en [50] es la direccionalidad; los autores argumentan que un estado corto no puede seguir a un estado largo o que la probabilidad de ocurrencia de bucles en su MDP no es muy alta. Adem√°s, Ref. [86] mantiene el n√∫mero de ejecuciones de iteraci√≥n de pol√≠tica limitado.

MCTS es un algoritmo de planificaci√≥n en tiempo de decisi√≥n que se beneficia de la estimaci√≥n de valor basada en muestras incremental y online y la mejora de pol√≠ticas [27], y ha sido utilizado por [93, 99, 100]. Para facilitar el aprendizaje del agente, en caso de que el espacio de canciones sea muy grande o el tiempo de b√∫squeda sea limitado, DJ-MC [93] agrupa canciones seg√∫n tipos de canciones y luego aplica MCTS a las canciones agrupadas. PHRR [100] adopta esquemas similares en la optimizaci√≥n de pol√≠ticas y en la agrupaci√≥n de canciones. Similar a AlphaGo [62], CAPR [99] utiliza UCT (Upper Confidence Bound aplicado a Trees) [109] para resolver el compromiso entre exploraci√≥n y explotaci√≥n en el paso de selecci√≥n de MCTS (ver secci√≥n 2.2).

Preda y Popescu [84] utilizan Sarsa(ùúÜ) con codificaci√≥n por mosaicos [27] y aproximaci√≥n lineal para la optimizaci√≥n de pol√≠ticas. Para poder aplicar Sarsa(ùúÜ), el trabajo transforma la informaci√≥n epist√©mica en matrices de n√∫meros reales. Moling et al. [49] definen el problema de la recomendaci√≥n √≥ptima de canales de radio como una tarea continua y luego emplean R-learning [59] para resolverlo.

Por otro lado, algunos RLRSs basados en RL han utilizado m√©todos aproximados para la optimizaci√≥n de pol√≠ticas, incluidos el Q ajustado [90, 91, 94, 95] y la iteraci√≥n de valor de gradiente [96]. El Q ajustado es un marco flexible que puede ajustar cualquier arquitectura de aproximaci√≥n a la funci√≥n Q [64]. En consecuencia, cualquier algoritmo de regresi√≥n supervisada en modo por lotes puede usarse para aproximar la funci√≥n Q, lo cual puede escalar bien a espacios de alta dimensionalidad [27]. Sin embargo, un problema con este m√©todo es que podr√≠a tener un alto costo computacional y de memoria con el aumento del n√∫mero de cu√°druples (ùë•ùë°,ùë¢ùë°,ùëüùë°,ùë•ùë°+1), donde ùë•ùë° indica el estado del sistema en el tiempo ùë°, ùë¢ùë° la acci√≥n de control tomada, ùëüùë° la recompensa inmediata, y ùë•ùë°+1 el siguiente estado del sistema [64]. Este algoritmo ha sido utilizado por varios RLRSs [90, 91, 94, 95]. Para ajustar la funci√≥n Q en estos m√©todos, se utilizan regresi√≥n lineal [90, 94], regresi√≥n de vector de soporte [91] y redes neuronales [95]. Finalmente, Zhang et al. [96] introducen una versi√≥n de descenso de gradiente del algoritmo de iteraci√≥n de valor para la recomendaci√≥n de colaboradores en un entorno de RL multiagente.

3.1.3 Formulaci√≥n de Recompensas

La Figura 5(b) muestra que R2, con una proporci√≥n del 60%, ha sido m√°s popular que R1, con un 40%, entre los m√©todos basados en RL. En R1, se han utilizado diferentes valores num√©ricos para la recompensa inmediata. Por ejemplo, Mahmood et al. utilizan +1 en estado terminal y un n√∫mero negativo en otro caso [86], +5 por agregar un producto al plan de viaje, +1 por mostrar una p√°gina de resultados, y 0 en otro caso [88], y +100 por comprar un libro, -30 por abandono del usuario, y 0 en otro caso [92]. Por otro lado, en R2, los investigadores han propuesto utilizar diferentes observaciones del entorno para formular la recompensa, incluyendo beneficio neto [50], tiempo de supervivencia global [91], algunos puntajes cl√≠nicos (es decir, PANSS) [90], y la distancia de Jaccard entre dos estados [44].

3.1.4 Construcci√≥n del Entorno

Es observable en la Figura 5(c) que el m√©todo dominante de construcci√≥n del entorno en los RS basados en RL es offline. Esto tiene sentido ya que entrenar al agente y probar el rendimiento en un conjunto de datos disponible es la opci√≥n m√°s f√°cil y segura. Dos conjuntos de datos populares utilizados en RS basados en RL son MovieLens [110] y el conjunto de datos Million Song [111]. Otro m√©todo de construcci√≥n del entorno es la simulaci√≥n, que es un m√©todo seguro para ajustar los par√°metros importantes del modelo antes del despliegue del sistema o un estudio en l√≠nea. La simulaci√≥n podr√≠a ser tan simple como asumir usuarios constantes con patrones de preferencia predefinidos [89, 91], o podr√≠a ser m√°s compleja y aprender el comportamiento del usuario a trav√©s de datos disponibles [86, 92, 93]. Por ejemplo, en una tarea de aprendizaje supervisado, Mahmood et al. [86] definen un modelo de comportamiento del usuario y lo utilizan en un RS de viaje, llamado NutKing, para aprender las probabilidades de transici√≥n. El objetivo es conocer c√≥mo reacciona el usuario simulado ante una cierta acci√≥n del sistema.

El estudio en l√≠nea es el m√©todo m√°s efectivo pero costoso para la construcci√≥n del entorno en los RLRSs. En un intento valioso y temprano [50], se evalu√≥ el rendimiento del RS propuesto basado en MDP en un estudio en l√≠nea de dos a√±os realizado en una librer√≠a en l√≠nea. El estudio tuvo una buena exposici√≥n diaria a los usuarios, con casi 5000-6000 usuarios diferentes diarios, con un n√∫mero razonable de art√≠culos para recomendar (m√°s de 15,000), en comparaci√≥n con otros estudios en l√≠nea realizados por m√©todos basados en RL con solo cinco [89], 13 [92], 47 [93], 469 [88], y 500 [84] usuarios.


3.2 RS basados en DRL

En esta secci√≥n, estudiamos los m√©todos basados en DRL; es decir, aquellos RS que utilizan un modelo de aprendizaje profundo para la optimizaci√≥n de pol√≠ticas. La Tabla 3 proporciona una visi√≥n general r√°pida de estos m√©todos.

3.2.1 Representaci√≥n del Estado

Como se muestra en la Figura 6(a), SR3 es el esquema dominante de representaci√≥n del estado para los RS basados en DRL. Como se mencion√≥ anteriormente, esto se debe a que los modelos profundos se entrenan de manera m√°s efectiva en vectores densos y de baja dimensionalidad. No obstante, los investigadores han ido un paso m√°s all√° y han intentado hacer m√°s efectivo el marco general de SR3 (ver Figura 3(d)). T√≠picamente, en los RS basados en RL, los art√≠culos calificados positivamente por el usuario se consideran como preferencias del usuario. Sin embargo, en DEERS [48], los autores discuten que la proporci√≥n de retroalimentaci√≥n negativa, como los elementos saltados, podr√≠a ser mucho mayor que la positiva, por lo que proponen tener dos estados: estados positivos y negativos. La Figura 7(a) ilustra esta modificaci√≥n. En particular, la entrada se divide en elementos con retroalimentaci√≥n positiva y negativa, se pasa a trav√©s de capas de incrustaci√≥n y RNN, y se alimenta a la red Q donde se concatenan. Esta t√©cnica tambi√©n ha inspirado a otros investigadores [120, 165]. En lugar de la capa RNN, DRCGR [120] utiliza una capa de convoluci√≥n (con n√∫cleos horizontales y verticales) para codificar las incrustaciones de retroalimentaci√≥n positiva. Por otro lado, un m√≥dulo de red generativa adversaria (GAN) se entrena para generar muestras negativas. Deep Page [46] tambi√©n extiende el marco SR3 agregando un m√≥dulo CNN entre las capas de incrustaci√≥n y RNN, para aprender el esquema de visualizaci√≥n espacial de elementos en un escenario de recomendaci√≥n por p√°gina. Antes de pasar las incrustaciones de elementos a trav√©s del m√≥dulo CNN, se utiliza una capa de p√°gina para convertir las incrustaciones de elementos en una cuadr√≠cula/matriz 2D para procesamiento CNN 2D. Adem√°s, los autores en [135] proponen utilizar un esquema de ponderaci√≥n de posici√≥n para la incrustaci√≥n del estado. Formalmente, si ùëä es una matriz con pasos hist√≥ricos como filas y el peso de importancia de posiciones como columnas, la incrustaci√≥n de un estado ùë†ùë° se puede definir como
\[ ùë†ùë° = ‚Ñé(ùêπùë°‚àíùëö:ùë°‚àí1) = ùë£ùëíùëê[ùúé(ùêπùë°‚àíùëö:ùë°‚àí1ùëä + ùêµ)], \]
donde ùêπ es el vector de caracter√≠sticas del historial con ùëö pasos, ùêµ es una matriz de sesgo, ùúé(¬∑) es una activaci√≥n no lineal, y ùë£ùëíùëê[¬∑] concatena las columnas de la matriz. Los autores afirman que este m√©todo para la incrustaci√≥n del estado es m√°s eficiente para la optimizaci√≥n que LSTM. Finalmente, en D2RLIR [162], se agrega una codificaci√≥n posicional a las incrustaciones del estado para que el modelo entienda el orden cronol√≥gico de los elementos.

En DRR [144], se propone un m√≥dulo individual llamado m√≥dulo de representaci√≥n del estado para la formulaci√≥n del estado. Los autores proponen tres estructuras para modelar las interacciones entre el usuario y los elementos. En la primera estructura, DRR-p, simplemente se concatenan las incrustaciones de los elementos y sus productos por pares, como se muestra en la Figura 8(a). M√°s formalmente, si ùêª = {ùë£1, ùë£2, ..., ùë£ùëõ} es el historial de interacci√≥n positiva del usuario y ùëÉ = {ùë§ùëñùë£ùëñ ‚äó ùë§ùëóùë£ùëó | ùëñ, ùëó = 1, 2, ..., ùëõ} es el producto por pares ponderado entre elementos, entonces el estado ùëÜ se define como la concatenaci√≥n de ùêª y ùëÉ, es decir, ùëÜ = (ùêª, ùëÉ). En la segunda estructura, DRR-u, tambi√©n se incorpora la incrustaci√≥n del usuario (mostrado en la Figura 8(b)). Esto significa que, con ùêæ = {ùë¢ ‚äó ùë§ùëñùë£ùëñ | ùëñ = 1, 2, ..., ùëõ}, ùëÜ = (ùêæ, ùëÉ). En la √∫ltima estructura ilustrada en la Figura 8(c), DRR-ave, se introduce una capa de agrupaci√≥n promedio para eliminar el sesgo de posici√≥n de los elementos en la lista recomendada. En particular, si ùê∫ = {ùëéùë£ùëí(ùë§ùëñùë£ùëñ) | ùëñ = 1, ..., ùëõ}, ùëÜ = (ùë¢, ùë¢ ‚äó ùê∫, ùê∫). En [145], los autores ampl√≠an DRR-ave y agregan una red de atenci√≥n para generar pesos dependientes del usuario para cada elemento, como se muestra en la Figura 8(d). En otro trabajo [160], los mismos autores estudian el efecto de actualizar el m√≥dulo de representaci√≥n del estado utilizando una se√±al de aprendizaje supervisado, y a trav√©s de estudios experimentales muestran que el rendimiento de la recomendaci√≥n podr√≠a mejorar.

Alrededor del 20% de los RS basados en DRL pertenecen a SR2. Por ejemplo, DRN [47] utiliza caracter√≠sticas del usuario y del contexto para la representaci√≥n del estado en la recomendaci√≥n de noticias. Las caracter√≠sticas del usuario extra√≠das en DRN incluyen las caracter√≠sticas de las noticias que el usuario ha hecho clic en diferentes marcos de tiempo, como una hora, seis horas, 24 horas, una semana y un a√±o. Estas caracter√≠sticas de las noticias incluyen proveedor de titulares, clasificaci√≥n, nombre de entidad, categor√≠a y categor√≠a de tema. Las caracter√≠sticas del contexto tambi√©n describen el contexto temporal de la solicitud de noticias, incluyendo la hora y el d√≠a de la semana. Al igual que en los m√©todos basados en RL, SR2 es el m√©todo popular de representaci√≥n del estado en aplicaciones de salud [113, 115]. Nemati et al. [113] utilizan una formulaci√≥n de MDP parcialmente observable (POMDP) en una aplicaci√≥n cl√≠nica. Formulan los estados como estados de creencia utilizando un modelo oculto de Markov discriminatorio (DHMM).

Encontramos solo dos trabajos [24, 112] que se encuentran en SR1. Tal vez la raz√≥n por la que estos trabajos utilizan un m√©todo simple de representaci√≥n del estado es que son trabajos representativos no dise√±ados espec√≠ficamente para RSs, sino desarrollados para abordar desaf√≠os espec√≠ficos en la aplicaci√≥n de DRL a dominios como RSs.

3.2.2 Optimizaci√≥n de Pol√≠tica. Despu√©s de definir los estados, el rol de la pol√≠tica ùúã es mapear estados a acciones. Los algoritmos de optimizaci√≥n de pol√≠ticas utilizados por los RS basados en DRL pueden dividirse generalmente en m√©todos basados en valor, gradiente de pol√≠tica y m√©todos actor-cr√≠tico.
M√©todos basados en valor. Aparte de MCTS utilizado en [134], DQN y sus extensiones, es decir, DDQN, dueling DQN y dueling DDQN, son los m√©todos basados en valor dominantes. B√°sicamente, hay tres elementos principales en DQN: 1) la arquitectura de la red Q, 2) el replay de experiencia, y 3) la exploraci√≥n. Revisamos los m√©todos basados en DQN seg√∫n estos elementos y la Tabla 4 resume los m√©todos basados en DQN.

1) Arquitectura de la red Q. La Figura 7 muestra dos posibles arquitecturas de la red Q utilizadas por los RS basados en DQN. La arquitectura original (A1), introducida en [67, 68], recibe el estado y emite el valor Q de todas las acciones, indicado por ùëÑ1, ..., ùëÑùëõ en la Figura 7(b). Si bien A1 funciona bien cuando el espacio de acciones es peque√±o, su aplicabilidad en el dominio de RS con un espacio de acciones grande, e incluso enorme (del orden de millones), es cuestionable. Otra arquitectura posible (A2) es recibir el par de estado y acci√≥n, y luego emitir el valor Q del par, es decir, ùëÑ(ùë†,ùëé) (representado en la Figura 7(c)). Aunque A2 resuelve el problema de A1, un problema con A2 es que la complejidad temporal del modelo podr√≠a ser alta.

A pesar de que en el DQN original se utiliza una CNN en la red Q para procesar datos de im√°genes, la red Q en los RS basados en RL t√≠picamente est√° compuesta por varias capas completamente conectadas (FC), ya que la entrada, es decir, los estados o acciones, tienen la forma de vectores 1D. Por ejemplo, como se mencion√≥ antes, DEERS [48] utiliza dos tipos de estados como entrada en la red Q: estados positivos y negativos, representados en la Figura 7(a). La red Q es una red FC de cinco capas donde las primeras tres capas son separadas para estados positivos y negativos, y luego las dos √∫ltimas capas conectan ambos estados, emitiendo el valor Q de un par dado de estado y acci√≥n. Para tener en cuenta esta arquitectura de doble estado, la funci√≥n de p√©rdida original de DQN en la ecuaci√≥n (14) se modifica como sigue:

\[ \mathcal{L}(\theta_i) = \mathbb{E}_{s,a \sim \rho(\cdot)} \left[ \left( y_i - Q(s^+, s^-, a; \theta_i) \right)^2 \right], \]

donde \( y_i = \mathbb{E}_{s'} [r + \gamma \max_{a'} Q(s'^+, s'^-, a'; \theta_{i-1}) \mid s^+, s^-, a] \).

Consecuentemente, el gradiente de la funci√≥n de p√©rdida se convierte en:

\[ \nabla_{\theta_i} \mathcal{L}(\theta_i) = \mathbb{E}_{s,a \sim \rho(\cdot)} \left[ \left( r + \gamma \max_{a'} Q(s'^+, s'^-, a'; \theta_{i-1}) - Q(s^+, s^-, a; \theta_i) \right) \nabla_{\theta_i} Q(s^+, s^-, a; \theta_i) \right]. \]

Otros investigadores han utilizado extensiones de DQN. Por ejemplo, DRN [47] adopta dueling DDQN para la optimizaci√≥n de pol√≠tica en la recomendaci√≥n de noticias. En particular, los autores argumentan que mientras la recompensa de tomar una acci√≥n se ve afectada por todas las caracter√≠sticas, es decir, usuario, noticias, contexto y caracter√≠sticas usuario-noticias, hay una recompensa que solo es impactada por las caracter√≠sticas de usuario y contexto. En consecuencia, la funci√≥n Q se divide en la funci√≥n de valor \( V(s) \) y la funci√≥n de ventaja \( A(s, a) \). Como se muestra en la Figura 9(a), mientras \( V(s) \) recibe caracter√≠sticas de estado, la entrada a \( A(s, a) \) est√° compuesta por caracter√≠sticas de estado y acci√≥n.

DEAR [173] estudia el problema de la publicidad junto con la recomendaci√≥n. Combina las dos arquitecturas de red Q de DQN, es decir, A1 y A2, y la arquitectura resultante genera el valor Q de una lista de anuncios candidatos si se insertan en la lista de recomendaci√≥n. En otras palabras, la entrada es similar a la arquitectura A2, es decir, estado y acci√≥n, y la salida es igual a A1, que es una lista que contiene los valores Q de todos los pares estado-acci√≥n.

2) Reproducci√≥n de la Experiencia. Seg√∫n la Tabla 4, la gran mayor√≠a (22 de 28) de los RS basados en DQN utilizan muestreo uniforme original para reproducir experiencias recolectadas. Adem√°s, solo tres de ellos utilizan reproducir la experiencia con prioridad [48, 115, 170]. Los autores en [43] proponen utilizar una reproducci√≥n de muestreo estratificado en lugar de un muestreo uniforme para abordar la varianza del muestreo en entornos din√°micos. El muestreo estratificado es una t√©cnica de muestreo de una poblaci√≥n en la cual la poblaci√≥n entera se divide en varios grupos (llamados estratos) y luego se seleccionan muestras aleatorias de estos estratos [177]. Proponen utilizar algunas caracter√≠sticas estables de los clientes, como g√©nero, edad y geograf√≠a, como estratos.

GoalRec [172] utiliza el replay de experiencia retrospectivo [178]. La idea principal en el replay retrospectivo es aprender tanto de un resultado no deseado como de uno deseado. Dado que el objetivo no afecta la din√°mica del entorno, una trayectoria fallida se etiqueta nuevamente como exitosa, como si el estado en la trayectoria fuera el objetivo real. Esto mejora considerablemente la eficiencia de la muestra.

En contraste con los m√©todos existentes basados en DQN, SADQN [138] no utiliza replay de experiencia para el entrenamiento. En cambio, en cada episodio de la fase de entrenamiento, se selecciona un usuario del conjunto de usuarios y el agente se entrena en las interacciones disponibles hasta que converge. Usando experimentos, los autores afirman que el replay de experiencia de hecho disminuye el rendimiento de SADQN.

3) Exploraci√≥n. Aunque la exploraci√≥n es un factor importante en el aprendizaje del agente, muchos m√©todos basados en DQN parecen pasar por alto este aspecto, ya que no hay indicaci√≥n espec√≠fica al respecto en las publicaciones respectivas. Aparte de t√©cnicas simples de exploraci√≥n como ùúñ-greedy, DRN [47] propone utilizar un enfoque de exploraci√≥n similar al algoritmo de descenso de gradientes de bandit duelo [179]. En particular, hay una red separada para la exploraci√≥n llamada red explore y sus par√°metros pueden obtenerse usando una perturbaci√≥n en los par√°metros de la red actual con par√°metros ùëä:

\[ \Delta ùëä = ùõº ¬∑ rand(-1,1) ¬∑ ùëä, \]

donde ùõº es el coeficiente de exploraci√≥n. Luego, el agente genera una lista combinada de recomendaciones utilizando una interlecci√≥n probabil√≠stica entre los elementos encontrados por la red actual y la red explore.

En recEnergy [141], para equilibrar el compromiso entre exploraci√≥n y explotaci√≥n, se utiliza la exploraci√≥n de Boltzmann [27]. M√°s precisamente, los valores Q de salida de las acciones de la red Q se pasan a trav√©s de una ecuaci√≥n softmax como sigue:

\[ ùëÉ(ùëé) = \frac{exp(Q(a) / \tau)}{\sum_{i=1}^{n} exp(Q(i) / \tau)}, \]

donde ùúè es una temperatura que decae con el tiempo. Este m√©todo garantiza que el modelo explore m√°s frecuentemente inicialmente, y luego comienza a explotar acciones con valores Q m√°s grandes con m√°s frecuencia.

M√©todos de Gradiente de Pol√≠tica. En contraste con los m√©todos basados en el valor, los m√©todos de gradiente de pol√≠tica aprenden una pol√≠tica parametrizada sin la necesidad de una funci√≥n de valor. REINFORCE es un m√©todo de gradiente estoc√°stico de Monte Carlo que actualiza directamente los pesos de la pol√≠tica. Los principales problemas del algoritmo REINFORCE son la alta varianza y el aprendizaje lento. Estos problemas provienen de la naturaleza de Monte Carlo de REINFORCE, ya que selecciona muestras al azar y las actualizaciones se realizan cuando se completa el episodio.

En un trabajo valioso, la Ref. [25] adapta el algoritmo REINFORCE a un generador de candidatos neural con un espacio de acci√≥n muy grande. En particular, en un entorno de RL en l√≠nea, el estimador del gradiente de pol√≠tica puede expresarse como:

\[ \sum_{\tau \sim \pi_{\theta}} \left[ |\tau| \sum_{t=0}^{T} R_t \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) \right], \]

donde ùúãùúÉ es la pol√≠tica parametrizada, ùúè = (s_0, a_0, s_1, ...), y ùëÖ_t es la recompensa acumulada. Dado que en el entorno de RS, a diferencia de los problemas cl√°sicos de RL, la interacci√≥n en l√≠nea o en tiempo real entre el agente y el entorno es inviable y generalmente solo est√° disponible la retroalimentaci√≥n registrada, aplicar el gradiente de pol√≠tica en la ecuaci√≥n (33) est√° sesgado y necesita correcci√≥n. El estimador de gradiente de pol√≠tica corregido fuera de pol√≠tica es entonces:

\[ \sum_{\tau \sim \beta} \frac{\pi_{\theta}(\tau)}{\beta(\tau)} \left[ |\tau| \sum_{t=0}^{T} R_t \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) \right], \]

donde ùõΩ es la pol√≠tica de comportamiento y ùúãùúÉ(ùúè) / ùõΩ(ùúè) es el peso de importancia.

Desde que esta correcci√≥n genera una gran varianza para el estimador debido a los productos encadenados, los autores utilizan una aproximaci√≥n de primer orden para los pesos de importancia, lo que conduce al siguiente estimador sesgado con una varianza menor:

\[ \sum_{\tau \sim \beta} \left[ |\tau| \sum_{t=0}^{T} \pi_{\theta}(a_t | s_t) \beta(\tau) R_t \nabla_{\theta} \log \pi_{\theta}(a_t | s_t) \right]. \]

La Figura 9(b) ilustra la arquitectura neural de la pol√≠tica parametrizada ùúãùúÉ en la ecuaci√≥n (36).

Como se discute en la secci√≥n 2.2, REINFORCE-wb a√±ade una l√≠nea de base a la regla de actualizaci√≥n de REINFORCE para disminuir la varianza (ver Eq. (12)). Varios RS basados en REINFORCE han utilizado este enfoque [114, 128, 132, 150, 167]. Espec√≠ficamente, la l√≠nea de base en estos m√©todos es una red de valor [114, 132, 167], una restricci√≥n [128] y una recompensa promedio [150]. Sin embargo, no est√° claro c√≥mo otros RS basados en REINFORCE [117, 123, 136, 140, 147, 157, 159, 166] abordan el problema de la varianza.

Siguiendo a SeqGAN [180], IRecGAN [130] emplea GANs para desarrollar un recomendador basado en RL basado en modelos. En particular, el generador es responsable de generar recomendaciones y modelar el comportamiento del usuario, y el discriminador se utiliza para reescalar las recompensas generadas. Usando datos generados y datos sin conexi√≥n, REINFORCE se utiliza para optimizar la pol√≠tica de recomendaci√≥n. Similar a SeqGAN, para reducir la varianza, IRecGAN utiliza MCTS con pol√≠tica de roll-out, es decir, muestreo de ùëÅ secuencias de interacci√≥n entre el recomendador y el modelo de usuario y luego promediando las estimaciones.

M√©todos Actor-Critic. DDPG es el m√©todo base utilizado en casi todos los RS basados en actor-critic. DDPG utiliza una arquitectura de actor-critic para combinar DPG y DQN. El actor, tambi√©n llamado red de pol√≠tica, es responsable de generar acciones, y el cr√≠tico, un m√≥dulo DQN, es responsable de evaluar la acci√≥n tomada. DDPG original utiliza varias capas FC o capas convolucionales m√°s FC cuando la entrada es p√≠xel. La capa de salida del actor es una capa tanh para limitar las acciones. Para la exploraci√≥n, DDPG utiliza un ruido temporalmente correlacionado, el proceso Ornstein-Uhlenbeck (OU) [181], que es adecuado para entornos f√≠sicos con momento. Adem√°s, al igual que DQN, se utiliza replay de experiencia con muestreo uniforme.

Wolpertinger [24] es el primer m√©todo actor-critic basado en DDPG para manejar grandes espacios de acci√≥n discretos, con un estudio de caso de recomendaci√≥n. La idea es proporcionar un m√©todo que tenga complejidad sublineal con respecto al espacio de acci√≥n y sea generalizable sobre acciones. Como se muestra en la Figura 9(c), Wolpertinger consta de dos partes: generaci√≥n de acci√≥n y refinamiento de acci√≥n. En la primera parte, las proto-acciones son generadas por el actor en espacio continuo y luego se mapean al espacio discreto utilizando el m√©todo ùëò-vecino m√°s cercano (ùëò-NN). M√°s precisamente, la proto-acci√≥n ÀÜùëé se genera como ÀÜùëé = ùëìùúÉ (ùë†). Esta proto-acci√≥n probablemente no sea una acci√≥n v√°lida, por lo que se mapea a un elemento en A como

\[ g_k(\hat{a}) = \arg \min_{a \in A} \| a - \hat{a} \|_2^2. \]

En la segunda parte, las acciones at√≠picas se filtran utilizando un cr√≠tico, que selecciona la mejor acci√≥n que tiene el valor Q m√°ximo. En otras palabras,

\[ \pi_{\theta}(s) = \arg \max_{a \in g_k} Q_{\theta}(s, a). \]

Wolpertinger se entrena utilizando DDPG. Para la exploraci√≥n, para la tarea de recomendaci√≥n, Wolpertinger utiliza una t√©cnica de exploraci√≥n ùúñ-greedy guiada. En particular, la exploraci√≥n se restringe a un conjunto probablemente bueno de acciones proporcionadas por el simulador del entorno.

La gran mayor√≠a de los m√©todos actor-critic est√°n basados en DDPG [24, 40, 45, 46, 116, 118, 127, 143-146, 148, 149, 151, 160-164, 168, 171]. La Tabla 5 resume estos m√©todos. Como se muestra, solo DRR utiliza replay de experiencia priorizado; los algoritmos restantes utilizan muestreo uniforme o no hay indicios al respecto en las publicaciones respectivas. Otra observaci√≥n importante de la Tabla 5 es que la gran mayor√≠a de los algoritmos no mencionan la exploraci√≥n. De los cinco algoritmos con un m√©todo de exploraci√≥n descrito, tres de ellos, es decir, Wolpertinger, DRR y HRL-Recused, est√°n basados en ùúñ-greedy. Similar a DDPG, DRGR utiliza el proceso OU para fomentar una mejor exploraci√≥n para el actor. Sin embargo, como se mencion√≥ anteriormente, el ruido OU es adecuado para procesos f√≠sicos. Finalmente, MASSA introduce un m√©todo novedoso de exploraci√≥n regularizado por entrop√≠a, un m√©todo similar a soft actor-critic [182].

El m√©todo actor-critic parece ser una arquitectura popular para el aprendizaje por refuerzo multi-agente (MARL). El aprendizaje/entrenamiento centralizado con ejecuci√≥n descentralizada [183, 184] es un marco adecuado para un entorno multi-agente y es adoptado por CROMA, MASSA, DeepChain y MASTER. Por ejemplo, la Figura 10(a) muestra la arquitectura de MADDPG [184], que utiliza un marco de entrenamiento centralizado y ejecuci√≥n descentralizada. MASSA se basa en esta arquitectura y agrega una red de se√±ales al MADDPG, como se muestra en la Figura 10(b), que es responsable de facilitar la cooperaci√≥n entre actores descentralizados.

Hay un par de m√©todos basados en actor-critic que utilizan entrenamiento adversarial para un mejor aprendizaje de pol√≠ticas [152, 155]. Por ejemplo, CRSAL [152] extiende el soft actor-critic [182] con aprendizaje adversarial, a√±adiendo un discriminador dentro del cr√≠tico para distinguir entre di√°logos generados por la red de pol√≠ticas y usuarios reales. En un escenario de razonamiento de caminos sobre un grafo de conocimiento, ADAC [155] utiliza aprendizaje de imitaci√≥n adversarial [185] y define dos discriminadores de camino y metapath para distinguir entre caminos expertos y caminos generados por el actor.

En contraste con otros m√©todos actor-critic, DRESS [124] utiliza PPO y SDAC [175] propone un actor-critic estoc√°stico discreto. Los autores de SDAC proponen un marco general fuera de l√≠nea para RSs de aprendizaje por refuerzo. Primero formulan el problema de recomendaci√≥n como un modelo generativo probabil√≠stico. Luego, proponen un algoritmo estoc√°stico actor-critic para optimizar la pol√≠tica de recomendaci√≥n.

3.2.3 Formulaci√≥n de la Recompensa. Como se muestra en la Figura 6(b), la mayor√≠a (60%) de los RSs basados en DRL pertenecen a R2. Un patr√≥n com√∫nmente utilizado por los RSs en R2 es formular la recompensa como una funci√≥n, o una combinaci√≥n simple, de varios factores o m√©tricas [47, 113, 115, 119, 121, 127, 131, 133, 141, 150, 152, 156-158, 160, 161, 169, 173, 175]. Por ejemplo, en un escenario de recomendaci√≥n de noticias, la recompensa en DRN [47] es una funci√≥n de clics de usuario y actividad del usuario. La raz√≥n detr√°s de incluir la actividad del usuario es que una buena recomendaci√≥n deber√≠a motivar al usuario a usar o interactuar con el sistema nuevamente. Los autores utilizan modelos de supervivencia [186] para modelar el retorno del usuario [187] y la actividad del usuario. FeedRec [119] formula la recompensa como una suma ponderada de m√©tricas instant√°neas, incluyendo clics y compras de usuarios, y m√©tricas retardadas, como la profundidad de navegaci√≥n y el tiempo de permanencia. Los autores consideran los clics de usuario como una m√©trica instant√°nea, y la profundidad de navegaci√≥n y el tiempo de retorno como m√©tricas retardadas. Yu et al. [121] dise√±an una funci√≥n de ventaja compuesta por recompensas visuales, atributivas e hist√≥ricas para abordar el problema de recomendaci√≥n multimodal.

Robust DQN [43] propone utilizar una recompensa aproximada por arrepentimiento para mejorar la estimaci√≥n de la recompensa. La idea es utilizar dos recompensas diferentes, es decir, recompensas actuales y √≥ptimas, y luego calcular el arrepentimiento como la recompensa final. Dado que calcular la recompensa √≥ptima en realidad no es posible, proponen utilizar una recompensa de referencia alternativa, que es la recompensa promedio lograda aplicando el modelo a un subconjunto de usuarios.

Un esquema simple pero efectivo para la optimizaci√≥n multiobjetivo en RSs de aprendizaje por refuerzo es formular la recompensa como una funci√≥n multiobjetivo [134, 146, 159, 162]. Singh et al. [159], por ejemplo, utilizan esta idea para un RS seguro. El formato de la recompensa en su trabajo es el siguiente:

\[ R_{mo} = R_t - C_{risk}, \]

donde \( C_{risk} \) es una restricci√≥n de riesgo de salud. Esta funci√≥n de recompensa equilibra la maximizaci√≥n de la recompensa con la preservaci√≥n de la restricci√≥n de salud. Una formulaci√≥n similar se utiliza para equilibrar el compromiso entre precisi√≥n y diversidad [134, 162] y equidad [146].

En el aprendizaje por refuerzo jer√°rquico, deben definirse dos funciones de recompensa, es decir, para agentes de nivel bajo y alto [114, 123, 143, 165, 166, 171]. Por ejemplo, en HRL-Rec [171], los tiempos de clic en el canal recomendado se consideran como la recompensa del agente de nivel bajo, mientras que la recompensa para el agente de nivel alto se compone de cuatro factores, incluidos los tiempos de clic, el tiempo de permanencia, la diversidad a nivel de lista y la novedad del elemento.

En KGRE-Rec [132], se utiliza una funci√≥n de recompensa retardada. Los autores discuten que es imposible definir una recompensa binaria y dispersa cuando no hay un art√≠culo bueno/target en su problema de recomendaci√≥n. En su lugar, se alienta al agente a encontrar caminos buenos en el grafo, aquellos que conducen a un art√≠culo de inter√©s del usuario con alta probabilidad. Por lo tanto, el agente recibe una recompensa solo en un estado terminal. La misma idea se puede ver en MASTER [168], donde se otorga una recompensa perezosa al agente cuando se realiza con √©xito una solicitud de carga. Sin embargo, la idea de recompensar al agente solo en el estado terminal no siempre es pr√°ctica. Por ejemplo, Liu et al. discuten que dado que no hay un estado terminal bien definido en AnchorKG [169], la funci√≥n de recompensa deber√≠a componerse de recompensas inmediatas y terminales.


Otro m√©todo de formulaci√≥n de recompensas utilizado en R2 es definir la recompensa como la distancia entre el √≠tem recomendado y un √≠tem objetivo [45, 140, 151]. En KGRL [151], por ejemplo, la recompensa se basa en la distancia entre el √≠tem predicho e √≠tem objetivo en el grafo:
\[ r = 100 \sqrt{d(v_p, v_t)} + \epsilon \cdot W_{pt}, \]
donde \( d(v_p, v_t) \) es la distancia entre el √≠tem predicho \( p \) e √≠tem objetivo \( t \), \( \epsilon \) es un regulador, y \( W_{pt} \) es la suma de los pesos del camino m√°s corto desde \( v_p \) hasta \( v_t \). La distancia \( d \) se calcula utilizando el algoritmo de Dijkstra.

Por otro lado, quiz√°s el m√©todo m√°s simple de definici√≥n de recompensas para el dise√±ador es utilizar emp√≠ricamente varios valores reales para diferentes objetivos en el sistema, lo cual se suele usar en R1. Por ejemplo, Zhao et al. utilizan un patr√≥n similar de recompensas num√©ricas en sus propuestas [40, 46, 48, 149], y recompensan tres comportamientos de usuarios, a saber, saltar, hacer clic y ordenar, con algunos n√∫meros, por ejemplo, 0, 1 y 5, respectivamente. El mismo patr√≥n se puede observar en otros RSs de RL pertenecientes a R1 [114, 116, 118, 120, 126, 138, 142].

Del mismo modo, en un escenario de RS conversacional, EAR [147] define una funci√≥n de recompensa dispersa con cuatro valores predefinidos, es decir, una recompensa fuertemente positiva cuando la recomendaci√≥n es exitosa (\( r_s \)), una recompensa positiva si el usuario da retroalimentaci√≥n positiva sobre el atributo preguntado (\( r_a \)), una recompensa fuertemente negativa por la salida del usuario (\( r_q \)), y una ligeramente negativa por cada turno de conversaci√≥n (\( r_p \)). La recompensa total es la suma de estas recompensas y en los experimentos utilizan los valores \( r_s = 1 \), \( r_a = 0.1 \), \( r_q = -0.3 \), y \( r_p = -0.1 \). Una funci√≥n de recompensa similar se utiliza en otros RSs conversacionales [122, 153, 170].

La recompensa retardada tambi√©n se utiliza en R1 por algunos RSs basados en grafos [136, 155, 167], donde el agente solo recibe recompensa cuando alcanza el estado terminal. Por ejemplo, en Ekar [136], el agente recibe una recompensa de +1 si alcanza un √≠tem en el estado terminal con el cual el usuario ha interactuado, 0 si alcanza un √≠tem pero el usuario no ha interactuado con √©l, y -1 si la entidad alcanzada no es un √≠tem en el grafo.

Claro, aqu√≠ tienes la continuaci√≥n y finalizaci√≥n de la traducci√≥n del texto:

### 3.2.4 Construcci√≥n del Entorno

Como se muestra en la Figura 6(c), m√°s de la mitad de los RS basados en DRL utilizan un m√©todo offline para la construcci√≥n del entorno. Casi el 40% de los m√©todos utilizan un simulador, y solo el 10% utiliza un estudio en l√≠nea. En comparaci√≥n con los m√©todos basados en RL, aunque una proporci√≥n similar utiliza el m√©todo offline, el uso de simulaci√≥n ha aumentado al doble y disminuido casi un 60% para los esquemas en l√≠nea. Este gr√°fico muestra que realizar un estudio en l√≠nea se ha vuelto m√°s dif√≠cil o costoso, y la simulaci√≥n se est√° volviendo cada vez m√°s popular entre la comunidad de RS basados en RL.

Entre aquellos que realizan un estudio de simulaci√≥n, SlateQ [80] introduce un entorno de simulaci√≥n de RS basado en RL de c√≥digo abierto, llamado RecSim [188], que brinda al investigador la flexibilidad para evaluar sus algoritmos en diferentes configuraciones. Cascading DQN [135] utiliza GANs para simular a un usuario real y estimar la funci√≥n de recompensa a partir de datos registrados. M√°s precisamente, el entrenamiento de GAN se formula como

\[ \min_\theta \max_\alpha \left( \mathbb{E}_{\phi_\alpha} \left[ \sum_{t=1}^T r_\theta(s_{tt}, a_{tt}) \right] - \frac{R(\phi_\alpha)}{\eta} \right) - \sum_{t=1}^T r_\theta(s_{tt}, a_{tt}^{\text{true}}), \]

donde \( \eta \) es un t√©rmino de regularizaci√≥n, \( tt \) significa datos reales, \( \phi \) representa al generador y genera la pr√≥xima acci√≥n del usuario, y \( r \) es el discriminador que intenta diferenciar entre acciones generadas y acciones reales.

En DEERS [48], un simulador de usuario, con la misma arquitectura que DEERS, se entrena en registros de usuario. Sin embargo, la capa de salida del simulador es una capa softmax para predecir la retroalimentaci√≥n del usuario (recompensa inmediata) basada en la entrada (par de estado e √≠tem recomendado). Los autores afirman que el simulador tiene un 90% de precisi√≥n en predecir la retroalimentaci√≥n del usuario. El mismo enfoque para el estudio de simulaci√≥n ha sido utilizado por otros RSs de RL [46, 119, 121, 128, 137, 143, 145, 148, 149, 160, 161, 172]. Por ejemplo, una idea similar se utiliza en [119], pero el simulador (S Network) proporciona diferentes retroalimentaciones, incluyendo la respuesta del usuario, tiempo de permanencia, tiempo de revisi√≥n y un indicador binario si el usuario se est√° yendo o no. En Pseudo Dyna-Q [137], se entrena un modelo del mundo (simulador de usuario) minimizando un error entre recompensas en l√≠nea y offline. Se utiliza muestreo de importancia truncado [189] para mitigar el sesgo en los datos offline.

Espero que esta traducci√≥n sea de ayuda.

Aqu√≠ est√° la traducci√≥n del texto restante:

### Otro m√©todo popular de simulaci√≥n es desarrollar un simulador basado en filtrado colaborativo [40, 125, 163]. Para ser espec√≠ficos, LIRD [40] construye una memoria con tuplas (ùë†,ùëé,ùëü) observadas en el conjunto de datos de registros y utiliza un m√©todo de similitud, basado en la similitud del coseno, para encontrar el par de estado-acci√≥n m√°s cercano al estado actual y la acci√≥n recomendada. DRR [125] y DRGR [163] utilizan la misma intuici√≥n pero basada en factorizaci√≥n matricial probabil√≠stica [103] y factorizaci√≥n matricial, respectivamente.

Construir un simulador para RS conversacionales es m√°s desafiante que para los escenarios de recomendaci√≥n t√≠picos mencionados anteriormente, ya que hay un peque√±o n√∫mero de conjuntos de datos p√∫blicos disponibles que tengan tanto calificaciones de usuario como lenguaje natural/conversaciones de usuario para ese par de calificaci√≥n de √≠tem. CRM [117] aborda este problema creando usuarios simulados basados en datos de Yelp [190] y un corpus de di√°logo, recopilado mediante trabajadores de crowdsourcing. Los usuarios simulados tienen tres comportamientos: responder la pregunta del agente, encontrar el √≠tem objetivo en una lista y abandonar el di√°logo. El mismo esquema se ha utilizado en otros RS conversacionales de RL [122, 147, 153, 170].

En general, realizar un buen estudio en l√≠nea se ha vuelto m√°s desafiante en los RS modernos con grandes espacios de usuarios e √≠tems, ya que el riesgo de implementar un RS no √≥ptimo es muy alto. Como se discuti√≥ antes, esta es la raz√≥n m√°s probable de una considerable disminuci√≥n en la popularidad del estudio en l√≠nea entre los m√©todos basados en DRL en comparaci√≥n con los m√©todos basados en RL. Quiz√°s dos de los mejores estudios en l√≠nea entre los RS de RL se realizan en [25] y [80] y se llevaron a cabo en YouTube.

Aqu√≠ est√° la traducci√≥n del texto:

### 4 TEMAS EMERGENTES
Despu√©s de revisar los RS basados en DRL, hemos reconocido que hay un par de tendencias que se est√°n formando y que tienen el potencial de madurar con el tiempo. En esta secci√≥n, revisamos brevemente estos temas emergentes.

**Aprendizaje por Refuerzo Multi-agente (MARL)**. El Aprendizaje por Refuerzo Multi-agente (MARL) es una generalizaci√≥n del aprendizaje por refuerzo de un solo agente y se formula como un juego estoc√°stico/markoviano [191, 192]. MARL permite a los RS basados en DRL abordar varias tareas complejas dividi√©ndolas en sub-tareas, donde cada agente puede manejar una de ellas. Por ejemplo, en lugar de optimizar una sola estrategia para todos los escenarios en una aplicaci√≥n de RS de comercio electr√≥nico (como la p√°gina de entrada, la recomendaci√≥n de productos y la finalizaci√≥n de compras), podr√≠a haber varios agentes de RS, cada uno responsable de un escenario espec√≠fico, y la pol√≠tica final se optimiza conjuntamente entre ellos [149]. Desde una perspectiva de teor√≠a de juegos, los m√©todos de MARL pueden dividirse generalmente en tres grupos: completamente cooperativos, completamente competitivos y una mezcla de ambos [192].

Recientemente, varios RS basados en DRL han empleado MARL para abordar problemas como la recomendaci√≥n de colaboradores acad√©micos [96], la recomendaci√≥n de menciones en Twitter [127], la recomendaci√≥n por p√°gina [148], la recomendaci√≥n de cadenas completas [149] y la recomendaci√≥n de puntos de carga [168]. Como se mencion√≥ anteriormente, el actor-cr√≠tico con entrenamiento centralizado y ejecuci√≥n descentralizada ha sido un marco popular para los RS basados en DRL que utilizan MARL [127, 148, 149, 168]. En un entorno cooperativo [127, 148, 149, 168], un desaf√≠o es determinar el papel de cada jugador en el √©xito general del equipo. CROMA [127], con dos actores y un cr√≠tico centralizado, aborda este problema mediante un esquema de ventaja diferenciado utilizando operaci√≥n inversa. Espec√≠ficamente, cada agente actor puede estimar su ventaja particular restando el valor Q general de la acci√≥n conjunta, calculado por el cr√≠tico centralizado, del valor Q de una acci√≥n inversa. Una arquitectura similar se utiliza en DeepChain [149] para optimizar conjuntamente la recompensa general de una sesi√≥n. Sin embargo, no est√° claro c√≥mo DeepChain resuelve el problema mencionado anteriormente, es decir, la recompensa compartida para dos actores, lo cual es crucial para su entrenamiento efectivo. En MASSA [148], se utiliza un MARL con agentes separados de actor y cr√≠tico para abordar una recomendaci√≥n por p√°gina multi-m√≥dulo. Se utiliza un concepto de teor√≠a de juegos llamado equilibrio correlacionado [193] en formato de red de se√±ales para manejar la comunicaci√≥n entre agentes.

MASTER [168] considera cada punto de carga para veh√≠culos el√©ctricos como un agente distribuido y utiliza un cr√≠tico centralizado para coordinar estos agentes. Se emplean un par de t√©cnicas, incluidos juegos de oferta y m√∫ltiples cr√≠ticos, para abordar desaf√≠os como la cooperaci√≥n entre agentes, la competencia futura entre solicitudes y la optimizaci√≥n multiobjetivo. En un escenario competitivo diferente, los autores en [96] utilizan MARL para recomendar colaboradores cient√≠ficos. Cada autor que busca un colaborador se considera un agente y aprende una pol√≠tica √≥ptima utilizando el algoritmo de iteraci√≥n de valor de gradiente.


### 4 TEMAS EMERGENTES
Tras revisar los RS basados en DRL, hemos identificado un par de tendencias que est√°n tomando forma y que tienen el potencial de madurar con el tiempo. En esta secci√≥n, revisaremos brevemente estos temas emergentes.

**Aprendizaje por Refuerzo Jer√°rquico y Meta-controlador (HRL)**. El Aprendizaje por Refuerzo Jer√°rquico (HRL) inicialmente se busc√≥ para abordar el problema de escalabilidad en los algoritmos tradicionales de RL [194]. Sin embargo, en HRL es posible definir m√∫ltiples capas de pol√≠ticas, cada una de las cuales puede entrenarse para proporcionar niveles superiores de abstracciones temporales y conductuales, lo que permite resolver tareas m√°s complejas [195, 196]. La recomendaci√≥n no es una excepci√≥n y varios investigadores han utilizado HRL en el dominio de los RS [114, 123, 143, 165, 166, 171].

En general, todos estos RS basados en RL definen una HRL con dos niveles de jerarqu√≠as donde un agente de alto nivel define un objetivo alto/abstracto y un agente de bajo nivel intenta satisfacer ese objetivo. Por ejemplo, CEI [114] construye un RS conversacional sobre un m√©todo HRL profundo [197], que utiliza ideas de un marco HRL popular y tradicional, llamado opciones [198]. CEI utiliza un meta-controlador que selecciona un objetivo (conversaci√≥n informal o recomendaci√≥n) en un estado dado, y un controlador realiza una acci√≥n siguiendo una pol√≠tica espec√≠fica del objetivo para satisfacer el objetivo definido. Zhang et al. [123] emplean HRL para la recomendaci√≥n de cursos en cursos masivos en l√≠nea (MOOCs). La idea clave es desarrollar un revisor de perfiles utilizando HRL, que elimina cursos ruidosos de los perfiles de los usuarios. Esto se descompone en dos tareas de alto y bajo nivel: dado un perfil de usuario y un curso objetivo, ¬ødeber√≠a revisarse el perfil (alto nivel) y, si es as√≠, qu√© cursos en el perfil deben eliminarse (bajo nivel)? DARL [166] mejora el RS de Zhang et al. haciendo que la unidad de recomendaci√≥n sea m√°s adaptable. Es decir, equipan el m√≥dulo b√°sico de recomendaci√≥n en el trabajo de Zhang con un mecanismo de atenci√≥n para tener en cuenta el inter√©s din√°mico de los usuarios en diversos cursos. HRL-Rec [171] utiliza HRL en un escenario de recomendaci√≥n integrada. Un agente de bajo nivel genera una lista de canales, y un agente de alto nivel recomienda una lista de elementos con la restricci√≥n de canal seleccionada por el agente de bajo nivel. Adem√°s, MaHRL [143] aborda la m√©trica de conversi√≥n escasa en comercio electr√≥nico utilizando HRL. M√°s precisamente, hay un agente de alto nivel responsable de rastrear el inter√©s de conversi√≥n escasa a largo plazo estableciendo m√∫ltiples objetivos abstractos para el agente de bajo nivel, mientras que el agente de bajo nivel sigue estos objetivos e intenta captar el inter√©s de clics a corto plazo. Finalmente, DHCRS [165] intenta abordar el gran espacio de acci√≥n en los RS utilizando un HRL de dos niveles, donde un DQN de alto nivel selecciona categor√≠as de elementos y un DQN de bajo nivel selecciona un elemento en la categor√≠a para recomendar.

En un tema emergente, un grupo de investigadores ha utilizado RL como m√≥dulo meta-controlador en RS conversacionales. Esto significa que, en lugar de usar RL para optimizar la pol√≠tica de recomendaci√≥n, similar a HRL, estos m√©todos utilizan RL para seleccionar ya sea la recomendaci√≥n de elementos o hacer preguntas a los usuarios para refinar las recomendaciones. Pero a diferencia de HRL, hay solo un nivel que utiliza RL y la unidad de recomendaci√≥n utiliza otras t√©cnicas, como aprendizaje supervisado, para generar las recomendaciones. Este es el tema com√∫n en un par de RS basados en RL [117, 122, 131, 147, 152, 153, 170]. Por ejemplo, CRM [117] se compone de tres partes principales: un rastreador de creencias, un recomendador y una red de pol√≠ticas (m√≥dulo RL). La unidad de rastreo de creencias es responsable de extraer pares faceta-valor (algunas restricciones) de las expresiones de los usuarios y convertirlas en creencias utilizando una red LSTM. Se utiliza una m√°quina de factorizaci√≥n [199] en el recomendador para generar un conjunto de recomendaciones. Finalmente, se utiliza una red de pol√≠ticas neurales, optimizada por REINFORCE, para gestionar el sistema conversacional, es decir, decidir si pedir m√°s informaci√≥n al usuario o recomendar los elementos.

**RS basados en grafos de conocimiento.** La incorporaci√≥n de grafos de conocimiento en RS puede aumentar la precisi√≥n y la explicabilidad de las recomendaciones [38]. Utilizar grafos de conocimiento proporciona a los RS basados en RL diferentes informaci√≥n √∫til, que puede abordar la ineficiencia de muestras en DRL. Recientemente, muchos investigadores comenzaron a utilizar esta idea para mejorar el rendimiento y la explicabilidad de las recomendaciones [132, 136, 150, 151, 155‚Äì158, 167, 169, 170]. Por ejemplo, la idea en KGRE-Rec [132] es no solo recomendar un conjunto de elementos, sino tambi√©n los caminos en el grafo de conocimiento para mostrar la raz√≥n por la cual el m√©todo ha hecho estas recomendaciones. Un ejemplo de este razonamiento gr√°fico se muestra en la Fig. 10(c). Para un usuario dado ùê¥, el algoritmo deber√≠a encontrar los elementos ùêµ y ùêπ con sus caminos de razonamiento en el grafo, como {Usuario ùê¥ ‚Üí Elemento ùê¥ ‚Üí Marca ùê¥ ‚Üí Elemento ùêµ} y {Usuario ùê¥ ‚Üí Caracter√≠stica ùêµ ‚Üí Elemento ùêπ}. Obviamente, las t√©cnicas basadas en gr√°ficos enfrentan el problema de escalabilidad a medida que el n√∫mero de nodos y enlaces puede crecer significativamente, en proporci√≥n al n√∫mero de usuarios y elementos. Para abordar este problema, KGRE-Rec propone una estrategia de poda de acciones condicionales al usuario, que utiliza una funci√≥n de puntuaci√≥n para mantener solo los bordes importantes condicionados en el usuario inicial.

**RL Supervisado.** La caracter√≠stica clave que distingue RL del aprendizaje supervisado es si los datos de entrenamiento sirven como una se√±al de evaluaci√≥n, como recompensa num√©rica, o como una se√±al de error [200]. Sin embargo, estos m√©todos, RL y aprendizaje supervisado, pueden combinarse para mejorar el aprendizaje de pol√≠ticas cuando ambos tipos de se√±ales est√°n disponibles. Wang et al. [116] utilizan esta idea para recomendar din√°micamente opciones de tratamiento a pacientes. La idea es que mientras el modelo debe maximizar el retorno esperado, tambi√©n debe minimizar la diferencia de las prescripciones m√©dicas. En particular, en una arquitectura actor-cr√≠tico, el actor es responsable de recomendar la mejor prescripci√≥n optimizando la siguiente funci√≥n objetivo:

\[ \mathcal{J}(\theta) = (1 - \alpha)\mathcal{J}_{RL}(\theta) + \alpha(-\mathcal{J}_{SL}(\theta)) \]

donde \( \mathcal{J}_{RL}(\theta) \) y \( \mathcal{J}_{SL}(\theta) \) son las funciones objetivo de las tareas de RL y aprendizaje supervisado, respectivamente, y \( \alpha \) es un factor de ponderaci√≥n. De manera similar, Liu et al. [160, 161] aprovechan el aprendizaje supervisado para guiar el m√≥dulo RL en el aprendizaje de pol√≠ticas mejores. M√°s precisamente, en [160], una se√±al de aprendizaje supervisado ayuda a generar mejores incrustaciones para la representaci√≥n del estado, y en [161], se entrena un modelo de aprendizaje supervisado para guiar la pol√≠tica RL para centrarse en la recompensa a corto plazo y generar recomendaciones orientadas a los principales.

**Aprendizaje por Imitaci√≥n y Tareas Auxiliares.** Adem√°s de los temas emergentes mencionados anteriormente, hay algunos temas que, aunque son menos populares en comparaci√≥n con los discutidos anteriormente, creemos que tienen el potencial de convertirse en temas emergentes en el futuro. Estos temas incluyen RL/adversarial, RL seguro, aprendizaje auto-supervisado y aprendizaje por imitaci√≥n. El entrenamiento adversarial utilizando GANs es un tema emergente interesante utilizado en [130, 152, 155]. Como se mencion√≥ anteriormente en la secci√≥n de M√©todos Actor-Cr√≠tico, CRSAL [152] y AD

AC [155] utilizan entrenamiento adversarial integrado con arquitectura actor-cr√≠tico para un mejor entrenamiento del agente. Adem√°s, como se discuti√≥ en los m√©todos de gradiente de pol√≠tica, IRecGAN [130] propone un RL basado en modelos utilizando GANs con el prop√≥sito de reducci√≥n de varianza y eficiencia de muestras.

En RL seguro, es importante que el agente respete algunas restricciones de seguridad, junto con maximizar la recompensa a largo plazo [201]. En la Ref. [159], se propone un RS basado en RL seguro multiobjetivo para mejorar el bienestar a largo plazo de los usuarios. En particular, el agente intenta simult√°neamente maximizar la participaci√≥n del usuario y la salud del usuario en el peor de los casos.

El aprendizaje auto-supervisado (SSL) capacita al modelo para utilizar etiquetas disponibles libremente con los datos. En [154], se introduce un marco para aumentar los RS basados en RL con SSL. M√°s precisamente, los autores proponen un marco con dos cabezas: RL y SSL. Mientras que la cabeza de RL se utiliza como regularizador para ajustar las recomendaciones, la cabeza de SSL proporciona muestras negativas para actualizar los par√°metros.

En el aprendizaje por imitaci√≥n, el agente se entrena para realizar una tarea a partir de demostraciones [202]. Zhang et al. [124] combinan la imaginaci√≥n (RL basado en modelos) y el aprendizaje por imitaci√≥n para recomendar historias de b√∫squeda personalizadas. Argumentan que el objetivo del aprendizaje por imitaci√≥n es imitar la pol√≠tica de un agente recomendador del cual se ha recopilado datos de registro. Adem√°s, se imaginan algunas sesiones ficticias por parte del agente y se guardan en una memoria separada, que se utiliza para ajustar el entrenamiento del agente.

Estos temas emergentes indican nuevas direcciones y enfoques en el campo de los RS basados en RL, mostrando c√≥mo la innovaci√≥n contin√∫a expandiendo las posibilidades de mejorar la precisi√≥n, la explicabilidad y la eficiencia de los sistemas de recomendaci√≥n mediante t√©cnicas avanzadas de aprendizaje autom√°tico.


5 DIRECCIONES DE INVESTIGACI√ìN ABIERTAS

Recomendaci√≥n de Conjuntos de Elementos. Los algoritmos de RL fueron desarrollados originalmente para seleccionar una sola acci√≥n, por ejemplo, la acci√≥n con el valor Q m√°s alto, en cada paso de tiempo entre diferentes acciones circundantes [203]. Sin embargo, en el campo de los RS, similar a muchos sistemas de soporte para decisiones secuenciales [203], es prudente recomendar un conjunto o lista de elementos y permitir que el usuario participe en el proceso de toma de decisiones para elegir la mejor acci√≥n, ya que el objetivo final suele ser la satisfacci√≥n del usuario y la aceptaci√≥n de la recomendaci√≥n. A pesar de algunos esfuerzos [25, 40, 80, 112, 135], los algoritmos de RL actuales no pueden manejar este problema. Solo hay dos estudios [80, 112] en el campo de los RLRS que investigan profundamente este problema. Slate-MDP [112] intenta resolver este problema buscando el espacio de pol√≠ticas para cada espacio en el conjunto individualmente. SlateQ [80] propone calcular la combinaci√≥n del conjunto de acciones y considerar cada combinaci√≥n como una acci√≥n. Slate-MDP no puede garantizar ninguna optimalidad, y SlateQ solo es aplicable en RSs de dos etapas y no logra escalar a RSs de una sola etapa con grandes espacios de acciones. Se necesita m√°s atenci√≥n en este aspecto y se deben realizar m√°s estudios con bases te√≥ricas s√≥lidas en el futuro.

Explicabilidad. La recomendaci√≥n explicativa es la capacidad de un RS no solo para proporcionar una recomendaci√≥n, sino tambi√©n para explicar por qu√© se ha hecho una recomendaci√≥n espec√≠fica [38]. La explicaci√≥n sobre las recomendaciones realizadas podr√≠a mejorar la experiencia del usuario, aumentar su confianza en el sistema y ayudarles a tomar mejores decisiones [204‚Äì206]. Los m√©todos explicativos podr√≠an dividirse en dos grupos generales: intr√≠nsecos al modelo o agn√≥sticos al modelo [207]. En el primero, la explicaci√≥n es parte del proceso de recomendaci√≥n, mientras que en el segundo, la explicaci√≥n se proporciona despu√©s de que se hace la recomendaci√≥n. Un m√©todo de explicaci√≥n intr√≠nseco podr√≠a ser el m√©todo que revisamos anteriormente [132]. Por otro lado, como ejemplo agn√≥stico al modelo [208], se utiliza RL para proporcionar explicaciones para diferentes m√©todos de recomendaci√≥n. En particular, el m√©todo utiliza un par de agentes; uno es responsable de generar explicaciones y otro predice si la explicaci√≥n generada es lo suficientemente buena para el usuario. Una aplicaci√≥n interesante de la recomendaci√≥n explicativa es en la depuraci√≥n de RSs fallidos [208]. Es decir, a trav√©s de las explicaciones proporcionadas, podemos rastrear la fuente de problemas en nuestro sistema y ver qu√© partes no est√°n funcionando correctamente. Aunque ha habido algunos esfuerzos en RLRSs para proporcionar recomendaciones explicativas [132, 136, 155, 167, 169], todav√≠a hay una falta en este aspecto y se requiere m√°s atenci√≥n en el futuro.

Dise√±o. Todos los RLRSs revisados emplean algoritmos de RL/DRL que originalmente fueron desarrollados en dominios distintos de los RSs, como los juegos [62, 67, 74]. Estos m√©todos generalmente est√°n dise√±ados basados en procesos f√≠sicos o procesos gaussianos, no basados en la naturaleza compleja y din√°mica de los seres humanos. Si bien es prudente adherirse a los algoritmos de RL de vanguardia disponibles y adaptarlos para RLRSs, a veces pensar de manera innovadora podr√≠a mejorar sustancialmente el campo. Por ejemplo, en lugar de los algoritmos de RL basados en MDP usuales, la Ref. [209] utiliza estrategias de evoluci√≥n [210] para optimizar la pol√≠tica de recomendaci√≥n, o la Ref. [211] adopta ideas de una literatura diferente y las adapta al problema de recomendaci√≥n. Relacionado con esto, como se encuest√≥ en [14], hay muchos modelos de aprendizaje profundo desarrollados para RSs. Dado que el aprendizaje profundo y el DRL est√°n estrechamente relacionados, combinar sabiamente estos modelos con algoritmos de RL tradicionales podr√≠a superar a los algoritmos DRL existentes. Por √∫ltimo, aunque algunos algoritmos de RL como Q-learning han sido m√°s populares entre los RLRSs que otros algoritmos de RL, no hay pistas o justificaciones detr√°s del uso de un algoritmo de RL espec√≠fico para una aplicaci√≥n de RS. Por lo tanto, ser√≠a un gran estudio encontrar posiblemente una relaci√≥n entre el algoritmo de RL y la aplicaci√≥n de RS.

Ambiente y Evaluaci√≥n. La Fig. 11(a) muestra las m√©tricas m√°s populares en los RLRSs. Como se muestra, no hay una m√©trica espec√≠ficamente desarrollada para los RLRSs y casi todas las m√©tricas se han tomado del campo de la Recuperaci√≥n de Informaci√≥n. Aunque los campos de RSs y Recuperaci√≥n de Informaci√≥n son muy cercanos, son eventualmente campos diferentes. La recompensa tambi√©n est√° entre las m√©tricas populares utilizadas por los RLRSs, que es una m√©trica com√∫n en la literatura de RL. Este an√°lisis muestra que hay una falta de m√©tricas espec√≠ficamente dise√±adas para los RLRSs. Por otro lado, la Fig. 11(b) ilustra los conjuntos de datos m√°s populares utilizados para evaluar los RLRSs. MovieLens es el conjunto de datos m√°s popular por mucho. Un gran n√∫mero de conjuntos de datos utilizados para la evaluaci√≥n no est√°n definidos o son p√∫blicos (consulte las Tablas 2 y 3). Esto limita la aplicaci√≥n y el dise√±o de los RLRSs a solo unas pocas aplicaciones, como el entretenimiento. Por lo tanto, es importante recopilar y compartir m√°s conjuntos de datos en diversos dominios para evaluar mejor los RLRSs. Otro aspecto en la evaluaci√≥n de los RLRSs que necesita m√°s mejora en el futuro es tener un entorno de simulaci√≥n m√°s fuerte y unificado, algo que pueda desempe√±ar el papel de un punto de referencia en la evaluaci√≥n de los RLRSs. Como se mencion√≥ anteriormente, la evaluaci√≥n en l√≠nea es el m√©todo natural para evaluar los RLRSs; sin embargo, es dif√≠cil y costoso realizar un estudio en l√≠nea adecuado. Por otro lado, el entorno fuera de l√≠nea, es decir, un conjunto de datos, es est√°tico y sesgado. Por lo tanto, esto muestra claramente la importancia de desarrollar un simulador general y s√≥lido para los RLRSs, algo similar a OpenAI Gym [212] en la literatura de RL. Aunque recientemente se han desarrollado simuladores de entorno para RLRSs [213‚Äì217], esta tendencia debe continuar y fortalecerse.

Reproducibilidad. El efecto de la reproducibilidad en el avance de un campo es innegable. Por ejemplo, el campo de la s√≠ntesis de im√°genes utilizando GANs ha visto resultados asombrosos en un corto per√≠odo de tiempo [218‚Äì220], y sin duda, un factor efectivo ha sido la pr√°ctica com√∫n de compartir c√≥digos de implementaci√≥n, conjuntos de datos y resultados de investigaci√≥n. Como se ilustra en las Tablas 2 y 3, no podemos ver esta tendencia en la comunidad de investigaci√≥n de RLRS y solo alrededor del 16% de los investigadores han compartido sus c√≥digos de implementaci√≥n. Ser√≠a √∫til y podr√≠a acelerar significativamente el progreso del campo si los investigadores presentaran con precisi√≥n el valor de los par√°metros importantes y los hiperpar√°metros utilizados en sus experimentos, realizaran pruebas de significancia estad√≠stica para los resultados presentados, revelaran qu√© semillas aleatorias se han utilizado para repetir los experimentos y compartieran sus c√≥digos de implementaci√≥n y conjuntos de datos (si los conjuntos de datos no son p√∫blicos).

6. CONCLUSI√ìN

En este art√≠culo, presentamos una encuesta exhaustiva sobre el estado del arte en los Sistemas de Recomendaci√≥n basados en RL. Destacamos el papel importante del DRL en cambiar la direcci√≥n de investigaci√≥n en el campo de los RLRS, y en consecuencia, clasificamos los algoritmos en dos grupos generales, es decir, m√©todos basados en RL y DRL. Luego, propusimos un marco para los RLRS con cuatro componentes: representaci√≥n del estado, optimizaci√≥n de pol√≠ticas, formulaci√≥n de recompensas y construcci√≥n del entorno, y revisamos los algoritmos en consecuencia. Aunque se han propuesto muchos RLRS recientemente, creemos que la investigaci√≥n en RLRS todav√≠a est√° en sus primeras etapas y necesita muchos avances. Tanto RL como los RS son √°reas de investigaci√≥n activas y de inter√©s espec√≠fico para grandes empresas y negocios, por lo que podemos esperar ver nuevos y emocionantes modelos surgir cada a√±o. Finalmente, esperamos que esta encuesta pueda ayudar a los investigadores a comprender los conceptos clave y a avanzar en el campo en el futuro.