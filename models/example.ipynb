{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 12:54:39.488187: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732290879.574603  179707 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732290879.596016  179707 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-22 12:54:39.677769: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_estudiante</th>\n",
       "      <th>programa</th>\n",
       "      <th>exitosos</th>\n",
       "      <th>fallidos</th>\n",
       "      <th>solemne_1</th>\n",
       "      <th>solemne_2</th>\n",
       "      <th>solemne_3</th>\n",
       "      <th>solemne_4</th>\n",
       "      <th>score_a</th>\n",
       "      <th>score_p</th>\n",
       "      <th>...</th>\n",
       "      <th>e43</th>\n",
       "      <th>e44</th>\n",
       "      <th>e45</th>\n",
       "      <th>e46</th>\n",
       "      <th>e47</th>\n",
       "      <th>e48</th>\n",
       "      <th>e49</th>\n",
       "      <th>e50</th>\n",
       "      <th>e51</th>\n",
       "      <th>e52</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>INGENIERIA INDUSTRIAL</td>\n",
       "      <td>15</td>\n",
       "      <td>24</td>\n",
       "      <td>6.2</td>\n",
       "      <td>6.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>INGENIERIA CIVIL INDUSTRIAL</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>INGENIERIA CIVIL INDUSTRIAL</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>5.6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>INGENIERIA CIVIL INDUSTRIAL</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>3.9</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.7</td>\n",
       "      <td>4.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>INGENIERIA EN COMPUTACION E INFORMATICA</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>4.6</td>\n",
       "      <td>6.6</td>\n",
       "      <td>4.7</td>\n",
       "      <td>5.3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_estudiante                                 programa  exitosos  fallidos  \\\n",
       "0              0                    INGENIERIA INDUSTRIAL        15        24   \n",
       "1              1              INGENIERIA CIVIL INDUSTRIAL        10         6   \n",
       "2              2              INGENIERIA CIVIL INDUSTRIAL        11        20   \n",
       "3              3              INGENIERIA CIVIL INDUSTRIAL         7        17   \n",
       "4              4  INGENIERIA EN COMPUTACION E INFORMATICA        13        18   \n",
       "\n",
       "   solemne_1  solemne_2  solemne_3  solemne_4  score_a  score_p  ...  e43  \\\n",
       "0        6.2        6.8        5.1        6.0      5.0      4.0  ...    0   \n",
       "1        7.0        6.9        7.0        7.0      4.0      4.0  ...    0   \n",
       "2        5.6        6.0        4.5        5.4      5.0      4.0  ...    0   \n",
       "3        3.9        4.7        3.7        4.1      3.0      3.0  ...    0   \n",
       "4        4.6        6.6        4.7        5.3      6.0      4.0  ...    0   \n",
       "\n",
       "   e44  e45  e46  e47  e48  e49  e50  e51  e52  \n",
       "0    1    0    0    0    0    0    0    0    0  \n",
       "1    0    0    0    0    0    0    0    0    0  \n",
       "2    0    0    0    0    0    0    0    0    0  \n",
       "3    0    0    0    0    0    0    0    0    0  \n",
       "4    1    0    0    0    0    0    0    0    0  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random \n",
    "import warnings\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils.modelos import *\n",
    "from utils.analisis import *\n",
    "from utils.transformacion import *\n",
    "from utils.recomendaciones import *\n",
    "\n",
    "from pandas import DataFrame\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "\n",
    "VAR_SEED = 42\n",
    "VAR_TESTSET_SIZE = 0.20\n",
    "VAR_DIR_DATA_CLEANING = '../data/cleaning'\n",
    "\n",
    "\n",
    "random.seed(VAR_SEED)\n",
    "np.random.seed(VAR_SEED)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "EJERCICIOS = pd.read_csv(f\"{VAR_DIR_DATA_CLEANING}/ejercicios.csv\", encoding=\"latin1\")\n",
    "ESTUDIANTES = pd.read_csv(f\"{VAR_DIR_DATA_CLEANING}/estudiantes.csv\", encoding=\"latin1\")\n",
    "\n",
    "\n",
    "# 'h1', 'h2', 'h3', 'h4', 's1', 's2', 's3', 's4', 'k1', 'k2', 'k3', 'k4'\n",
    "# 'hito', 'skill', 'knowledge', 'complexity', 'complexity12', 'puntos', 'enunciado', 'dificultad', 'score_a', 'score_d', 'score_p', 'score_s'\n",
    "df_items = EJERCICIOS[['id_ejercicio','h1', 'h2', 'h3', 'h4', 's1', 's2', 's3', 's4', 'k1', 'k2', 'k3', 'k4']] \n",
    "\n",
    "\n",
    "# estudiantes_reprobados = ESTUDIANTES.query(\" `solemne_1` < 4.0 and `solemne_2` < 4.0 and `solemne_3` < 4.0 and `solemne_4` < 4.0 \")\n",
    "# estudiantes_aprovados = ESTUDIANTES[~ESTUDIANTES['id_estudiante'].isin(estudiantes_reprobados['id_estudiante'])]\n",
    "df_users = ESTUDIANTES.copy()\n",
    "df_users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# División inicial en train y test\n",
    "train_data, test_data = train_test_split(df_users, test_size=VAR_TESTSET_SIZE, random_state=VAR_SEED)\n",
    "# División adicional en train y validation\n",
    "train_data, validation_data = train_test_split(train_data, test_size=VAR_TESTSET_SIZE, random_state=VAR_SEED)\n",
    "\n",
    "# Crear escalador y codificador\n",
    "scaler = MinMaxScaler()\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Columnas a normalizar y codificar\n",
    "columns_to_normalize = ['exitosos', 'fallidos', 'solemne_1', 'solemne_2', 'solemne_3', 'solemne_4', 'score_a', 'score_p', 'score_d', 'score_s']\n",
    "column_to_encode = 'programa'\n",
    "\n",
    "# Ajustar en el conjunto de entrenamiento\n",
    "scaler.fit(train_data[columns_to_normalize])        # Ajustar el escalador\n",
    "label_encoder.fit(train_data[column_to_encode])     # Ajustar el codificador\n",
    "\n",
    "# Entrenamiento\n",
    "train_data[columns_to_normalize] = scaler.transform(train_data[columns_to_normalize])\n",
    "train_data[column_to_encode] = label_encoder.transform(train_data[column_to_encode])\n",
    "\n",
    "# Validación\n",
    "validation_data[columns_to_normalize] = scaler.transform(validation_data[columns_to_normalize])\n",
    "validation_data[column_to_encode] = label_encoder.transform(validation_data[column_to_encode])\n",
    "\n",
    "# Prueba\n",
    "test_data[columns_to_normalize] = scaler.transform(test_data[columns_to_normalize])\n",
    "test_data[column_to_encode] = label_encoder.transform(test_data[column_to_encode])\n",
    "\n",
    "features_users_data = ['id_estudiante', 'programa', 'exitosos', 'fallidos', 'score_a', 'score_p', 'score_d', 'score_s']\n",
    "features_users_inte = ['id_estudiante'] + [f\"e{i}\" for i in range(len(df_items))]\n",
    "\n",
    "df_test_users = test_data[features_users_data]\n",
    "df_train_users = train_data[features_users_data]\n",
    "df_validation_users = validation_data[features_users_data]\n",
    "\n",
    "df_test_interacciones = test_data[features_users_inte]\n",
    "df_train_interacciones = train_data[features_users_inte]\n",
    "df_validation_interacciones = validation_data[features_users_inte]\n",
    "\n",
    "\n",
    "# # Filtrar estudiantes aprobados según las condiciones\n",
    "estudiantes_aprovados = ESTUDIANTES[~ESTUDIANTES['id_estudiante'].isin(\n",
    "    ESTUDIANTES.query(\"`solemne_1` < 4.0 and `solemne_2` < 4.0 and `solemne_3` < 4.0 and `solemne_4` < 4.0\")['id_estudiante']\n",
    ")]\n",
    "\n",
    "# Total de ítems\n",
    "n = len(EJERCICIOS)\n",
    "\n",
    "# Últimas 'n' columnas\n",
    "columnas = ESTUDIANTES.columns[-n:]\n",
    "\n",
    "# Limpiar nombres de columnas eliminando la 'e' al inicio\n",
    "columnas_limpias = columnas.str.replace('e', '').astype(int)\n",
    "\n",
    "# Renombrar las columnas temporalmente para evitar problemas\n",
    "estudiantes_aprovados.columns = list(ESTUDIANTES.columns[:-n]) + columnas_limpias.tolist()\n",
    "\n",
    "# Calcular la suma para cada ítem y convertir a diccionario\n",
    "popularidad_items = estudiantes_aprovados.iloc[:, -n:].sum(axis=0).to_dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASES DE DOS TORRES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserTower(nn.Module):\n",
    "    \"\"\"\n",
    "    Red neuronal para procesar características de los usuarios.\n",
    "    \"\"\"\n",
    "    def __init__(self, user_input_size, embedding_size, dropout_rate):\n",
    "        super(UserTower, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(user_input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, embedding_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Generar embedding para las características del usuario\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class ItemTower(nn.Module):\n",
    "    \"\"\"\n",
    "    Red neuronal para procesar características de los ítems.\n",
    "    \"\"\"\n",
    "    def __init__(self, item_input_size, embedding_size, dropout_rate):\n",
    "        super(ItemTower, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(item_input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, embedding_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Generar embedding para las características del ítem\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class TwoTowerModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo de dos torres para calcular la afinidad entre usuarios e ítems.\n",
    "    \"\"\"\n",
    "    def __init__(self, user_input_size, item_input_size, embedding_size, dropout_rate):\n",
    "        super(TwoTowerModel, self).__init__()\n",
    "        self.user_tower = UserTower(user_input_size, embedding_size, dropout_rate)\n",
    "        self.item_tower = ItemTower(item_input_size, embedding_size, dropout_rate)\n",
    "\n",
    "    def forward(self, user_input, item_input):\n",
    "        # Generar embeddings del usuario y del ítem\n",
    "        user_embedding = self.user_tower(user_input)\n",
    "        item_embedding = self.item_tower(item_input)\n",
    "        \n",
    "        # Calcular el score como el producto punto entre los embeddings\n",
    "        score = torch.sum(user_embedding * item_embedding, dim=1)\n",
    "        return torch.sigmoid(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserTowerV1(nn.Module):\n",
    "    \"\"\"\n",
    "    Red neuronal para procesar características de los usuarios con más capas.\n",
    "    \"\"\"\n",
    "    def __init__(self, user_input_size, embedding_size, dropout_rate):\n",
    "        super(UserTowerV1, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(user_input_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, embedding_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class ItemTowerV1(nn.Module):\n",
    "    \"\"\"\n",
    "    Red neuronal para procesar características de los ítems con más capas.\n",
    "    \"\"\"\n",
    "    def __init__(self, item_input_size, embedding_size, dropout_rate):\n",
    "        super(ItemTowerV1, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(item_input_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, embedding_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class TwoTowerModelV1(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo de dos torres para calcular la afinidad entre usuarios e ítems.\n",
    "    \"\"\"\n",
    "    def __init__(self, user_input_size, item_input_size, embedding_size, dropout_rate):\n",
    "        super(TwoTowerModelV1, self).__init__()\n",
    "        self.user_tower = UserTowerV1(user_input_size, embedding_size, dropout_rate)\n",
    "        self.item_tower = ItemTowerV1(item_input_size, embedding_size, dropout_rate)\n",
    "\n",
    "    def forward(self, user_input, item_input):\n",
    "        user_embedding = self.user_tower(user_input)\n",
    "        item_embedding = self.item_tower(item_input)\n",
    "        score = torch.sum(user_embedding * item_embedding, dim=1)\n",
    "        return torch.sigmoid(score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserTowerV2(nn.Module):\n",
    "    \"\"\"\n",
    "    Red neuronal para procesar características de los usuarios sin Batch Normalization y con LeakyReLU.\n",
    "    \"\"\"\n",
    "    def __init__(self, user_input_size, embedding_size, dropout_rate):\n",
    "        super(UserTowerV2, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(user_input_size, 256),  # Aumentar la dimensión inicial\n",
    "            nn.LeakyReLU(negative_slope=0.1),  # Activación LeakyReLU\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, 128),  # Segunda capa con menor dimensión\n",
    "            nn.LeakyReLU(negative_slope=0.1),  # Activación LeakyReLU\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, embedding_size)  # Proyectar al tamaño del embedding\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class ItemTowerV2(nn.Module):\n",
    "    \"\"\"\n",
    "    Red neuronal para procesar características de los ítems sin Batch Normalization y con LeakyReLU.\n",
    "    \"\"\"\n",
    "    def __init__(self, item_input_size, embedding_size, dropout_rate):\n",
    "        super(ItemTowerV2, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(item_input_size, 256),  # Aumentar la dimensión inicial\n",
    "            nn.LeakyReLU(negative_slope=0.1),  # Activación LeakyReLU\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, 128),  # Segunda capa con menor dimensión\n",
    "            nn.LeakyReLU(negative_slope=0.1),  # Activación LeakyReLU\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, embedding_size)  # Proyectar al tamaño del embedding\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class TwoTowerModelV2(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo de dos torres actualizado sin Batch Normalization y con LeakyReLU.\n",
    "    \"\"\"\n",
    "    def __init__(self, user_input_size, item_input_size, embedding_size, dropout_rate):\n",
    "        super(TwoTowerModelV2, self).__init__()\n",
    "        self.user_tower = UserTowerV2(user_input_size, embedding_size, dropout_rate)\n",
    "        self.item_tower = ItemTowerV2(item_input_size, embedding_size, dropout_rate)\n",
    "\n",
    "    def forward(self, user_input, item_input):\n",
    "        # Embeddings del usuario y del ítem\n",
    "        user_embedding = self.user_tower(user_input)\n",
    "        item_embedding = self.item_tower(item_input)\n",
    "\n",
    "        # Afinidad mediante producto punto\n",
    "        score = torch.sum(user_embedding * item_embedding, dim=1)\n",
    "        return torch.sigmoid(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserTowerV3(nn.Module):\n",
    "    \"\"\"\n",
    "    Red neuronal para procesar características de los usuarios.\n",
    "    \"\"\"\n",
    "    def __init__(self, user_input_size, embedding_size, dropout_rate):\n",
    "        super(UserTowerV3, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(user_input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, embedding_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class ItemTowerV3(nn.Module):\n",
    "    \"\"\"\n",
    "    Red neuronal para procesar características de los ítems.\n",
    "    \"\"\"\n",
    "    def __init__(self, item_input_size, embedding_size, dropout_rate):\n",
    "        super(ItemTowerV3, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(item_input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, embedding_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class TwoTowerModelV3(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo de dos torres con afinidad basada en la distancia coseno.\n",
    "    \"\"\"\n",
    "    def __init__(self, user_input_size, item_input_size, embedding_size, dropout_rate):\n",
    "        super(TwoTowerModelV3, self).__init__()\n",
    "        self.user_tower = UserTowerV3(user_input_size, embedding_size, dropout_rate)\n",
    "        self.item_tower = ItemTowerV3(item_input_size, embedding_size, dropout_rate)\n",
    "        self.cosine_similarity = torch.nn.CosineSimilarity(dim=1)\n",
    "\n",
    "    def forward(self, user_input, item_input):\n",
    "        user_embedding = self.user_tower(user_input)\n",
    "        item_embedding = self.item_tower(item_input)\n",
    "        score = self.cosine_similarity(user_embedding, item_embedding)\n",
    "        return torch.sigmoid(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserTowerV4(nn.Module):\n",
    "    \"\"\"\n",
    "    Red neuronal para procesar características de los usuarios.\n",
    "    \"\"\"\n",
    "    def __init__(self, user_input_size, embedding_size, dropout_rate):\n",
    "        super(UserTowerV4, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(user_input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, embedding_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class ItemTowerV4(nn.Module):\n",
    "    \"\"\"\n",
    "    Red neuronal para procesar características de los ítems.\n",
    "    \"\"\"\n",
    "    def __init__(self, item_input_size, embedding_size, dropout_rate):\n",
    "        super(ItemTowerV4, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(item_input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, embedding_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class TwoTowerModelV4(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo de dos torres con embeddings concatenados y red adicional.\n",
    "    \"\"\"\n",
    "    def __init__(self, user_input_size, item_input_size, embedding_size, dropout_rate):\n",
    "        super(TwoTowerModelV4, self).__init__()\n",
    "        self.user_tower = UserTowerV4(user_input_size, embedding_size, dropout_rate)\n",
    "        self.item_tower = ItemTowerV4(item_input_size, embedding_size, dropout_rate)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embedding_size * 2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, user_input, item_input):\n",
    "        user_embedding = self.user_tower(user_input)\n",
    "        item_embedding = self.item_tower(item_input)\n",
    "        combined_embedding = torch.cat((user_embedding, item_embedding), dim=1)\n",
    "        score = self.fc(combined_embedding).squeeze()\n",
    "        return torch.sigmoid(score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SISTEMA DE RECOMENDACION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoTowerRecommenderSystem:\n",
    "    \"\"\"\n",
    "    Sistema de recomendación basado en el modelo de dos torres.\n",
    "    \"\"\"\n",
    "    def __init__(self, two_tower_model, user_input_size: int, item_input_size: int, embedding_size: int = 64, dropout_rate: float = 0.5, learning_rate: float = 0.001, optimizer_system=optim.Adam, criterion_system=nn.BCELoss):\n",
    "        \n",
    "        # Inicializar el modelo, optimizador y función de pérdida\n",
    "        self.model = two_tower_model(user_input_size, item_input_size, embedding_size, dropout_rate)\n",
    "        self.optimizer = optimizer_system(self.model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "        self.criterion = criterion_system()\n",
    "\n",
    "    def save_model(self, file_path: str):\n",
    "        # Guardar el estado del modelo\n",
    "        torch.save(self.model.state_dict(), file_path)\n",
    "        print(f\"[+] Modelo guardado en {file_path}\")\n",
    "\n",
    "    def load_model(self, file_path: str):\n",
    "        # Cargar el estado del modelo\n",
    "        self.model.load_state_dict(torch.load(file_path))\n",
    "        self.model.eval()\n",
    "        print(f\"[+] Modelo cargado desde {file_path}\")\n",
    "\n",
    "    def load_items(self, df_items: DataFrame):\n",
    "        \"\"\"\n",
    "        Cargar ítems desde un DataFrame y convertirlos en tensores.\n",
    "        - `df_items`: DataFrame de items con características (con ID).\n",
    "        \"\"\"\n",
    "        self.item_inputs = torch.tensor(df_items.iloc[:, 1:].values).float()\n",
    "        print(f\"[+] Ítems cargados: {self.item_inputs.size(0)} ítems con {self.item_inputs.size(1)} características cada uno.\")\n",
    "\n",
    "    def train(self, df_users: DataFrame, df_interactions: DataFrame, epochs: int = 30):\n",
    "        \"\"\"\n",
    "        Entrenar el modelo utilizando datos de usuarios e interacciones.\n",
    "        - `df_users`: DataFrame de usuarios con características (con ID).\n",
    "        - `df_interactions`: DataFrame de interacciones binarias usuario-ítem (con ID).\n",
    "        \"\"\"\n",
    "\n",
    "        if self.item_inputs is None:\n",
    "            raise ValueError(\"[-] Datos de los items no fueron cargados. Usa load_items() primero.\")\n",
    "\n",
    "        # Convertir datos de usuarios e interacciones a tensores\n",
    "        user_inputs = torch.tensor(df_users.iloc[:, 1:].values).float()\n",
    "        interactions = torch.tensor(df_interactions.iloc[:, 1:].values).float()\n",
    "\n",
    "        # Dimensiones de entrada\n",
    "        num_users = user_inputs.size(0) # Número de usuarios (n)\n",
    "        num_items = self.item_inputs.size(0) # Número de ítems (k)\n",
    "\n",
    "        # Expandir datos de usuario e ítem para todas las combinaciones usuario-ítem\n",
    "        user_input_expanded = user_inputs.unsqueeze(1).expand(-1, num_items, -1).reshape(-1, user_inputs.size(1)) # (n * k, m)\n",
    "        item_input_expanded = self.item_inputs.repeat(num_users, 1) # (n * k, h)\n",
    "        \n",
    "        # Aplanar las etiquetas de interacciones para todas las combinaciones usuario-ítem\n",
    "        labels = interactions.flatten() # Tensor de tamaño (n * k)\n",
    "        \n",
    "        # Verificar dimensiones\n",
    "        assert user_input_expanded.size(0) == item_input_expanded.size(0) == labels.size(0), \\\n",
    "            f\"[-] Dimensiones incompatibles: user_input_expanded={user_input_expanded.size(0)}, \" \\\n",
    "            f\"[-] item_input_expanded={item_input_expanded.size(0)}, labels={labels.size(0)}\"\n",
    "\n",
    "        # Proceso de entrenamiento\n",
    "        for epoch in range(epochs):\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(user_input_expanded, item_input_expanded)\n",
    "            loss = self.criterion(output, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            print(f\"[+] Epoch {epoch + 1}/{epochs} => Loss: {loss.item():.4f}\")    \n",
    "\n",
    "    def evaluate(self, df_users: DataFrame, df_interactions: DataFrame, k: int = 10):\n",
    "        \"\"\"\n",
    "        Evalúa el modelo con un nuevo conjunto de datos de usuarios e interacciones.\n",
    "\n",
    "        - `df_users`: DataFrame de usuarios con características (con ID).\n",
    "        - `df_interactions`: DataFrame de interacciones binarias usuario-ítem (con ID).\n",
    "        - `k`: Número de ítems a considerar para las métricas top-k.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convertir datos a tensores\n",
    "        user_inputs = torch.tensor(df_users.iloc[:, 1:].values).float()\n",
    "        interactions = torch.tensor(df_interactions.iloc[:, 1:].values).float()\n",
    "\n",
    "        # Dimensiones\n",
    "        num_users = user_inputs.size(0)\n",
    "        num_items = self.item_inputs.size(0)\n",
    "\n",
    "        # Expandir para todas las combinaciones usuario-ítem\n",
    "        user_input_expanded = user_inputs.unsqueeze(1).expand(-1, num_items, -1).reshape(-1, user_inputs.size(1))\n",
    "        item_input_expanded = self.item_inputs.repeat(num_users, 1)\n",
    "\n",
    "        # Etiquetas reales\n",
    "        labels = interactions\n",
    "\n",
    "        # Predicciones del modelo\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = self.model(user_input_expanded, item_input_expanded).reshape(num_users, num_items)\n",
    "\n",
    "        # Calcular métricas\n",
    "        precisions, recalls, ndcgs = [], [], []\n",
    "        for user_idx in range(num_users):\n",
    "            true_labels = labels[user_idx]\n",
    "            pred_scores = output[user_idx]\n",
    "\n",
    "            precisions.append(self.precision_at_k(true_labels, pred_scores, k))\n",
    "            recalls.append(self.recall_at_k(true_labels, pred_scores, k))\n",
    "            ndcgs.append(self.ndcg_at_k(true_labels, pred_scores, k))\n",
    "\n",
    "        # Promediar métricas\n",
    "        mean_precision = sum(precisions) / num_users\n",
    "        mean_recall = sum(recalls) / num_users\n",
    "        mean_ndcg = sum(ndcgs) / num_users\n",
    "\n",
    "        print(f\"[+] Evaluation Results - Precision@{k}: {mean_precision:.4f}, Recall@{k}: {mean_recall:.4f}, NDCG@{k}: {mean_ndcg:.4f}\")\n",
    "\n",
    "    def precision_at_k(self, true_labels, pred_scores, k):\n",
    "        \"\"\"\n",
    "        Calcula Precision@k para un usuario.\n",
    "        \"\"\"\n",
    "        _, top_k_indices = torch.topk(pred_scores, k)   # Índices de los top-k ítems predichos\n",
    "        top_k_pred = torch.zeros_like(true_labels)      # Inicializar predicciones binarias\n",
    "        top_k_pred[top_k_indices] = 1                   # Marcar los top-k como predichos\n",
    "\n",
    "        num_true_positives = torch.sum(top_k_pred * true_labels).item()  # Ítems relevantes en top-k\n",
    "        precision = num_true_positives / k                               # Precisión\n",
    "        return precision\n",
    "\n",
    "    def recall_at_k(self, true_labels, pred_scores, k):\n",
    "        \"\"\"\n",
    "        Calcula Recall@k para un usuario.\n",
    "        \"\"\"\n",
    "        _, top_k_indices = torch.topk(pred_scores, k)   # Índices de los top-k ítems predichos\n",
    "        top_k_pred = torch.zeros_like(true_labels)      # Inicializar predicciones binarias\n",
    "        top_k_pred[top_k_indices] = 1                   # Marcar los top-k como predichos\n",
    "\n",
    "        num_true_positives = torch.sum(top_k_pred * true_labels).item()  # Ítems relevantes en top-k\n",
    "        num_relevant_items = torch.sum(true_labels).item()               # Ítems relevantes reales\n",
    "        recall = num_true_positives / num_relevant_items if num_relevant_items > 0 else 0\n",
    "        return recall\n",
    "\n",
    "    def ndcg_at_k(self, true_labels, pred_scores, k):\n",
    "        \"\"\"\n",
    "        Calcula NDCG@k para un usuario.\n",
    "        \"\"\"\n",
    "        _, top_k_indices = torch.topk(pred_scores, k)                           # Índices de los top-k ítems predichos\n",
    "        ideal_sorted_labels = torch.sort(true_labels, descending=True)[0][:k]   # Relevancias ideales ordenadas\n",
    "\n",
    "        # DCG (Discounted Cumulative Gain)\n",
    "        dcg = torch.sum(true_labels[top_k_indices] / torch.log2(torch.arange(2, k + 2).float())).item()\n",
    "\n",
    "        # IDCG (Ideal Discounted Cumulative Gain)\n",
    "        ideal_dcg = torch.sum(ideal_sorted_labels / torch.log2(torch.arange(2, k + 2).float())).item()\n",
    "\n",
    "        ndcg = dcg / ideal_dcg if ideal_dcg > 0 else 0\n",
    "        return ndcg\n",
    "\n",
    "    def recommend(self, user_features, interacted_items, top_k: int = 10):\n",
    "        if not hasattr(self, 'item_inputs'):\n",
    "            raise ValueError(\"[-] Los ítems no han sido cargados. Usa load_items() primero.\")\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Generar embedding del usuario\n",
    "            user_embedding = self.model.user_tower(user_features.unsqueeze(0))  # (1, embedding_size)\n",
    "            # Generar embeddings de los ítems\n",
    "            item_embeddings = self.model.item_tower(self.item_inputs)  # (num_items, embedding_size)\n",
    "\n",
    "            # Calcular los scores\n",
    "            scores = torch.matmul(item_embeddings, user_embedding.squeeze().unsqueeze(1)).squeeze()\n",
    "\n",
    "            # Validar `interacted_items`\n",
    "            if not interacted_items:\n",
    "                print(\"[-] Warning: La lista interacted_items está vacía.\")\n",
    "            else:\n",
    "                for idx in interacted_items:\n",
    "                    if idx < 0 or idx >= len(scores):\n",
    "                        raise ValueError(f\"Índice fuera de rango: {idx}\")\n",
    "\n",
    "            # Penalizar ítems ya interactuados\n",
    "            for idx in interacted_items:\n",
    "                scores[idx] = float('-inf')  # Penalizar ítems interactuados con -inf\n",
    "\n",
    "            # # Validar los valores penalizados\n",
    "            # penalized_scores = [scores[idx].item() for idx in interacted_items]\n",
    "            # print(\"Valores penalizados en índices interactuados:\", penalized_scores)\n",
    "\n",
    "            # Obtener los top-k ítems\n",
    "            top_k_scores, top_k_indices = torch.topk(scores, top_k)\n",
    "\n",
    "            # Generar recomendaciones\n",
    "            recommendations = [\n",
    "                (idx, score)\n",
    "                for idx, score in zip(top_k_indices.tolist(), top_k_scores.tolist())\n",
    "            ]\n",
    "\n",
    "            # # Validar que los ítems recomendados no estén en los interactuados\n",
    "            # for idx, _ in recommendations:\n",
    "            #     if idx in interacted_items:\n",
    "            #         print(f\"Error: Ítem interactuado {idx} fue recomendado.\")\n",
    "\n",
    "            return recommendations\n",
    "\n",
    "    def evaluate_general_relevance(self, recommended_items, popularity):\n",
    "        \"\"\"\n",
    "        Evalúa la relevancia general de las recomendaciones.\n",
    "        - `recommended_items`: Diccionario {usuario: lista de ítems recomendados}.\n",
    "        - `popularity`: Diccionario {ítem: popularidad (frecuencia de interacciones)}.\n",
    "        \"\"\"\n",
    "        # Coverage\n",
    "        unique_items = set(item for items in recommended_items.values() for item in items)\n",
    "        coverage = len(unique_items) / self.item_inputs.shape[0]\n",
    "\n",
    "        # Calcular un valor mínimo positivo para ítems sin interacciones\n",
    "        min_popularity = 1 / (sum(popularity.values()) + 1)  # Evita división por cero\n",
    "\n",
    "        # Novelty\n",
    "        novelty = 0\n",
    "        total_recommendations = 0\n",
    "        for items in recommended_items.values():\n",
    "            for item in items:\n",
    "                # Obtener la popularidad del ítem, usando el mínimo si no tiene interacciones\n",
    "                item_popularity = max(popularity.get(item, 0), min_popularity)\n",
    "                novelty += -torch.log2(torch.tensor(item_popularity))\n",
    "                total_recommendations += 1\n",
    "        novelty /= total_recommendations\n",
    "\n",
    "        # Popularity Bias\n",
    "        avg_popularity = 0\n",
    "        for items in recommended_items.values():\n",
    "            avg_popularity += sum(popularity.get(item, 0) for item in items)\n",
    "        avg_popularity /= total_recommendations\n",
    "\n",
    "        # Imprimir métricas\n",
    "        print(f\"Coverage: {coverage:.4f}, Novelty: {novelty:.4f}, Popularity Bias: {avg_popularity:.4f}\")\n",
    "        return coverage, novelty, avg_popularity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el sistema recomendador \n",
    "recommender = TwoTowerRecommenderSystem(\n",
    "    two_tower_model=TwoTowerModel,\n",
    "    user_input_size=df_train_users.shape[1] - 1,\n",
    "    item_input_size=df_items.shape[1] - 1,\n",
    ")\n",
    "\n",
    "\n",
    "recommender1 = TwoTowerRecommenderSystem(\n",
    "    two_tower_model=TwoTowerModelV1,\n",
    "    user_input_size=df_train_users.shape[1] - 1,\n",
    "    item_input_size=df_items.shape[1] - 1,\n",
    ")\n",
    "\n",
    "recommender2 = TwoTowerRecommenderSystem(\n",
    "    two_tower_model=TwoTowerModelV2,\n",
    "    user_input_size=df_train_users.shape[1] - 1,\n",
    "    item_input_size=df_items.shape[1] - 1,\n",
    ")\n",
    "\n",
    "\n",
    "recommender3 = TwoTowerRecommenderSystem(\n",
    "    two_tower_model=TwoTowerModelV3,\n",
    "    user_input_size=df_train_users.shape[1] - 1,\n",
    "    item_input_size=df_items.shape[1] - 1,\n",
    ")\n",
    "\n",
    "\n",
    "recommender4 = TwoTowerRecommenderSystem(\n",
    "    two_tower_model=TwoTowerModelV4,\n",
    "    user_input_size=df_train_users.shape[1] - 1,\n",
    "    item_input_size=df_items.shape[1] - 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Ítems cargados: 53 ítems con 12 características cada uno.\n",
      "[+] Ítems cargados: 53 ítems con 12 características cada uno.\n",
      "[+] Ítems cargados: 53 ítems con 12 características cada uno.\n",
      "[+] Ítems cargados: 53 ítems con 12 características cada uno.\n",
      "[+] Ítems cargados: 53 ítems con 12 características cada uno.\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos en el sistema\n",
    "\n",
    "recommender.load_items(\n",
    "    df_items=df_items\n",
    ")\n",
    "\n",
    "recommender1.load_items(\n",
    "    df_items=df_items\n",
    ")\n",
    "\n",
    "recommender2.load_items(\n",
    "    df_items=df_items\n",
    ")\n",
    "\n",
    "recommender3.load_items(\n",
    "    df_items=df_items\n",
    ")\n",
    "\n",
    "recommender4.load_items(\n",
    "    df_items=df_items\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Epoch 1/30 => Loss: 0.7794\n",
      "[+] Epoch 2/30 => Loss: 0.6211\n",
      "[+] Epoch 3/30 => Loss: 0.5329\n",
      "[+] Epoch 4/30 => Loss: 0.4914\n",
      "[+] Epoch 5/30 => Loss: 0.4743\n",
      "[+] Epoch 6/30 => Loss: 0.4675\n",
      "[+] Epoch 7/30 => Loss: 0.4588\n",
      "[+] Epoch 8/30 => Loss: 0.4495\n",
      "[+] Epoch 9/30 => Loss: 0.4327\n",
      "[+] Epoch 10/30 => Loss: 0.4118\n",
      "[+] Epoch 11/30 => Loss: 0.3949\n",
      "[+] Epoch 12/30 => Loss: 0.3815\n",
      "[+] Epoch 13/30 => Loss: 0.3676\n",
      "[+] Epoch 14/30 => Loss: 0.3671\n",
      "[+] Epoch 15/30 => Loss: 0.3642\n",
      "[+] Epoch 16/30 => Loss: 0.3665\n",
      "[+] Epoch 17/30 => Loss: 0.3658\n",
      "[+] Epoch 18/30 => Loss: 0.3657\n",
      "[+] Epoch 19/30 => Loss: 0.3608\n",
      "[+] Epoch 20/30 => Loss: 0.3551\n",
      "[+] Epoch 21/30 => Loss: 0.3498\n",
      "[+] Epoch 22/30 => Loss: 0.3438\n",
      "[+] Epoch 23/30 => Loss: 0.3448\n",
      "[+] Epoch 24/30 => Loss: 0.3410\n",
      "[+] Epoch 25/30 => Loss: 0.3368\n",
      "[+] Epoch 26/30 => Loss: 0.3364\n",
      "[+] Epoch 27/30 => Loss: 0.3336\n",
      "[+] Epoch 28/30 => Loss: 0.3320\n",
      "[+] Epoch 29/30 => Loss: 0.3292\n",
      "[+] Epoch 30/30 => Loss: 0.3269\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo para recomendar\n",
    "recommender.train(\n",
    "    df_users=df_train_users,\n",
    "    df_interactions=df_train_interacciones,\n",
    "    epochs=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Epoch 1/30 => Loss: 0.6597\n",
      "[+] Epoch 2/30 => Loss: 0.5749\n",
      "[+] Epoch 3/30 => Loss: 0.5117\n",
      "[+] Epoch 4/30 => Loss: 0.4859\n",
      "[+] Epoch 5/30 => Loss: 0.4847\n",
      "[+] Epoch 6/30 => Loss: 0.4706\n",
      "[+] Epoch 7/30 => Loss: 0.4383\n",
      "[+] Epoch 8/30 => Loss: 0.4086\n",
      "[+] Epoch 9/30 => Loss: 0.3873\n",
      "[+] Epoch 10/30 => Loss: 0.3726\n",
      "[+] Epoch 11/30 => Loss: 0.3571\n",
      "[+] Epoch 12/30 => Loss: 0.3397\n",
      "[+] Epoch 13/30 => Loss: 0.3250\n",
      "[+] Epoch 14/30 => Loss: 0.3187\n",
      "[+] Epoch 15/30 => Loss: 0.3188\n",
      "[+] Epoch 16/30 => Loss: 0.3260\n",
      "[+] Epoch 17/30 => Loss: 0.3261\n",
      "[+] Epoch 18/30 => Loss: 0.3213\n",
      "[+] Epoch 19/30 => Loss: 0.3130\n",
      "[+] Epoch 20/30 => Loss: 0.3049\n",
      "[+] Epoch 21/30 => Loss: 0.2973\n",
      "[+] Epoch 22/30 => Loss: 0.2904\n",
      "[+] Epoch 23/30 => Loss: 0.2878\n",
      "[+] Epoch 24/30 => Loss: 0.2867\n",
      "[+] Epoch 25/30 => Loss: 0.2874\n",
      "[+] Epoch 26/30 => Loss: 0.2858\n",
      "[+] Epoch 27/30 => Loss: 0.2822\n",
      "[+] Epoch 28/30 => Loss: 0.2778\n",
      "[+] Epoch 29/30 => Loss: 0.2757\n",
      "[+] Epoch 30/30 => Loss: 0.2702\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo para recomendar\n",
    "recommender1.train(\n",
    "    df_users=df_train_users,\n",
    "    df_interactions=df_train_interacciones,\n",
    "    epochs=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Epoch 1/30 => Loss: 0.7115\n",
      "[+] Epoch 2/30 => Loss: 0.5949\n",
      "[+] Epoch 3/30 => Loss: 0.5245\n",
      "[+] Epoch 4/30 => Loss: 0.4982\n",
      "[+] Epoch 5/30 => Loss: 0.4995\n",
      "[+] Epoch 6/30 => Loss: 0.4922\n",
      "[+] Epoch 7/30 => Loss: 0.4655\n",
      "[+] Epoch 8/30 => Loss: 0.4382\n",
      "[+] Epoch 9/30 => Loss: 0.4185\n",
      "[+] Epoch 10/30 => Loss: 0.4066\n",
      "[+] Epoch 11/30 => Loss: 0.3939\n",
      "[+] Epoch 12/30 => Loss: 0.3776\n",
      "[+] Epoch 13/30 => Loss: 0.3602\n",
      "[+] Epoch 14/30 => Loss: 0.3474\n",
      "[+] Epoch 15/30 => Loss: 0.3441\n",
      "[+] Epoch 16/30 => Loss: 0.3465\n",
      "[+] Epoch 17/30 => Loss: 0.3489\n",
      "[+] Epoch 18/30 => Loss: 0.3473\n",
      "[+] Epoch 19/30 => Loss: 0.3417\n",
      "[+] Epoch 20/30 => Loss: 0.3379\n",
      "[+] Epoch 21/30 => Loss: 0.3290\n",
      "[+] Epoch 22/30 => Loss: 0.3221\n",
      "[+] Epoch 23/30 => Loss: 0.3144\n",
      "[+] Epoch 24/30 => Loss: 0.3112\n",
      "[+] Epoch 25/30 => Loss: 0.3103\n",
      "[+] Epoch 26/30 => Loss: 0.3084\n",
      "[+] Epoch 27/30 => Loss: 0.3082\n",
      "[+] Epoch 28/30 => Loss: 0.3046\n",
      "[+] Epoch 29/30 => Loss: 0.3045\n",
      "[+] Epoch 30/30 => Loss: 0.2999\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo para recomendar\n",
    "recommender2.train(\n",
    "    df_users=df_train_users,\n",
    "    df_interactions=df_train_interacciones,\n",
    "    epochs=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Epoch 1/30 => Loss: 0.6910\n",
      "[+] Epoch 2/30 => Loss: 0.6620\n",
      "[+] Epoch 3/30 => Loss: 0.6368\n",
      "[+] Epoch 4/30 => Loss: 0.6158\n",
      "[+] Epoch 5/30 => Loss: 0.5989\n",
      "[+] Epoch 6/30 => Loss: 0.5862\n",
      "[+] Epoch 7/30 => Loss: 0.5757\n",
      "[+] Epoch 8/30 => Loss: 0.5675\n",
      "[+] Epoch 9/30 => Loss: 0.5616\n",
      "[+] Epoch 10/30 => Loss: 0.5563\n",
      "[+] Epoch 11/30 => Loss: 0.5518\n",
      "[+] Epoch 12/30 => Loss: 0.5477\n",
      "[+] Epoch 13/30 => Loss: 0.5440\n",
      "[+] Epoch 14/30 => Loss: 0.5410\n",
      "[+] Epoch 15/30 => Loss: 0.5378\n",
      "[+] Epoch 16/30 => Loss: 0.5351\n",
      "[+] Epoch 17/30 => Loss: 0.5326\n",
      "[+] Epoch 18/30 => Loss: 0.5300\n",
      "[+] Epoch 19/30 => Loss: 0.5273\n",
      "[+] Epoch 20/30 => Loss: 0.5249\n",
      "[+] Epoch 21/30 => Loss: 0.5224\n",
      "[+] Epoch 22/30 => Loss: 0.5199\n",
      "[+] Epoch 23/30 => Loss: 0.5174\n",
      "[+] Epoch 24/30 => Loss: 0.5149\n",
      "[+] Epoch 25/30 => Loss: 0.5122\n",
      "[+] Epoch 26/30 => Loss: 0.5095\n",
      "[+] Epoch 27/30 => Loss: 0.5062\n",
      "[+] Epoch 28/30 => Loss: 0.5040\n",
      "[+] Epoch 29/30 => Loss: 0.5012\n",
      "[+] Epoch 30/30 => Loss: 0.4980\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo para recomendar\n",
    "recommender3.train(\n",
    "    df_users=df_train_users,\n",
    "    df_interactions=df_train_interacciones,\n",
    "    epochs=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Epoch 1/30 => Loss: 0.6617\n",
      "[+] Epoch 2/30 => Loss: 0.6367\n",
      "[+] Epoch 3/30 => Loss: 0.6125\n",
      "[+] Epoch 4/30 => Loss: 0.5905\n",
      "[+] Epoch 5/30 => Loss: 0.5710\n",
      "[+] Epoch 6/30 => Loss: 0.5534\n",
      "[+] Epoch 7/30 => Loss: 0.5387\n",
      "[+] Epoch 8/30 => Loss: 0.5278\n",
      "[+] Epoch 9/30 => Loss: 0.5205\n",
      "[+] Epoch 10/30 => Loss: 0.5171\n",
      "[+] Epoch 11/30 => Loss: 0.5151\n",
      "[+] Epoch 12/30 => Loss: 0.5148\n",
      "[+] Epoch 13/30 => Loss: 0.5129\n",
      "[+] Epoch 14/30 => Loss: 0.5095\n",
      "[+] Epoch 15/30 => Loss: 0.5030\n",
      "[+] Epoch 16/30 => Loss: 0.4955\n",
      "[+] Epoch 17/30 => Loss: 0.4843\n",
      "[+] Epoch 18/30 => Loss: 0.4757\n",
      "[+] Epoch 19/30 => Loss: 0.4634\n",
      "[+] Epoch 20/30 => Loss: 0.4539\n",
      "[+] Epoch 21/30 => Loss: 0.4441\n",
      "[+] Epoch 22/30 => Loss: 0.4358\n",
      "[+] Epoch 23/30 => Loss: 0.4265\n",
      "[+] Epoch 24/30 => Loss: 0.4158\n",
      "[+] Epoch 25/30 => Loss: 0.4054\n",
      "[+] Epoch 26/30 => Loss: 0.3949\n",
      "[+] Epoch 27/30 => Loss: 0.3857\n",
      "[+] Epoch 28/30 => Loss: 0.3753\n",
      "[+] Epoch 29/30 => Loss: 0.3660\n",
      "[+] Epoch 30/30 => Loss: 0.3584\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo para recomendar\n",
    "recommender4.train(\n",
    "    df_users=df_train_users,\n",
    "    df_interactions=df_train_interacciones,\n",
    "    epochs=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Evaluation Results - Precision@5: 0.7646, Recall@5: 0.3885, NDCG@5: 0.7925\n",
      "[+] Evaluation Results - Precision@10: 0.7309, Recall@10: 0.6960, NDCG@10: 0.8207\n",
      "[+] Evaluation Results - Precision@15: 0.6316, Recall@15: 0.8520, NDCG@15: 0.8340\n",
      "[+] Evaluation Results - Precision@20: 0.5554, Recall@20: 0.9712, NDCG@20: 0.8677\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo recomendacion\n",
    "recommender.evaluate(\n",
    "    df_users=df_validation_users,\n",
    "    df_interactions=df_validation_interacciones,\n",
    "    k=5\n",
    ")\n",
    "\n",
    "# Evaluar el modelo recomendacion\n",
    "recommender.evaluate(\n",
    "    df_users=df_validation_users,\n",
    "    df_interactions=df_validation_interacciones,\n",
    "    k=10\n",
    ")\n",
    "\n",
    "# Evaluar el modelo recomendacion\n",
    "recommender.evaluate(\n",
    "    df_users=df_validation_users,\n",
    "    df_interactions=df_validation_interacciones,\n",
    "    k=15\n",
    ")\n",
    "\n",
    "# Evaluar el modelo recomendacion\n",
    "recommender.evaluate(\n",
    "    df_users=df_validation_users,\n",
    "    df_interactions=df_validation_interacciones,\n",
    "    k=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Evaluation Results - Precision@5: 0.7623, Recall@5: 0.3837, NDCG@5: 0.7889\n",
      "[+] Evaluation Results - Precision@10: 0.7360, Recall@10: 0.6996, NDCG@10: 0.8433\n",
      "[+] Evaluation Results - Precision@15: 0.6316, Recall@15: 0.8520, NDCG@15: 0.8562\n",
      "[+] Evaluation Results - Precision@20: 0.5557, Recall@20: 0.9714, NDCG@20: 0.8924\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo recomendacion\n",
    "recommender1.evaluate(\n",
    "    df_users=df_validation_users,\n",
    "    df_interactions=df_validation_interacciones,\n",
    "    k=5\n",
    ")\n",
    "\n",
    "# Evaluar el modelo recomendacion\n",
    "recommender1.evaluate(\n",
    "    df_users=df_validation_users,\n",
    "    df_interactions=df_validation_interacciones,\n",
    "    k=10\n",
    ")\n",
    "\n",
    "# Evaluar el modelo recomendacion\n",
    "recommender1.evaluate(\n",
    "    df_users=df_validation_users,\n",
    "    df_interactions=df_validation_interacciones,\n",
    "    k=15\n",
    ")\n",
    "\n",
    "# Evaluar el modelo recomendacion\n",
    "recommender1.evaluate(\n",
    "    df_users=df_validation_users,\n",
    "    df_interactions=df_validation_interacciones,\n",
    "    k=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Evaluation Results - Precision@5: 0.7474, Recall@5: 0.3791, NDCG@5: 0.7746\n",
      "[+] Evaluation Results - Precision@10: 0.7371, Recall@10: 0.7004, NDCG@10: 0.8433\n",
      "[+] Evaluation Results - Precision@15: 0.6316, Recall@15: 0.8520, NDCG@15: 0.8559\n",
      "[+] Evaluation Results - Precision@20: 0.5554, Recall@20: 0.9712, NDCG@20: 0.8911\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo recomendacion\n",
    "recommender2.evaluate(\n",
    "    df_users=df_validation_users,\n",
    "    df_interactions=df_validation_interacciones,\n",
    "    k=5\n",
    ")\n",
    "\n",
    "# Evaluar el modelo recomendacion\n",
    "recommender2.evaluate(\n",
    "    df_users=df_validation_users,\n",
    "    df_interactions=df_validation_interacciones,\n",
    "    k=10\n",
    ")\n",
    "\n",
    "# Evaluar el modelo recomendacion\n",
    "recommender2.evaluate(\n",
    "    df_users=df_validation_users,\n",
    "    df_interactions=df_validation_interacciones,\n",
    "    k=15\n",
    ")\n",
    "\n",
    "# Evaluar el modelo recomendacion\n",
    "recommender2.evaluate(\n",
    "    df_users=df_validation_users,\n",
    "    df_interactions=df_validation_interacciones,\n",
    "    k=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Evaluation Results - Precision@5: 0.7394, Recall@5: 0.3650, NDCG@5: 0.7562\n",
      "[+] Evaluation Results - Precision@10: 0.6651, Recall@10: 0.6065, NDCG@10: 0.7834\n",
      "[+] Evaluation Results - Precision@15: 0.6316, Recall@15: 0.8520, NDCG@15: 0.8073\n",
      "[+] Evaluation Results - Precision@20: 0.5500, Recall@20: 0.9643, NDCG@20: 0.8566\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo recomendacion\n",
    "recommender3.evaluate(\n",
    "    df_users=df_validation_users,\n",
    "    df_interactions=df_validation_interacciones,\n",
    "    k=5\n",
    ")\n",
    "\n",
    "# Evaluar el modelo recomendacion\n",
    "recommender3.evaluate(\n",
    "    df_users=df_validation_users,\n",
    "    df_interactions=df_validation_interacciones,\n",
    "    k=10\n",
    ")\n",
    "\n",
    "# Evaluar el modelo recomendacion\n",
    "recommender3.evaluate(\n",
    "    df_users=df_validation_users,\n",
    "    df_interactions=df_validation_interacciones,\n",
    "    k=15\n",
    ")\n",
    "\n",
    "# Evaluar el modelo recomendacion\n",
    "recommender3.evaluate(\n",
    "    df_users=df_validation_users,\n",
    "    df_interactions=df_validation_interacciones,\n",
    "    k=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Evaluation Results - Precision@5: 0.6640, Recall@5: 0.3229, NDCG@5: 0.7159\n",
      "[+] Evaluation Results - Precision@10: 0.6457, Recall@10: 0.5824, NDCG@10: 0.7637\n",
      "[+] Evaluation Results - Precision@15: 0.6316, Recall@15: 0.8520, NDCG@15: 0.7992\n",
      "[+] Evaluation Results - Precision@20: 0.5403, Recall@20: 0.9519, NDCG@20: 0.8422\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo recomendacion\n",
    "recommender4.evaluate(\n",
    "    df_users=df_validation_users,\n",
    "    df_interactions=df_validation_interacciones,\n",
    "    k=5\n",
    ")\n",
    "\n",
    "# Evaluar el modelo recomendacion\n",
    "recommender4.evaluate(\n",
    "    df_users=df_validation_users,\n",
    "    df_interactions=df_validation_interacciones,\n",
    "    k=10\n",
    ")\n",
    "\n",
    "# Evaluar el modelo recomendacion\n",
    "recommender4.evaluate(\n",
    "    df_users=df_validation_users,\n",
    "    df_interactions=df_validation_interacciones,\n",
    "    k=15\n",
    ")\n",
    "\n",
    "# Evaluar el modelo recomendacion\n",
    "recommender4.evaluate(\n",
    "    df_users=df_validation_users,\n",
    "    df_interactions=df_validation_interacciones,\n",
    "    k=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario para almacenar las recomendaciones\n",
    "recommended_items = {}\n",
    "recommended_items1 = {}\n",
    "recommended_items2 = {}\n",
    "recommended_items3 = {}\n",
    "recommended_items4 = {}\n",
    "\n",
    "# Iterar sobre todos los usuarios en el conjunto de prueba\n",
    "for user_id in df_test_users['id_estudiante']:\n",
    "    # Obtener las características del usuario actual\n",
    "    user_features = torch.tensor(df_test_users[df_test_users['id_estudiante'] == user_id].iloc[:, 1:].values).float()\n",
    "\n",
    "    # Obtener los ítems ya interactuados por el usuario actual\n",
    "    interacted_items = df_test_interacciones[df_test_interacciones['id_estudiante'] == user_id].iloc[:, 1:].values.flatten()\n",
    "    interacted_indices = [i for i, interaction in enumerate(interacted_items) if interaction == 1]\n",
    "\n",
    "    # Generar recomendaciones para el usuario actual\n",
    "    recommendations = recommender.recommend(user_features, interacted_indices, top_k=15)\n",
    "    recommendations1 = recommender1.recommend(user_features, interacted_indices, top_k=15)\n",
    "    recommendations2 = recommender2.recommend(user_features, interacted_indices, top_k=15)\n",
    "    recommendations3 = recommender3.recommend(user_features, interacted_indices, top_k=15)\n",
    "    recommendations4 = recommender4.recommend(user_features, interacted_indices, top_k=15)\n",
    "\n",
    "    # Almacenar solo los índices de los ítems recomendados (sin los scores)\n",
    "    recommended_items[user_id] = [rec[0] for rec in recommendations]\n",
    "    recommended_items1[user_id] = [rec[0] for rec in recommendations1]\n",
    "    recommended_items2[user_id] = [rec[0] for rec in recommendations2]\n",
    "    recommended_items3[user_id] = [rec[0] for rec in recommendations3]\n",
    "    recommended_items4[user_id] = [rec[0] for rec in recommendations4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage: 0.8113, Novelty: -2.4303, Popularity Bias: 220.6578\n"
     ]
    }
   ],
   "source": [
    "coverage, novelty, popularity_bias = recommender.evaluate_general_relevance(\n",
    "    recommended_items=recommended_items,\n",
    "    popularity=popularidad_items\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage: 0.8113, Novelty: -4.8450, Popularity Bias: 227.2877\n"
     ]
    }
   ],
   "source": [
    "coverage1, novelty1, popularity_bias1 = recommender1.evaluate_general_relevance(\n",
    "    recommended_items=recommended_items1,\n",
    "    popularity=popularidad_items\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage: 0.8113, Novelty: -3.9376, Popularity Bias: 221.6374\n"
     ]
    }
   ],
   "source": [
    "coverage2, novelty2, popularity_bias2 = recommender2.evaluate_general_relevance(\n",
    "    recommended_items=recommended_items2,\n",
    "    popularity=popularidad_items\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage: 0.7547, Novelty: 1.7276, Popularity Bias: 200.2956\n"
     ]
    }
   ],
   "source": [
    "coverage3, novelty3, popularity_bias3 = recommender3.evaluate_general_relevance(\n",
    "    recommended_items=recommended_items3,\n",
    "    popularity=popularidad_items\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage: 0.8113, Novelty: -4.8450, Popularity Bias: 227.2877\n"
     ]
    }
   ],
   "source": [
    "coverage4, novelty4, popularity_bias4 = recommender4.evaluate_general_relevance(\n",
    "    recommended_items=recommended_items1,\n",
    "    popularity=popularidad_items\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 7, 10, 13, 17, 18, 22, 23, 25, 32, 35, 42]\n",
      "[(26, 0.3007104992866516), (29, 0.30071043968200684), (0, -0.9705798625946045), (1, -1.441230058670044), (6, -1.7118687629699707), (44, -1.7118687629699707), (50, -2.1610219478607178), (16, -3.2638938426971436), (52, -3.2638940811157227), (27, -3.335946559906006), (28, -3.6505961418151855), (38, -3.6505961418151855), (21, -3.6505961418151855), (31, -3.6505961418151855), (37, -3.6505961418151855)]\n",
      "[26, 29, 0, 1, 6, 44, 50, 16, 52, 27, 28, 38, 21, 31, 37]\n"
     ]
    }
   ],
   "source": [
    "# Supongamos que el usuario con ID=123 tiene las siguientes características:\n",
    "user_id = 1023\n",
    "user_features = torch.tensor(df_test_users[df_test_users['id_estudiante'] == user_id].iloc[:, 1:].values).float()\n",
    "\n",
    "# Ítems ya interactuados por el usuario\n",
    "interacted_items = df_test_interacciones[df_test_interacciones['id_estudiante'] == user_id].iloc[:, 1:].values.flatten()\n",
    "interacted_indices = [i for i, interaction in enumerate(interacted_items) if interaction == 1]\n",
    "print(interacted_indices)\n",
    "\n",
    "# Generar recomendaciones\n",
    "reomendacions = recommender.recommend(user_features, interacted_indices, top_k=15)\n",
    "\n",
    "print(reomendacions)\n",
    "print([x[0] for x in reomendacions])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".sistema-de-recomendacion-unab-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
