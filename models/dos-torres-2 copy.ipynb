{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id_ejercicio', 'nombre', 'h1', 'h2', 'h3', 'h4', 's1', 's2', 's3',\n",
      "       's4', 'k1', 'k2', 'k3', 'k4', 'hito', 'skill', 'knowledge',\n",
      "       'complexity', 'complexity12', 'puntos', 'enunciado', 'dificultad',\n",
      "       'score_a', 'score_d', 'score_p', 'score_s'],\n",
      "      dtype='object')\n",
      "Index(['id_estudiante', 'programa', 'exitosos', 'fallidos', 'solemne_1',\n",
      "       'solemne_2', 'solemne_3', 'solemne_4', 'score_a', 'score_p', 'score_d',\n",
      "       'score_s', 'e0', 'e1', 'e2', 'e3', 'e4', 'e5', 'e6', 'e7', 'e8', 'e9',\n",
      "       'e10', 'e11', 'e12', 'e13', 'e14', 'e15', 'e16', 'e17', 'e18', 'e19',\n",
      "       'e20', 'e21', 'e22', 'e23', 'e24', 'e25', 'e26', 'e27', 'e28', 'e29',\n",
      "       'e30', 'e31', 'e32', 'e33', 'e34', 'e35', 'e36', 'e37', 'e38', 'e39',\n",
      "       'e40', 'e41', 'e42', 'e43', 'e44', 'e45', 'e46', 'e47', 'e48', 'e49',\n",
      "       'e50', 'e51', 'e52'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "import warnings\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pandas import DataFrame\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "VAR_SEED = 42\n",
    "VAR_TESTSET_SIZE = 0.20\n",
    "VAR_DIR_DATA_CLEANING = '../data/cleaning'\n",
    "\n",
    "random.seed(VAR_SEED)\n",
    "np.random.seed(VAR_SEED)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "EJERCICIOS = pd.read_csv(f\"{VAR_DIR_DATA_CLEANING}/ejercicios.csv\", encoding=\"latin1\")\n",
    "ESTUDIANTES = pd.read_csv(f\"{VAR_DIR_DATA_CLEANING}/estudiantes.csv\", encoding=\"latin1\")\n",
    "\n",
    "print(EJERCICIOS.columns)\n",
    "print(ESTUDIANTES.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items = EJERCICIOS[['id_ejercicio','h1', 'h2', 'h3', 'h4', 's1', 's2', 's3', 's4', 'k1', 'k2', 'k3', 'k4']] \n",
    "df_users = ESTUDIANTES.copy()\n",
    "df_users.head()\n",
    "\n",
    "# División inicial en train y test\n",
    "train_data, test_data = train_test_split(df_users, test_size=VAR_TESTSET_SIZE, random_state=VAR_SEED)\n",
    "# División adicional en train y validation\n",
    "train_data, validation_data = train_test_split(train_data, test_size=VAR_TESTSET_SIZE, random_state=VAR_SEED)\n",
    "\n",
    "# Crear escalador y codificador\n",
    "scaler = MinMaxScaler()\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Columnas a normalizar y codificar\n",
    "columns_to_normalize = ['exitosos', 'fallidos', 'solemne_1', 'solemne_2', 'solemne_3', 'solemne_4', 'score_a', 'score_p', 'score_d', 'score_s']\n",
    "column_to_encode = 'programa'\n",
    "\n",
    "# Ajustar en el conjunto de entrenamiento\n",
    "scaler.fit(train_data[columns_to_normalize])        # Ajustar el escalador\n",
    "label_encoder.fit(train_data[column_to_encode])     # Ajustar el codificador\n",
    "\n",
    "# Entrenamiento\n",
    "train_data[columns_to_normalize] = scaler.transform(train_data[columns_to_normalize])\n",
    "train_data[column_to_encode] = label_encoder.transform(train_data[column_to_encode])\n",
    "\n",
    "# Validación\n",
    "validation_data[columns_to_normalize] = scaler.transform(validation_data[columns_to_normalize])\n",
    "validation_data[column_to_encode] = label_encoder.transform(validation_data[column_to_encode])\n",
    "\n",
    "# Prueba\n",
    "test_data[columns_to_normalize] = scaler.transform(test_data[columns_to_normalize])\n",
    "test_data[column_to_encode] = label_encoder.transform(test_data[column_to_encode])\n",
    "\n",
    "features_users_data = ['id_estudiante', 'programa', 'exitosos', 'fallidos', 'score_a', 'score_p', 'score_d', 'score_s']\n",
    "features_users_inte = ['id_estudiante'] + [f\"e{i}\" for i in range(len(df_items))]\n",
    "\n",
    "df_test_users = test_data[features_users_data]\n",
    "df_train_users = train_data[features_users_data]\n",
    "df_validation_users = validation_data[features_users_data]\n",
    "\n",
    "df_test_interacciones = test_data[features_users_inte]\n",
    "df_train_interacciones = train_data[features_users_inte]\n",
    "df_validation_interacciones = validation_data[features_users_inte]\n",
    "\n",
    "\n",
    "# # Filtrar estudiantes aprobados según las condiciones\n",
    "estudiantes_aprovados = ESTUDIANTES[~ESTUDIANTES['id_estudiante'].isin(\n",
    "    ESTUDIANTES.query(\"`solemne_1` < 4.0 and `solemne_2` < 4.0 and `solemne_3` < 4.0 and `solemne_4` < 4.0\")['id_estudiante']\n",
    ")]\n",
    "\n",
    "# Total de ítems\n",
    "n = len(EJERCICIOS)\n",
    "\n",
    "# Últimas 'n' columnas\n",
    "columnas = ESTUDIANTES.columns[-n:]\n",
    "\n",
    "# Limpiar nombres de columnas eliminando la 'e' al inicio\n",
    "columnas_limpias = columnas.str.replace('e', '').astype(int)\n",
    "\n",
    "# Renombrar las columnas temporalmente para evitar problemas\n",
    "estudiantes_aprovados.columns = list(ESTUDIANTES.columns[:-n]) + columnas_limpias.tolist()\n",
    "\n",
    "# Calcular la suma para cada ítem y convertir a diccionario\n",
    "popularidad_items = estudiantes_aprovados.iloc[:, -n:].sum(axis=0).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoTowerRecommenderSystem:\n",
    "\n",
    "    def __init__(self, Two_Tower_Model, User_Tower, Item_Tower, \n",
    "                 user_input_size: int, item_input_size: int,\n",
    "                 embedding_size: int = 64, dropout_rate: float = 0.5, learning_rate: float = 0.001, weight_decay: float = 1e-4, \n",
    "                 optimizer_system=optim.Adam, criterion_system=nn.BCELoss):\n",
    "        \n",
    "        self.model = Two_Tower_Model(user_input_size, item_input_size, embedding_size, dropout_rate, User_Tower, Item_Tower)\n",
    "        self.optimizer = optimizer_system(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        self.criterion = criterion_system()\n",
    "\n",
    "\n",
    "    def save_model(self, file_path: str) -> None:\n",
    "        torch.save(self.model.state_dict(), file_path)\n",
    "        print(f\"[+] Modelo guardado en {file_path}\")\n",
    "\n",
    "\n",
    "    def load_model(self, file_path: str) -> None:\n",
    "        self.model.load_state_dict(torch.load(file_path))\n",
    "        self.model.eval()\n",
    "        print(f\"[+] Modelo cargado desde {file_path}\")\n",
    "\n",
    "\n",
    "    def load_data_train(self, data_items: DataFrame, data_users: DataFrame, data_interactions: DataFrame) -> None:\n",
    "        self.items_inputs = torch.tensor(data_items.iloc[:, 1:].values).float()\n",
    "        self.users_inputs = torch.tensor(data_users.iloc[:, 1:].values).float()\n",
    "        self.interactions_inputs = torch.tensor(data_interactions.iloc[:, 1:].values).float()\n",
    "        print(f\"[+] Ítems cargados: {self.items_inputs.size(0)} ítems con {self.items_inputs.size(1)} características cada uno.\")\n",
    "        print(f\"[+] Users cargados: {self.users_inputs.size(0)} users con {self.users_inputs.size(1)} características cada uno.\")\n",
    "        print(f\"[+] Interactions cargados: {self.interactions_inputs.size(0)} interactions con {self.interactions_inputs.size(1)} características cada uno.\")\n",
    "\n",
    "\n",
    "    def train(self, epochs: int = 30) -> None:\n",
    "        if self.items_inputs is None or self.users_inputs is None or self.interactions_inputs is None:\n",
    "            raise ValueError(\"[-] No se cargaron datos de entrenamiento. Usa load_data_train() primero.\")\n",
    "        # Dimensiones de entrada\n",
    "        num_users = self.users_inputs.size(0)           # Número de usuarios (n)\n",
    "        num_items = self.items_inputs.size(0)           # Número de ítems (k)\n",
    "        # Expandir datos de usuario e ítem para todas las combinaciones usuario-ítem\n",
    "        user_input_expanded = self.users_inputs.unsqueeze(1).expand(-1, num_items, -1).reshape(-1, self.users_inputs.size(1))   # (n * k, m)\n",
    "        item_input_expanded = self.items_inputs.repeat(num_users, 1)                                                            # (n * k, h)\n",
    "        # Aplanar las etiquetas de interacciones para todas las combinaciones usuario-ítem\n",
    "        labels = self.interactions_inputs.flatten()                                                                             # Tensor de tamaño (n * k)\n",
    "        # Verificar dimensiones\n",
    "        assert user_input_expanded.size(0) == item_input_expanded.size(0) == labels.size(0), \\\n",
    "            f\"[-] Dimensiones incompatibles: user_input_expanded={user_input_expanded.size(0)}, \" \\\n",
    "            f\"[-] item_input_expanded={item_input_expanded.size(0)}, labels={labels.size(0)}\"\n",
    "        # Proceso de entrenamiento\n",
    "        for epoch in range(epochs):\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(user_input_expanded, item_input_expanded)\n",
    "            loss = self.criterion(output, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            print(f\"[+] Epoch {epoch + 1}/{epochs} => Loss: {loss.item():.4f}\")    \n",
    "\n",
    "\n",
    "    def precision_at_k(self, true_labels, pred_scores, k):\n",
    "        _, top_k_indices = torch.topk(pred_scores, k)                       # Índices de los top-k ítems predichos\n",
    "        top_k_pred = torch.zeros_like(true_labels)                          # Inicializar predicciones binarias\n",
    "        top_k_pred[top_k_indices] = 1                                       # Marcar los top-k como predichos\n",
    "        num_true_positives = torch.sum(top_k_pred * true_labels).item()     # Ítems relevantes en top-k\n",
    "        precision = num_true_positives / k                                  # Precisión\n",
    "        return precision\n",
    "\n",
    "\n",
    "    def recall_at_k(self, true_labels, pred_scores, k):\n",
    "        _, top_k_indices = torch.topk(pred_scores, k)                       # Índices de los top-k ítems predichos\n",
    "        top_k_pred = torch.zeros_like(true_labels)                          # Inicializar predicciones binarias\n",
    "        top_k_pred[top_k_indices] = 1                                       # Marcar los top-k como predichos\n",
    "        num_true_positives = torch.sum(top_k_pred * true_labels).item()     # Ítems relevantes en top-k\n",
    "        num_relevant_items = torch.sum(true_labels).item()                  # Ítems relevantes reales\n",
    "        recall = num_true_positives / num_relevant_items if num_relevant_items > 0 else 0\n",
    "        return recall\n",
    "\n",
    "\n",
    "    def ndcg_at_k(self, true_labels, pred_scores, k):\n",
    "        _, top_k_indices = torch.topk(pred_scores, k)                                                       # Índices de los top-k ítems predichos\n",
    "        ideal_sorted_labels = torch.sort(true_labels, descending=True)[0][:k]                               # Relevancias ideales ordenadas        \n",
    "        dcg = torch.sum(true_labels[top_k_indices] / torch.log2(torch.arange(2, k + 2).float())).item()     # DCG (Discounted Cumulative Gain)\n",
    "        ideal_dcg = torch.sum(ideal_sorted_labels / torch.log2(torch.arange(2, k + 2).float())).item()      # IDCG (Ideal Discounted Cumulative Gain)\n",
    "        ndcg = dcg / ideal_dcg if ideal_dcg > 0 else 0\n",
    "        return ndcg\n",
    "\n",
    "\n",
    "    def evaluate(self, val_users: DataFrame, val_interactions: DataFrame, k: int = 10) -> None:\n",
    "        # Convertir datos a tensores\n",
    "        user_inputs = torch.tensor(val_users.iloc[:, 1:].values).float()\n",
    "        interactions = torch.tensor(val_interactions.iloc[:, 1:].values).float()\n",
    "        \n",
    "        # Dimensiones\n",
    "        num_items = self.items_inputs.size(0)\n",
    "        num_users = user_inputs.size(0)\n",
    "        \n",
    "        # Expandir para todas las combinaciones usuario-ítem\n",
    "        user_input_expanded = user_inputs.unsqueeze(1).expand(-1, num_items, -1).reshape(-1, user_inputs.size(1))\n",
    "        item_input_expanded = self.items_inputs.repeat(num_users, 1)\n",
    "        \n",
    "        # Etiquetas reales\n",
    "        labels = interactions\n",
    "        \n",
    "        # Predicciones del modelo\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = self.model(user_input_expanded, item_input_expanded).reshape(num_users, num_items)\n",
    "\n",
    "        # Calcular métricas\n",
    "        precisions, recalls, ndcgs = [], [], []\n",
    "        for user_idx in range(num_users):\n",
    "            true_labels = labels[user_idx]\n",
    "            pred_scores = output[user_idx]\n",
    "\n",
    "            precisions.append(self.precision_at_k(true_labels, pred_scores, k))\n",
    "            recalls.append(self.recall_at_k(true_labels, pred_scores, k))\n",
    "            ndcgs.append(self.ndcg_at_k(true_labels, pred_scores, k))\n",
    "\n",
    "        # Promediar métricas\n",
    "        mean_precision = sum(precisions) / num_users\n",
    "        mean_recall = sum(recalls) / num_users\n",
    "        mean_ndcg = sum(ndcgs) / num_users\n",
    "        print(f\"[+] Evaluation Results => Precision@{k}: {mean_precision:.4f}, Recall@{k}: {mean_recall:.4f}, NDCG@{k}: {mean_ndcg:.4f}\")\n",
    "\n",
    "\n",
    "    def recommend(self, user_features: DataFrame, interacted_items: list[int], top_k: int = 10) -> list[tuple]:\n",
    "        if self.items_inputs is None:\n",
    "            raise ValueError(\"[-] No se cargaron datos de entrenamiento. Usa load_data_train() primero.\")\n",
    "\n",
    "        user_features = torch.tensor(user_features.iloc[:, 1:].values).float()\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            user_embedding = self.model.user_tower(user_features.unsqueeze(0))                      # Generar embedding del usuario     # (1, embedding_size)\n",
    "            item_embeddings = self.model.item_tower(self.items_inputs)                              # Generar embeddings de los ítems   # (num_items, embedding_size)\n",
    "            scores = torch.matmul(item_embeddings, user_embedding.squeeze().unsqueeze(1)).squeeze() # Calcular los scores\n",
    "            \n",
    "            if not interacted_items: # Validar `interacted_items`\n",
    "                print(\"[-] Warning: La lista interacted_items está vacía.\")\n",
    "            else:\n",
    "                for idx in interacted_items:\n",
    "                    if idx < 0 or idx >= len(scores):\n",
    "                        raise ValueError(f\"Índice fuera de rango: {idx}\")\n",
    "\n",
    "            for idx in interacted_items:                                                                                # Penalizar ítems ya interactuados\n",
    "                scores[idx] = float('-inf')                                                                             # Penalizar ítems interactuados con -inf\n",
    "\n",
    "            top_k_scores, top_k_indices = torch.topk(scores, top_k)                                                     # Obtener los top-k ítems\n",
    "            recommendations = [ (idx, score) for idx, score in zip(top_k_indices.tolist(), top_k_scores.tolist()) ]     # Generar recomendaciones\n",
    "            return recommendations\n",
    "\n",
    "\n",
    "    def evaluate_relevance(self, recommended_items: dict[int, list[int]], popularity_bias: dict[int, int]) -> None:\n",
    "        # Coverage\n",
    "        unique_items = set(item for items in recommended_items.values() for item in items)\n",
    "        coverage = len(unique_items) / self.items_inputs.shape[0]\n",
    "\n",
    "        min_popularity = 1 / (sum(popularity_bias.values()) + 1)  # Calcular un valor mínimo positivo para ítems sin interacciones (Evita división por cero)\n",
    "\n",
    "        # Novelty\n",
    "        novelty = 0\n",
    "        total_recommendations = 0\n",
    "        for items in recommended_items.values():\n",
    "            for item in items:  # Obtener la popularidad del ítem, usando el mínimo si no tiene interacciones\n",
    "                item_popularity = max(popularity_bias.get(item, 0), min_popularity)\n",
    "                novelty += -torch.log2(torch.tensor(item_popularity))\n",
    "                total_recommendations += 1\n",
    "        novelty /= total_recommendations\n",
    "\n",
    "        # Popularity Bias\n",
    "        avg_popularity = 0\n",
    "        for items in recommended_items.values():\n",
    "            avg_popularity += sum(popularity_bias.get(item, 0) for item in items)\n",
    "        avg_popularity /= total_recommendations\n",
    "\n",
    "        # Imprimir métricas\n",
    "        print(f\"Coverage: {coverage:.4f}, Novelty: {novelty:.4f}, Popularity Bias: {avg_popularity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.config_torres import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Ítems cargados: 53 ítems con 12 características cada uno.\n",
      "[+] Users cargados: 698 users con 7 características cada uno.\n",
      "[+] Interactions cargados: 698 interactions con 53 características cada uno.\n",
      "[+] Epoch 1/30 => Loss: 0.6424\n",
      "[+] Epoch 2/30 => Loss: 0.5488\n",
      "[+] Epoch 3/30 => Loss: 0.5139\n",
      "[+] Epoch 4/30 => Loss: 0.5002\n",
      "[+] Epoch 5/30 => Loss: 0.4884\n",
      "[+] Epoch 6/30 => Loss: 0.4752\n",
      "[+] Epoch 7/30 => Loss: 0.4523\n",
      "[+] Epoch 8/30 => Loss: 0.4339\n",
      "[+] Epoch 9/30 => Loss: 0.4129\n",
      "[+] Epoch 10/30 => Loss: 0.3998\n",
      "[+] Epoch 11/30 => Loss: 0.3918\n",
      "[+] Epoch 12/30 => Loss: 0.3883\n",
      "[+] Epoch 13/30 => Loss: 0.3817\n",
      "[+] Epoch 14/30 => Loss: 0.3747\n",
      "[+] Epoch 15/30 => Loss: 0.3671\n",
      "[+] Epoch 16/30 => Loss: 0.3623\n",
      "[+] Epoch 17/30 => Loss: 0.3566\n",
      "[+] Epoch 18/30 => Loss: 0.3536\n",
      "[+] Epoch 19/30 => Loss: 0.3506\n",
      "[+] Epoch 20/30 => Loss: 0.3501\n",
      "[+] Epoch 21/30 => Loss: 0.3474\n",
      "[+] Epoch 22/30 => Loss: 0.3443\n",
      "[+] Epoch 23/30 => Loss: 0.3405\n",
      "[+] Epoch 24/30 => Loss: 0.3375\n",
      "[+] Epoch 25/30 => Loss: 0.3337\n",
      "[+] Epoch 26/30 => Loss: 0.3272\n",
      "[+] Epoch 27/30 => Loss: 0.3285\n",
      "[+] Epoch 28/30 => Loss: 0.3239\n",
      "[+] Epoch 29/30 => Loss: 0.3225\n",
      "[+] Epoch 30/30 => Loss: 0.3167\n",
      "[+] Ítems cargados: 53 ítems con 12 características cada uno.\n",
      "[+] Users cargados: 698 users con 7 características cada uno.\n",
      "[+] Interactions cargados: 698 interactions con 53 características cada uno.\n",
      "[+] Epoch 1/30 => Loss: 0.7123\n",
      "[+] Epoch 2/30 => Loss: 0.6333\n",
      "[+] Epoch 3/30 => Loss: 0.5676\n",
      "[+] Epoch 4/30 => Loss: 0.5162\n",
      "[+] Epoch 5/30 => Loss: 0.4897\n",
      "[+] Epoch 6/30 => Loss: 0.4884\n",
      "[+] Epoch 7/30 => Loss: 0.4823\n",
      "[+] Epoch 8/30 => Loss: 0.4595\n",
      "[+] Epoch 9/30 => Loss: 0.4301\n",
      "[+] Epoch 10/30 => Loss: 0.4047\n",
      "[+] Epoch 11/30 => Loss: 0.3876\n",
      "[+] Epoch 12/30 => Loss: 0.3751\n",
      "[+] Epoch 13/30 => Loss: 0.3627\n",
      "[+] Epoch 14/30 => Loss: 0.3484\n",
      "[+] Epoch 15/30 => Loss: 0.3365\n",
      "[+] Epoch 16/30 => Loss: 0.3305\n",
      "[+] Epoch 17/30 => Loss: 0.3346\n",
      "[+] Epoch 18/30 => Loss: 0.3380\n",
      "[+] Epoch 19/30 => Loss: 0.3344\n",
      "[+] Epoch 20/30 => Loss: 0.3290\n",
      "[+] Epoch 21/30 => Loss: 0.3196\n",
      "[+] Epoch 22/30 => Loss: 0.3118\n",
      "[+] Epoch 23/30 => Loss: 0.3065\n",
      "[+] Epoch 24/30 => Loss: 0.3013\n",
      "[+] Epoch 25/30 => Loss: 0.3002\n",
      "[+] Epoch 26/30 => Loss: 0.2974\n",
      "[+] Epoch 27/30 => Loss: 0.2946\n",
      "[+] Epoch 28/30 => Loss: 0.2921\n",
      "[+] Epoch 29/30 => Loss: 0.2898\n",
      "[+] Epoch 30/30 => Loss: 0.2879\n",
      "[+] Ítems cargados: 53 ítems con 12 características cada uno.\n",
      "[+] Users cargados: 698 users con 7 características cada uno.\n",
      "[+] Interactions cargados: 698 interactions con 53 características cada uno.\n",
      "[+] Epoch 1/30 => Loss: 0.7270\n",
      "[+] Epoch 2/30 => Loss: 0.6220\n",
      "[+] Epoch 3/30 => Loss: 0.5532\n",
      "[+] Epoch 4/30 => Loss: 0.5177\n",
      "[+] Epoch 5/30 => Loss: 0.5111\n",
      "[+] Epoch 6/30 => Loss: 0.5076\n",
      "[+] Epoch 7/30 => Loss: 0.4862\n",
      "[+] Epoch 8/30 => Loss: 0.4558\n",
      "[+] Epoch 9/30 => Loss: 0.4301\n",
      "[+] Epoch 10/30 => Loss: 0.4089\n",
      "[+] Epoch 11/30 => Loss: 0.3935\n",
      "[+] Epoch 12/30 => Loss: 0.3813\n",
      "[+] Epoch 13/30 => Loss: 0.3660\n",
      "[+] Epoch 14/30 => Loss: 0.3524\n",
      "[+] Epoch 15/30 => Loss: 0.3440\n",
      "[+] Epoch 16/30 => Loss: 0.3456\n",
      "[+] Epoch 17/30 => Loss: 0.3480\n",
      "[+] Epoch 18/30 => Loss: 0.3484\n",
      "[+] Epoch 19/30 => Loss: 0.3439\n",
      "[+] Epoch 20/30 => Loss: 0.3366\n",
      "[+] Epoch 21/30 => Loss: 0.3279\n",
      "[+] Epoch 22/30 => Loss: 0.3190\n",
      "[+] Epoch 23/30 => Loss: 0.3144\n",
      "[+] Epoch 24/30 => Loss: 0.3129\n",
      "[+] Epoch 25/30 => Loss: 0.3103\n",
      "[+] Epoch 26/30 => Loss: 0.3098\n",
      "[+] Epoch 27/30 => Loss: 0.3072\n",
      "[+] Epoch 28/30 => Loss: 0.3033\n",
      "[+] Epoch 29/30 => Loss: 0.3011\n",
      "[+] Epoch 30/30 => Loss: 0.2982\n"
     ]
    }
   ],
   "source": [
    "#  ████████╗██╗    ██╗ ██████╗ ████████╗ ██████╗ ██╗    ██╗███████╗██████╗ ███╗   ███╗ ██████╗ ██████╗ ███████╗██╗     \n",
    "#  ╚══██╔══╝██║    ██║██╔═══██╗╚══██╔══╝██╔═══██╗██║    ██║██╔════╝██╔══██╗████╗ ████║██╔═══██╗██╔══██╗██╔════╝██║     \n",
    "#     ██║   ██║ █╗ ██║██║   ██║   ██║   ██║   ██║██║ █╗ ██║█████╗  ██████╔╝██╔████╔██║██║   ██║██║  ██║█████╗  ██║     \n",
    "#     ██║   ██║███╗██║██║   ██║   ██║   ██║   ██║██║███╗██║██╔══╝  ██╔══██╗██║╚██╔╝██║██║   ██║██║  ██║██╔══╝  ██║     \n",
    "#     ██║   ╚███╔███╔╝╚██████╔╝   ██║   ╚██████╔╝╚███╔███╔╝███████╗██║  ██║██║ ╚═╝ ██║╚██████╔╝██████╔╝███████╗███████╗\n",
    "#     ╚═╝    ╚══╝╚══╝  ╚═════╝    ╚═╝    ╚═════╝  ╚══╝╚══╝ ╚══════╝╚═╝  ╚═╝╚═╝     ╚═╝ ╚═════╝ ╚═════╝ ╚══════╝╚══════╝\n",
    "\n",
    "\n",
    "# V1\n",
    "recomendador_v1_1 = TwoTowerRecommenderSystem(\n",
    "    Two_Tower_Model=TwoTowerModel, \n",
    "    User_Tower=UserTowerV1,\n",
    "    Item_Tower=ItemTowerV1,\n",
    "    user_input_size=df_train_users.shape[1] - 1,\n",
    "    item_input_size=df_items.shape[1] - 1,\n",
    ")\n",
    "recomendador_v1_1.load_data_train(data_items=df_items, data_users=df_train_users, data_interactions=df_train_interacciones)\n",
    "recomendador_v1_1.train()\n",
    "\n",
    "\n",
    "# V2\n",
    "recomendador_v1_2 = TwoTowerRecommenderSystem(\n",
    "    Two_Tower_Model=TwoTowerModel, \n",
    "    User_Tower=UserTowerV2,\n",
    "    Item_Tower=ItemTowerV2,\n",
    "    user_input_size=df_train_users.shape[1] - 1,\n",
    "    item_input_size=df_items.shape[1] - 1,\n",
    ")\n",
    "recomendador_v1_2.load_data_train(data_items=df_items, data_users=df_train_users, data_interactions=df_train_interacciones)\n",
    "recomendador_v1_2.train()\n",
    "\n",
    "\n",
    "# V3\n",
    "recomendador_v1_3 = TwoTowerRecommenderSystem(\n",
    "    Two_Tower_Model=TwoTowerModel, \n",
    "    User_Tower=UserTowerV3,\n",
    "    Item_Tower=ItemTowerV3,\n",
    "    user_input_size=df_train_users.shape[1] - 1,\n",
    "    item_input_size=df_items.shape[1] - 1,\n",
    ")\n",
    "recomendador_v1_3.load_data_train(data_items=df_items, data_users=df_train_users, data_interactions=df_train_interacciones)\n",
    "recomendador_v1_3.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Evaluation Results => Precision@5: 0.7646, Recall@5: 0.3684, NDCG@5: 0.8085\n",
      "[+] Evaluation Results => Precision@5: 0.7474, Recall@5: 0.3791, NDCG@5: 0.7863\n",
      "[+] Evaluation Results => Precision@5: 0.7474, Recall@5: 0.3791, NDCG@5: 0.8040\n",
      "\n",
      "[+] Evaluation Results => Precision@10: 0.6680, Recall@10: 0.6016, NDCG@10: 0.7804\n",
      "[+] Evaluation Results => Precision@10: 0.7371, Recall@10: 0.7004, NDCG@10: 0.8459\n",
      "[+] Evaluation Results => Precision@10: 0.7034, Recall@10: 0.6664, NDCG@10: 0.8208\n",
      "\n",
      "[+] Evaluation Results => Precision@15: 0.6316, Recall@15: 0.8520, NDCG@15: 0.8426\n",
      "[+] Evaluation Results => Precision@15: 0.6316, Recall@15: 0.8520, NDCG@15: 0.8585\n",
      "[+] Evaluation Results => Precision@15: 0.6316, Recall@15: 0.8520, NDCG@15: 0.8505\n"
     ]
    }
   ],
   "source": [
    "recomendador_v1_1.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=5)\n",
    "recomendador_v1_2.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=5)\n",
    "recomendador_v1_3.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=5)\n",
    "print()\n",
    "recomendador_v1_1.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=10)\n",
    "recomendador_v1_2.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=10)\n",
    "recomendador_v1_3.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=10)\n",
    "print()\n",
    "recomendador_v1_1.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=15)\n",
    "recomendador_v1_2.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=15)\n",
    "recomendador_v1_3.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Ítems cargados: 53 ítems con 12 características cada uno.\n",
      "[+] Users cargados: 698 users con 7 características cada uno.\n",
      "[+] Interactions cargados: 698 interactions con 53 características cada uno.\n",
      "[+] Epoch 1/30 => Loss: 0.7059\n",
      "[+] Epoch 2/30 => Loss: 0.6751\n",
      "[+] Epoch 3/30 => Loss: 0.6473\n",
      "[+] Epoch 4/30 => Loss: 0.6246\n",
      "[+] Epoch 5/30 => Loss: 0.6067\n",
      "[+] Epoch 6/30 => Loss: 0.5917\n",
      "[+] Epoch 7/30 => Loss: 0.5796\n",
      "[+] Epoch 8/30 => Loss: 0.5707\n",
      "[+] Epoch 9/30 => Loss: 0.5630\n",
      "[+] Epoch 10/30 => Loss: 0.5569\n",
      "[+] Epoch 11/30 => Loss: 0.5513\n",
      "[+] Epoch 12/30 => Loss: 0.5468\n",
      "[+] Epoch 13/30 => Loss: 0.5431\n",
      "[+] Epoch 14/30 => Loss: 0.5397\n",
      "[+] Epoch 15/30 => Loss: 0.5357\n",
      "[+] Epoch 16/30 => Loss: 0.5331\n",
      "[+] Epoch 17/30 => Loss: 0.5303\n",
      "[+] Epoch 18/30 => Loss: 0.5274\n",
      "[+] Epoch 19/30 => Loss: 0.5246\n",
      "[+] Epoch 20/30 => Loss: 0.5217\n",
      "[+] Epoch 21/30 => Loss: 0.5192\n",
      "[+] Epoch 22/30 => Loss: 0.5166\n",
      "[+] Epoch 23/30 => Loss: 0.5138\n",
      "[+] Epoch 24/30 => Loss: 0.5110\n",
      "[+] Epoch 25/30 => Loss: 0.5080\n",
      "[+] Epoch 26/30 => Loss: 0.5057\n",
      "[+] Epoch 27/30 => Loss: 0.5030\n",
      "[+] Epoch 28/30 => Loss: 0.5004\n",
      "[+] Epoch 29/30 => Loss: 0.4974\n",
      "[+] Epoch 30/30 => Loss: 0.4942\n",
      "[+] Ítems cargados: 53 ítems con 12 características cada uno.\n",
      "[+] Users cargados: 698 users con 7 características cada uno.\n",
      "[+] Interactions cargados: 698 interactions con 53 características cada uno.\n",
      "[+] Epoch 1/30 => Loss: 0.6705\n",
      "[+] Epoch 2/30 => Loss: 0.6069\n",
      "[+] Epoch 3/30 => Loss: 0.5739\n",
      "[+] Epoch 4/30 => Loss: 0.5577\n",
      "[+] Epoch 5/30 => Loss: 0.5494\n",
      "[+] Epoch 6/30 => Loss: 0.5432\n",
      "[+] Epoch 7/30 => Loss: 0.5373\n",
      "[+] Epoch 8/30 => Loss: 0.5312\n",
      "[+] Epoch 9/30 => Loss: 0.5240\n",
      "[+] Epoch 10/30 => Loss: 0.5165\n",
      "[+] Epoch 11/30 => Loss: 0.5094\n",
      "[+] Epoch 12/30 => Loss: 0.5021\n",
      "[+] Epoch 13/30 => Loss: 0.4958\n",
      "[+] Epoch 14/30 => Loss: 0.4895\n",
      "[+] Epoch 15/30 => Loss: 0.4843\n",
      "[+] Epoch 16/30 => Loss: 0.4801\n",
      "[+] Epoch 17/30 => Loss: 0.4760\n",
      "[+] Epoch 18/30 => Loss: 0.4726\n",
      "[+] Epoch 19/30 => Loss: 0.4695\n",
      "[+] Epoch 20/30 => Loss: 0.4676\n",
      "[+] Epoch 21/30 => Loss: 0.4655\n",
      "[+] Epoch 22/30 => Loss: 0.4637\n",
      "[+] Epoch 23/30 => Loss: 0.4619\n",
      "[+] Epoch 24/30 => Loss: 0.4608\n",
      "[+] Epoch 25/30 => Loss: 0.4596\n",
      "[+] Epoch 26/30 => Loss: 0.4585\n",
      "[+] Epoch 27/30 => Loss: 0.4573\n",
      "[+] Epoch 28/30 => Loss: 0.4563\n",
      "[+] Epoch 29/30 => Loss: 0.4555\n",
      "[+] Epoch 30/30 => Loss: 0.4541\n",
      "[+] Ítems cargados: 53 ítems con 12 características cada uno.\n",
      "[+] Users cargados: 698 users con 7 características cada uno.\n",
      "[+] Interactions cargados: 698 interactions con 53 características cada uno.\n",
      "[+] Epoch 1/30 => Loss: 0.7155\n",
      "[+] Epoch 2/30 => Loss: 0.6546\n",
      "[+] Epoch 3/30 => Loss: 0.6153\n",
      "[+] Epoch 4/30 => Loss: 0.5930\n",
      "[+] Epoch 5/30 => Loss: 0.5793\n",
      "[+] Epoch 6/30 => Loss: 0.5698\n",
      "[+] Epoch 7/30 => Loss: 0.5627\n",
      "[+] Epoch 8/30 => Loss: 0.5562\n",
      "[+] Epoch 9/30 => Loss: 0.5511\n",
      "[+] Epoch 10/30 => Loss: 0.5467\n",
      "[+] Epoch 11/30 => Loss: 0.5426\n",
      "[+] Epoch 12/30 => Loss: 0.5393\n",
      "[+] Epoch 13/30 => Loss: 0.5358\n",
      "[+] Epoch 14/30 => Loss: 0.5326\n",
      "[+] Epoch 15/30 => Loss: 0.5288\n",
      "[+] Epoch 16/30 => Loss: 0.5248\n",
      "[+] Epoch 17/30 => Loss: 0.5207\n",
      "[+] Epoch 18/30 => Loss: 0.5157\n",
      "[+] Epoch 19/30 => Loss: 0.5104\n",
      "[+] Epoch 20/30 => Loss: 0.5047\n",
      "[+] Epoch 21/30 => Loss: 0.4988\n",
      "[+] Epoch 22/30 => Loss: 0.4928\n",
      "[+] Epoch 23/30 => Loss: 0.4877\n",
      "[+] Epoch 24/30 => Loss: 0.4823\n",
      "[+] Epoch 25/30 => Loss: 0.4780\n",
      "[+] Epoch 26/30 => Loss: 0.4739\n",
      "[+] Epoch 27/30 => Loss: 0.4702\n",
      "[+] Epoch 28/30 => Loss: 0.4677\n",
      "[+] Epoch 29/30 => Loss: 0.4652\n",
      "[+] Epoch 30/30 => Loss: 0.4634\n"
     ]
    }
   ],
   "source": [
    "#  ████████╗██╗    ██╗ ██████╗ ████████╗ ██████╗ ██╗    ██╗███████╗██████╗ ███╗   ███╗ ██████╗ ██████╗ ███████╗██╗    ██╗   ██╗ ██╗\n",
    "#  ╚══██╔══╝██║    ██║██╔═══██╗╚══██╔══╝██╔═══██╗██║    ██║██╔════╝██╔══██╗████╗ ████║██╔═══██╗██╔══██╗██╔════╝██║    ██║   ██║███║\n",
    "#     ██║   ██║ █╗ ██║██║   ██║   ██║   ██║   ██║██║ █╗ ██║█████╗  ██████╔╝██╔████╔██║██║   ██║██║  ██║█████╗  ██║    ██║   ██║╚██║\n",
    "#     ██║   ██║███╗██║██║   ██║   ██║   ██║   ██║██║███╗██║██╔══╝  ██╔══██╗██║╚██╔╝██║██║   ██║██║  ██║██╔══╝  ██║    ╚██╗ ██╔╝ ██║\n",
    "#     ██║   ╚███╔███╔╝╚██████╔╝   ██║   ╚██████╔╝╚███╔███╔╝███████╗██║  ██║██║ ╚═╝ ██║╚██████╔╝██████╔╝███████╗███████╗╚████╔╝  ██║\n",
    "#     ╚═╝    ╚══╝╚══╝  ╚═════╝    ╚═╝    ╚═════╝  ╚══╝╚══╝ ╚══════╝╚═╝  ╚═╝╚═╝     ╚═╝ ╚═════╝ ╚═════╝ ╚══════╝╚══════╝ ╚═══╝   ╚═╝\n",
    "\n",
    "# V1\n",
    "recomendador_v2_1 = TwoTowerRecommenderSystem(\n",
    "    Two_Tower_Model=TwoTowerModelV1, \n",
    "    User_Tower=UserTowerV1,\n",
    "    Item_Tower=ItemTowerV1,\n",
    "    user_input_size=df_train_users.shape[1] - 1,\n",
    "    item_input_size=df_items.shape[1] - 1,\n",
    ")\n",
    "recomendador_v2_1.load_data_train(data_items=df_items, data_users=df_train_users, data_interactions=df_train_interacciones)\n",
    "recomendador_v2_1.train()\n",
    "\n",
    "\n",
    "# V2\n",
    "recomendador_v2_2 = TwoTowerRecommenderSystem(\n",
    "    Two_Tower_Model=TwoTowerModelV1, \n",
    "    User_Tower=UserTowerV2,\n",
    "    Item_Tower=ItemTowerV2,\n",
    "    user_input_size=df_train_users.shape[1] - 1,\n",
    "    item_input_size=df_items.shape[1] - 1,\n",
    ")\n",
    "recomendador_v2_2.load_data_train(data_items=df_items, data_users=df_train_users, data_interactions=df_train_interacciones)\n",
    "recomendador_v2_2.train()\n",
    "\n",
    "\n",
    "# V3\n",
    "recomendador_v2_3 = TwoTowerRecommenderSystem(\n",
    "    Two_Tower_Model=TwoTowerModelV1, \n",
    "    User_Tower=UserTowerV3,\n",
    "    Item_Tower=ItemTowerV3,\n",
    "    user_input_size=df_train_users.shape[1] - 1,\n",
    "    item_input_size=df_items.shape[1] - 1,\n",
    ")\n",
    "recomendador_v2_3.load_data_train(data_items=df_items, data_users=df_train_users, data_interactions=df_train_interacciones)\n",
    "recomendador_v2_3.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Evaluation Results => Precision@5: 0.7394, Recall@5: 0.3650, NDCG@5: 0.7562\n",
      "[+] Evaluation Results => Precision@5: 0.7474, Recall@5: 0.3791, NDCG@5: 0.7753\n",
      "[+] Evaluation Results => Precision@5: 0.7314, Recall@5: 0.3338, NDCG@5: 0.7770\n",
      "\n",
      "[+] Evaluation Results => Precision@10: 0.6669, Recall@10: 0.6079, NDCG@10: 0.7842\n",
      "[+] Evaluation Results => Precision@10: 0.7149, Recall@10: 0.6740, NDCG@10: 0.8258\n",
      "[+] Evaluation Results => Precision@10: 0.6920, Recall@10: 0.6151, NDCG@10: 0.7820\n",
      "\n",
      "[+] Evaluation Results => Precision@15: 0.6316, Recall@15: 0.8520, NDCG@15: 0.8074\n",
      "[+] Evaluation Results => Precision@15: 0.6316, Recall@15: 0.8520, NDCG@15: 0.8457\n",
      "[+] Evaluation Results => Precision@15: 0.6316, Recall@15: 0.8520, NDCG@15: 0.8304\n"
     ]
    }
   ],
   "source": [
    "recomendador_v2_1.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=5)\n",
    "recomendador_v2_2.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=5)\n",
    "recomendador_v2_3.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=5)\n",
    "print()\n",
    "recomendador_v2_1.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=10)\n",
    "recomendador_v2_2.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=10)\n",
    "recomendador_v2_3.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=10)\n",
    "print()\n",
    "recomendador_v2_1.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=15)\n",
    "recomendador_v2_2.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=15)\n",
    "recomendador_v2_3.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Ítems cargados: 53 ítems con 12 características cada uno.\n",
      "[+] Users cargados: 698 users con 7 características cada uno.\n",
      "[+] Interactions cargados: 698 interactions con 53 características cada uno.\n",
      "[+] Epoch 1/30 => Loss: 0.7172\n",
      "[+] Epoch 2/30 => Loss: 0.6917\n",
      "[+] Epoch 3/30 => Loss: 0.6684\n",
      "[+] Epoch 4/30 => Loss: 0.6462\n",
      "[+] Epoch 5/30 => Loss: 0.6254\n",
      "[+] Epoch 6/30 => Loss: 0.6052\n",
      "[+] Epoch 7/30 => Loss: 0.5863\n",
      "[+] Epoch 8/30 => Loss: 0.5683\n",
      "[+] Epoch 9/30 => Loss: 0.5529\n",
      "[+] Epoch 10/30 => Loss: 0.5405\n",
      "[+] Epoch 11/30 => Loss: 0.5284\n",
      "[+] Epoch 12/30 => Loss: 0.5213\n",
      "[+] Epoch 13/30 => Loss: 0.5168\n",
      "[+] Epoch 14/30 => Loss: 0.5141\n",
      "[+] Epoch 15/30 => Loss: 0.5136\n",
      "[+] Epoch 16/30 => Loss: 0.5144\n",
      "[+] Epoch 17/30 => Loss: 0.5101\n",
      "[+] Epoch 18/30 => Loss: 0.5053\n",
      "[+] Epoch 19/30 => Loss: 0.5010\n",
      "[+] Epoch 20/30 => Loss: 0.4951\n",
      "[+] Epoch 21/30 => Loss: 0.4865\n",
      "[+] Epoch 22/30 => Loss: 0.4769\n",
      "[+] Epoch 23/30 => Loss: 0.4674\n",
      "[+] Epoch 24/30 => Loss: 0.4595\n",
      "[+] Epoch 25/30 => Loss: 0.4501\n",
      "[+] Epoch 26/30 => Loss: 0.4411\n",
      "[+] Epoch 27/30 => Loss: 0.4318\n",
      "[+] Epoch 28/30 => Loss: 0.4231\n",
      "[+] Epoch 29/30 => Loss: 0.4158\n",
      "[+] Epoch 30/30 => Loss: 0.4064\n",
      "[+] Ítems cargados: 53 ítems con 12 características cada uno.\n",
      "[+] Users cargados: 698 users con 7 características cada uno.\n",
      "[+] Interactions cargados: 698 interactions con 53 características cada uno.\n",
      "[+] Epoch 1/30 => Loss: 0.7091\n",
      "[+] Epoch 2/30 => Loss: 0.6842\n",
      "[+] Epoch 3/30 => Loss: 0.6624\n",
      "[+] Epoch 4/30 => Loss: 0.6390\n",
      "[+] Epoch 5/30 => Loss: 0.6125\n",
      "[+] Epoch 6/30 => Loss: 0.5851\n",
      "[+] Epoch 7/30 => Loss: 0.5560\n",
      "[+] Epoch 8/30 => Loss: 0.5311\n",
      "[+] Epoch 9/30 => Loss: 0.5133\n",
      "[+] Epoch 10/30 => Loss: 0.5044\n",
      "[+] Epoch 11/30 => Loss: 0.5026\n",
      "[+] Epoch 12/30 => Loss: 0.5063\n",
      "[+] Epoch 13/30 => Loss: 0.5042\n",
      "[+] Epoch 14/30 => Loss: 0.4930\n",
      "[+] Epoch 15/30 => Loss: 0.4808\n",
      "[+] Epoch 16/30 => Loss: 0.4628\n",
      "[+] Epoch 17/30 => Loss: 0.4495\n",
      "[+] Epoch 18/30 => Loss: 0.4314\n",
      "[+] Epoch 19/30 => Loss: 0.4161\n",
      "[+] Epoch 20/30 => Loss: 0.4018\n",
      "[+] Epoch 21/30 => Loss: 0.3886\n",
      "[+] Epoch 22/30 => Loss: 0.3771\n",
      "[+] Epoch 23/30 => Loss: 0.3646\n",
      "[+] Epoch 24/30 => Loss: 0.3538\n",
      "[+] Epoch 25/30 => Loss: 0.3435\n",
      "[+] Epoch 26/30 => Loss: 0.3363\n",
      "[+] Epoch 27/30 => Loss: 0.3309\n",
      "[+] Epoch 28/30 => Loss: 0.3287\n",
      "[+] Epoch 29/30 => Loss: 0.3257\n",
      "[+] Epoch 30/30 => Loss: 0.3235\n",
      "[+] Ítems cargados: 53 ítems con 12 características cada uno.\n",
      "[+] Users cargados: 698 users con 7 características cada uno.\n",
      "[+] Interactions cargados: 698 interactions con 53 características cada uno.\n",
      "[+] Epoch 1/30 => Loss: 0.6722\n",
      "[+] Epoch 2/30 => Loss: 0.6483\n",
      "[+] Epoch 3/30 => Loss: 0.6268\n",
      "[+] Epoch 4/30 => Loss: 0.6045\n",
      "[+] Epoch 5/30 => Loss: 0.5812\n",
      "[+] Epoch 6/30 => Loss: 0.5568\n",
      "[+] Epoch 7/30 => Loss: 0.5353\n",
      "[+] Epoch 8/30 => Loss: 0.5190\n",
      "[+] Epoch 9/30 => Loss: 0.5092\n",
      "[+] Epoch 10/30 => Loss: 0.5081\n",
      "[+] Epoch 11/30 => Loss: 0.5090\n",
      "[+] Epoch 12/30 => Loss: 0.5075\n",
      "[+] Epoch 13/30 => Loss: 0.4963\n",
      "[+] Epoch 14/30 => Loss: 0.4821\n",
      "[+] Epoch 15/30 => Loss: 0.4665\n",
      "[+] Epoch 16/30 => Loss: 0.4497\n",
      "[+] Epoch 17/30 => Loss: 0.4354\n",
      "[+] Epoch 18/30 => Loss: 0.4213\n",
      "[+] Epoch 19/30 => Loss: 0.4069\n",
      "[+] Epoch 20/30 => Loss: 0.3940\n",
      "[+] Epoch 21/30 => Loss: 0.3812\n",
      "[+] Epoch 22/30 => Loss: 0.3688\n",
      "[+] Epoch 23/30 => Loss: 0.3564\n",
      "[+] Epoch 24/30 => Loss: 0.3500\n",
      "[+] Epoch 25/30 => Loss: 0.3438\n",
      "[+] Epoch 26/30 => Loss: 0.3390\n",
      "[+] Epoch 27/30 => Loss: 0.3360\n",
      "[+] Epoch 28/30 => Loss: 0.3325\n",
      "[+] Epoch 29/30 => Loss: 0.3303\n",
      "[+] Epoch 30/30 => Loss: 0.3283\n"
     ]
    }
   ],
   "source": [
    "#  ████████╗██╗    ██╗ ██████╗ ████████╗ ██████╗ ██╗    ██╗███████╗██████╗ ███╗   ███╗ ██████╗ ██████╗ ███████╗██╗    ██╗   ██╗██████╗ \n",
    "#  ╚══██╔══╝██║    ██║██╔═══██╗╚══██╔══╝██╔═══██╗██║    ██║██╔════╝██╔══██╗████╗ ████║██╔═══██╗██╔══██╗██╔════╝██║    ██║   ██║╚════██╗\n",
    "#     ██║   ██║ █╗ ██║██║   ██║   ██║   ██║   ██║██║ █╗ ██║█████╗  ██████╔╝██╔████╔██║██║   ██║██║  ██║█████╗  ██║    ██║   ██║ █████╔╝\n",
    "#     ██║   ██║███╗██║██║   ██║   ██║   ██║   ██║██║███╗██║██╔══╝  ██╔══██╗██║╚██╔╝██║██║   ██║██║  ██║██╔══╝  ██║    ╚██╗ ██╔╝██╔═══╝ \n",
    "#     ██║   ╚███╔███╔╝╚██████╔╝   ██║   ╚██████╔╝╚███╔███╔╝███████╗██║  ██║██║ ╚═╝ ██║╚██████╔╝██████╔╝███████╗███████╗╚████╔╝ ███████╗\n",
    "#     ╚═╝    ╚══╝╚══╝  ╚═════╝    ╚═╝    ╚═════╝  ╚══╝╚══╝ ╚══════╝╚═╝  ╚═╝╚═╝     ╚═╝ ╚═════╝ ╚═════╝ ╚══════╝╚══════╝ ╚═══╝  ╚══════╝\n",
    "\n",
    "# V1\n",
    "recomendador_v3_1 = TwoTowerRecommenderSystem(\n",
    "    Two_Tower_Model=TwoTowerModelV2, \n",
    "    User_Tower=UserTowerV1,\n",
    "    Item_Tower=ItemTowerV1,\n",
    "    user_input_size=df_train_users.shape[1] - 1,\n",
    "    item_input_size=df_items.shape[1] - 1,\n",
    ")\n",
    "recomendador_v3_1.load_data_train(data_items=df_items, data_users=df_train_users, data_interactions=df_train_interacciones)\n",
    "recomendador_v3_1.train()\n",
    "\n",
    "\n",
    "# V2\n",
    "recomendador_v3_2 = TwoTowerRecommenderSystem(\n",
    "    Two_Tower_Model=TwoTowerModelV2, \n",
    "    User_Tower=UserTowerV2,\n",
    "    Item_Tower=ItemTowerV2,\n",
    "    user_input_size=df_train_users.shape[1] - 1,\n",
    "    item_input_size=df_items.shape[1] - 1,\n",
    ")\n",
    "recomendador_v3_2.load_data_train(data_items=df_items, data_users=df_train_users, data_interactions=df_train_interacciones)\n",
    "recomendador_v3_2.train()\n",
    "\n",
    "\n",
    "# V3\n",
    "recomendador_v3_3 = TwoTowerRecommenderSystem(\n",
    "    Two_Tower_Model=TwoTowerModelV2, \n",
    "    User_Tower=UserTowerV3,\n",
    "    Item_Tower=ItemTowerV3,\n",
    "    user_input_size=df_train_users.shape[1] - 1,\n",
    "    item_input_size=df_items.shape[1] - 1,\n",
    ")\n",
    "recomendador_v3_3.load_data_train(data_items=df_items, data_users=df_train_users, data_interactions=df_train_interacciones)\n",
    "recomendador_v3_3.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Evaluation Results => Precision@5: 0.7177, Recall@5: 0.3517, NDCG@5: 0.7780\n",
      "[+] Evaluation Results => Precision@5: 0.6651, Recall@5: 0.2847, NDCG@5: 0.6875\n",
      "[+] Evaluation Results => Precision@5: 0.6651, Recall@5: 0.2847, NDCG@5: 0.6875\n",
      "\n",
      "[+] Evaluation Results => Precision@10: 0.7166, Recall@10: 0.6735, NDCG@10: 0.7996\n",
      "[+] Evaluation Results => Precision@10: 0.6457, Recall@10: 0.5824, NDCG@10: 0.6516\n",
      "[+] Evaluation Results => Precision@10: 0.6457, Recall@10: 0.5824, NDCG@10: 0.6516\n",
      "\n",
      "[+] Evaluation Results => Precision@15: 0.6316, Recall@15: 0.8520, NDCG@15: 0.8297\n",
      "[+] Evaluation Results => Precision@15: 0.6316, Recall@15: 0.8520, NDCG@15: 0.7149\n",
      "[+] Evaluation Results => Precision@15: 0.6316, Recall@15: 0.8520, NDCG@15: 0.7149\n"
     ]
    }
   ],
   "source": [
    "recomendador_v3_1.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=5)\n",
    "recomendador_v3_2.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=5)\n",
    "recomendador_v3_3.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=5)\n",
    "print()\n",
    "recomendador_v3_1.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=10)\n",
    "recomendador_v3_2.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=10)\n",
    "recomendador_v3_3.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=10)\n",
    "print()\n",
    "recomendador_v3_1.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=15)\n",
    "recomendador_v3_2.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=15)\n",
    "recomendador_v3_3.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Ítems cargados: 53 ítems con 12 características cada uno.\n",
      "[+] Users cargados: 698 users con 7 características cada uno.\n",
      "[+] Interactions cargados: 698 interactions con 53 características cada uno.\n",
      "[+] Epoch 1/30 => Loss: 0.7006\n",
      "[+] Epoch 2/30 => Loss: 0.6664\n",
      "[+] Epoch 3/30 => Loss: 0.6356\n",
      "[+] Epoch 4/30 => Loss: 0.6068\n",
      "[+] Epoch 5/30 => Loss: 0.5807\n",
      "[+] Epoch 6/30 => Loss: 0.5573\n",
      "[+] Epoch 7/30 => Loss: 0.5370\n",
      "[+] Epoch 8/30 => Loss: 0.5226\n",
      "[+] Epoch 9/30 => Loss: 0.5135\n",
      "[+] Epoch 10/30 => Loss: 0.5102\n",
      "[+] Epoch 11/30 => Loss: 0.5111\n",
      "[+] Epoch 12/30 => Loss: 0.5117\n",
      "[+] Epoch 13/30 => Loss: 0.5102\n",
      "[+] Epoch 14/30 => Loss: 0.5058\n",
      "[+] Epoch 15/30 => Loss: 0.4978\n",
      "[+] Epoch 16/30 => Loss: 0.4876\n",
      "[+] Epoch 17/30 => Loss: 0.4767\n",
      "[+] Epoch 18/30 => Loss: 0.4669\n",
      "[+] Epoch 19/30 => Loss: 0.4567\n",
      "[+] Epoch 20/30 => Loss: 0.4478\n",
      "[+] Epoch 21/30 => Loss: 0.4387\n",
      "[+] Epoch 22/30 => Loss: 0.4308\n",
      "[+] Epoch 23/30 => Loss: 0.4226\n",
      "[+] Epoch 24/30 => Loss: 0.4138\n",
      "[+] Epoch 25/30 => Loss: 0.4044\n",
      "[+] Epoch 26/30 => Loss: 0.3944\n",
      "[+] Epoch 27/30 => Loss: 0.3838\n",
      "[+] Epoch 28/30 => Loss: 0.3732\n",
      "[+] Epoch 29/30 => Loss: 0.3633\n",
      "[+] Epoch 30/30 => Loss: 0.3547\n",
      "[+] Ítems cargados: 53 ítems con 12 características cada uno.\n",
      "[+] Users cargados: 698 users con 7 características cada uno.\n",
      "[+] Interactions cargados: 698 interactions con 53 características cada uno.\n",
      "[+] Epoch 1/30 => Loss: 0.6766\n",
      "[+] Epoch 2/30 => Loss: 0.6522\n",
      "[+] Epoch 3/30 => Loss: 0.6284\n",
      "[+] Epoch 4/30 => Loss: 0.6035\n",
      "[+] Epoch 5/30 => Loss: 0.5779\n",
      "[+] Epoch 6/30 => Loss: 0.5521\n",
      "[+] Epoch 7/30 => Loss: 0.5303\n",
      "[+] Epoch 8/30 => Loss: 0.5166\n",
      "[+] Epoch 9/30 => Loss: 0.5144\n",
      "[+] Epoch 10/30 => Loss: 0.5171\n",
      "[+] Epoch 11/30 => Loss: 0.5143\n",
      "[+] Epoch 12/30 => Loss: 0.5032\n",
      "[+] Epoch 13/30 => Loss: 0.4868\n",
      "[+] Epoch 14/30 => Loss: 0.4683\n",
      "[+] Epoch 15/30 => Loss: 0.4495\n",
      "[+] Epoch 16/30 => Loss: 0.4314\n",
      "[+] Epoch 17/30 => Loss: 0.4152\n",
      "[+] Epoch 18/30 => Loss: 0.3987\n",
      "[+] Epoch 19/30 => Loss: 0.3825\n",
      "[+] Epoch 20/30 => Loss: 0.3671\n",
      "[+] Epoch 21/30 => Loss: 0.3535\n",
      "[+] Epoch 22/30 => Loss: 0.3422\n",
      "[+] Epoch 23/30 => Loss: 0.3350\n",
      "[+] Epoch 24/30 => Loss: 0.3299\n",
      "[+] Epoch 25/30 => Loss: 0.3276\n",
      "[+] Epoch 26/30 => Loss: 0.3240\n",
      "[+] Epoch 27/30 => Loss: 0.3213\n",
      "[+] Epoch 28/30 => Loss: 0.3174\n",
      "[+] Epoch 29/30 => Loss: 0.3148\n",
      "[+] Epoch 30/30 => Loss: 0.3111\n",
      "[+] Ítems cargados: 53 ítems con 12 características cada uno.\n",
      "[+] Users cargados: 698 users con 7 características cada uno.\n",
      "[+] Interactions cargados: 698 interactions con 53 características cada uno.\n",
      "[+] Epoch 1/30 => Loss: 0.6984\n",
      "[+] Epoch 2/30 => Loss: 0.6701\n",
      "[+] Epoch 3/30 => Loss: 0.6462\n",
      "[+] Epoch 4/30 => Loss: 0.6225\n",
      "[+] Epoch 5/30 => Loss: 0.5960\n",
      "[+] Epoch 6/30 => Loss: 0.5677\n",
      "[+] Epoch 7/30 => Loss: 0.5398\n",
      "[+] Epoch 8/30 => Loss: 0.5152\n",
      "[+] Epoch 9/30 => Loss: 0.5004\n",
      "[+] Epoch 10/30 => Loss: 0.4974\n",
      "[+] Epoch 11/30 => Loss: 0.5004\n",
      "[+] Epoch 12/30 => Loss: 0.4987\n",
      "[+] Epoch 13/30 => Loss: 0.4907\n",
      "[+] Epoch 14/30 => Loss: 0.4754\n",
      "[+] Epoch 15/30 => Loss: 0.4586\n",
      "[+] Epoch 16/30 => Loss: 0.4416\n",
      "[+] Epoch 17/30 => Loss: 0.4254\n",
      "[+] Epoch 18/30 => Loss: 0.4108\n",
      "[+] Epoch 19/30 => Loss: 0.3977\n",
      "[+] Epoch 20/30 => Loss: 0.3863\n",
      "[+] Epoch 21/30 => Loss: 0.3733\n",
      "[+] Epoch 22/30 => Loss: 0.3637\n",
      "[+] Epoch 23/30 => Loss: 0.3529\n",
      "[+] Epoch 24/30 => Loss: 0.3448\n",
      "[+] Epoch 25/30 => Loss: 0.3380\n",
      "[+] Epoch 26/30 => Loss: 0.3331\n",
      "[+] Epoch 27/30 => Loss: 0.3289\n",
      "[+] Epoch 28/30 => Loss: 0.3255\n",
      "[+] Epoch 29/30 => Loss: 0.3224\n",
      "[+] Epoch 30/30 => Loss: 0.3180\n"
     ]
    }
   ],
   "source": [
    "#  ██╗███╗   ██╗████████╗████████╗ ██████╗ ██╗    ██╗███████╗██████╗ \n",
    "#  ██║████╗  ██║╚══██╔══╝╚══██╔══╝██╔═══██╗██║    ██║██╔════╝██╔══██╗\n",
    "#  ██║██╔██╗ ██║   ██║      ██║   ██║   ██║██║ █╗ ██║█████╗  ██████╔╝\n",
    "#  ██║██║╚██╗██║   ██║      ██║   ██║   ██║██║███╗██║██╔══╝  ██╔══██╗\n",
    "#  ██║██║ ╚████║   ██║      ██║   ╚██████╔╝╚███╔███╔╝███████╗██║  ██║\n",
    "#  ╚═╝╚═╝  ╚═══╝   ╚═╝      ╚═╝    ╚═════╝  ╚══╝╚══╝ ╚══════╝╚═╝  ╚═╝\n",
    "\n",
    "# V1\n",
    "recomendador_v4_1 = TwoTowerRecommenderSystem(\n",
    "    Two_Tower_Model=IntTower, \n",
    "    User_Tower=UserTowerV1,\n",
    "    Item_Tower=ItemTowerV1,\n",
    "    user_input_size=df_train_users.shape[1] - 1,\n",
    "    item_input_size=df_items.shape[1] - 1,\n",
    ")\n",
    "recomendador_v4_1.load_data_train(data_items=df_items, data_users=df_train_users, data_interactions=df_train_interacciones)\n",
    "recomendador_v4_1.train()\n",
    "\n",
    "\n",
    "# V2\n",
    "recomendador_v4_2 = TwoTowerRecommenderSystem(\n",
    "    Two_Tower_Model=IntTower, \n",
    "    User_Tower=UserTowerV2,\n",
    "    Item_Tower=ItemTowerV2,\n",
    "    user_input_size=df_train_users.shape[1] - 1,\n",
    "    item_input_size=df_items.shape[1] - 1,\n",
    ")\n",
    "recomendador_v4_2.load_data_train(data_items=df_items, data_users=df_train_users, data_interactions=df_train_interacciones)\n",
    "recomendador_v4_2.train()\n",
    "\n",
    "\n",
    "# V3\n",
    "recomendador_v4_3 = TwoTowerRecommenderSystem(\n",
    "    Two_Tower_Model=IntTower, \n",
    "    User_Tower=UserTowerV3,\n",
    "    Item_Tower=ItemTowerV3,\n",
    "    user_input_size=df_train_users.shape[1] - 1,\n",
    "    item_input_size=df_items.shape[1] - 1,\n",
    ")\n",
    "recomendador_v4_3.load_data_train(data_items=df_items, data_users=df_train_users, data_interactions=df_train_interacciones)\n",
    "recomendador_v4_3.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Evaluation Results => Precision@5: 0.7554, Recall@5: 0.3808, NDCG@5: 0.7681\n",
      "[+] Evaluation Results => Precision@5: 0.5223, Recall@5: 0.2145, NDCG@5: 0.4666\n",
      "[+] Evaluation Results => Precision@5: 0.6034, Recall@5: 0.2852, NDCG@5: 0.7039\n",
      "\n",
      "[+] Evaluation Results => Precision@10: 0.6457, Recall@10: 0.5824, NDCG@10: 0.7549\n",
      "[+] Evaluation Results => Precision@10: 0.5737, Recall@10: 0.4729, NDCG@10: 0.5462\n",
      "[+] Evaluation Results => Precision@10: 0.6457, Recall@10: 0.5824, NDCG@10: 0.7691\n",
      "\n",
      "[+] Evaluation Results => Precision@15: 0.6316, Recall@15: 0.8520, NDCG@15: 0.8033\n",
      "[+] Evaluation Results => Precision@15: 0.6316, Recall@15: 0.8520, NDCG@15: 0.6783\n",
      "[+] Evaluation Results => Precision@15: 0.6316, Recall@15: 0.8520, NDCG@15: 0.8362\n"
     ]
    }
   ],
   "source": [
    "recomendador_v4_1.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=5)\n",
    "recomendador_v4_2.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=5)\n",
    "recomendador_v4_3.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=5)\n",
    "print()\n",
    "recomendador_v4_1.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=10)\n",
    "recomendador_v4_2.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=10)\n",
    "recomendador_v4_3.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=10)\n",
    "print()\n",
    "recomendador_v4_1.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=15)\n",
    "recomendador_v4_2.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=15)\n",
    "recomendador_v4_3.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Ítems cargados: 53 ítems con 12 características cada uno.\n",
      "[+] Users cargados: 698 users con 7 características cada uno.\n",
      "[+] Interactions cargados: 698 interactions con 53 características cada uno.\n",
      "[+] Epoch 1/30 => Loss: 0.6038\n",
      "[+] Epoch 2/30 => Loss: 0.5246\n",
      "[+] Epoch 3/30 => Loss: 0.4891\n",
      "[+] Epoch 4/30 => Loss: 0.4772\n",
      "[+] Epoch 5/30 => Loss: 0.4644\n",
      "[+] Epoch 6/30 => Loss: 0.4486\n",
      "[+] Epoch 7/30 => Loss: 0.4267\n",
      "[+] Epoch 8/30 => Loss: 0.4042\n",
      "[+] Epoch 9/30 => Loss: 0.3817\n",
      "[+] Epoch 10/30 => Loss: 0.3695\n",
      "[+] Epoch 11/30 => Loss: 0.3630\n",
      "[+] Epoch 12/30 => Loss: 0.3594\n",
      "[+] Epoch 13/30 => Loss: 0.3608\n",
      "[+] Epoch 14/30 => Loss: 0.3562\n",
      "[+] Epoch 15/30 => Loss: 0.3488\n",
      "[+] Epoch 16/30 => Loss: 0.3462\n",
      "[+] Epoch 17/30 => Loss: 0.3409\n",
      "[+] Epoch 18/30 => Loss: 0.3406\n",
      "[+] Epoch 19/30 => Loss: 0.3401\n",
      "[+] Epoch 20/30 => Loss: 0.3336\n",
      "[+] Epoch 21/30 => Loss: 0.3289\n",
      "[+] Epoch 22/30 => Loss: 0.3260\n",
      "[+] Epoch 23/30 => Loss: 0.3188\n",
      "[+] Epoch 24/30 => Loss: 0.3148\n",
      "[+] Epoch 25/30 => Loss: 0.3135\n",
      "[+] Epoch 26/30 => Loss: 0.3115\n",
      "[+] Epoch 27/30 => Loss: 0.3096\n",
      "[+] Epoch 28/30 => Loss: 0.3075\n",
      "[+] Epoch 29/30 => Loss: 0.3087\n",
      "[+] Epoch 30/30 => Loss: 0.3021\n",
      "[+] Ítems cargados: 53 ítems con 12 características cada uno.\n",
      "[+] Users cargados: 698 users con 7 características cada uno.\n",
      "[+] Interactions cargados: 698 interactions con 53 características cada uno.\n",
      "[+] Epoch 1/30 => Loss: 0.7236\n",
      "[+] Epoch 2/30 => Loss: 0.6637\n",
      "[+] Epoch 3/30 => Loss: 0.6027\n",
      "[+] Epoch 4/30 => Loss: 0.5424\n",
      "[+] Epoch 5/30 => Loss: 0.4986\n",
      "[+] Epoch 6/30 => Loss: 0.4914\n",
      "[+] Epoch 7/30 => Loss: 0.5019\n",
      "[+] Epoch 8/30 => Loss: 0.4919\n",
      "[+] Epoch 9/30 => Loss: 0.4656\n",
      "[+] Epoch 10/30 => Loss: 0.4375\n",
      "[+] Epoch 11/30 => Loss: 0.4169\n",
      "[+] Epoch 12/30 => Loss: 0.4050\n",
      "[+] Epoch 13/30 => Loss: 0.3931\n",
      "[+] Epoch 14/30 => Loss: 0.3809\n",
      "[+] Epoch 15/30 => Loss: 0.3648\n",
      "[+] Epoch 16/30 => Loss: 0.3468\n",
      "[+] Epoch 17/30 => Loss: 0.3380\n",
      "[+] Epoch 18/30 => Loss: 0.3363\n",
      "[+] Epoch 19/30 => Loss: 0.3381\n",
      "[+] Epoch 20/30 => Loss: 0.3400\n",
      "[+] Epoch 21/30 => Loss: 0.3349\n",
      "[+] Epoch 22/30 => Loss: 0.3258\n",
      "[+] Epoch 23/30 => Loss: 0.3177\n",
      "[+] Epoch 24/30 => Loss: 0.3094\n",
      "[+] Epoch 25/30 => Loss: 0.3030\n",
      "[+] Epoch 26/30 => Loss: 0.3017\n",
      "[+] Epoch 27/30 => Loss: 0.3023\n",
      "[+] Epoch 28/30 => Loss: 0.2997\n",
      "[+] Epoch 29/30 => Loss: 0.2956\n",
      "[+] Epoch 30/30 => Loss: 0.2904\n",
      "[+] Ítems cargados: 53 ítems con 12 características cada uno.\n",
      "[+] Users cargados: 698 users con 7 características cada uno.\n",
      "[+] Interactions cargados: 698 interactions con 53 características cada uno.\n",
      "[+] Epoch 1/30 => Loss: 0.7835\n",
      "[+] Epoch 2/30 => Loss: 0.6800\n",
      "[+] Epoch 3/30 => Loss: 0.5856\n",
      "[+] Epoch 4/30 => Loss: 0.5162\n",
      "[+] Epoch 5/30 => Loss: 0.5170\n",
      "[+] Epoch 6/30 => Loss: 0.5318\n",
      "[+] Epoch 7/30 => Loss: 0.5158\n",
      "[+] Epoch 8/30 => Loss: 0.4802\n",
      "[+] Epoch 9/30 => Loss: 0.4551\n",
      "[+] Epoch 10/30 => Loss: 0.4420\n",
      "[+] Epoch 11/30 => Loss: 0.4360\n",
      "[+] Epoch 12/30 => Loss: 0.4298\n",
      "[+] Epoch 13/30 => Loss: 0.4148\n",
      "[+] Epoch 14/30 => Loss: 0.3935\n",
      "[+] Epoch 15/30 => Loss: 0.3741\n",
      "[+] Epoch 16/30 => Loss: 0.3580\n",
      "[+] Epoch 17/30 => Loss: 0.3545\n",
      "[+] Epoch 18/30 => Loss: 0.3470\n",
      "[+] Epoch 19/30 => Loss: 0.3532\n",
      "[+] Epoch 20/30 => Loss: 0.3524\n",
      "[+] Epoch 21/30 => Loss: 0.3560\n",
      "[+] Epoch 22/30 => Loss: 0.3451\n",
      "[+] Epoch 23/30 => Loss: 0.3370\n",
      "[+] Epoch 24/30 => Loss: 0.3274\n",
      "[+] Epoch 25/30 => Loss: 0.3217\n",
      "[+] Epoch 26/30 => Loss: 0.3161\n",
      "[+] Epoch 27/30 => Loss: 0.3162\n",
      "[+] Epoch 28/30 => Loss: 0.3155\n",
      "[+] Epoch 29/30 => Loss: 0.3145\n",
      "[+] Epoch 30/30 => Loss: 0.3133\n"
     ]
    }
   ],
   "source": [
    "#  ██████╗ ███████╗██╗   ██╗ ██████╗ ███╗   ██╗██████╗     ████████╗██╗    ██╗ ██████╗    ████████╗ ██████╗ ██╗    ██╗███████╗██████╗ \n",
    "#  ██╔══██╗██╔════╝╚██╗ ██╔╝██╔═══██╗████╗  ██║██╔══██╗    ╚══██╔══╝██║    ██║██╔═══██╗   ╚══██╔══╝██╔═══██╗██║    ██║██╔════╝██╔══██╗\n",
    "#  ██████╔╝█████╗   ╚████╔╝ ██║   ██║██╔██╗ ██║██║  ██║       ██║   ██║ █╗ ██║██║   ██║█████╗██║   ██║   ██║██║ █╗ ██║█████╗  ██████╔╝\n",
    "#  ██╔══██╗██╔══╝    ╚██╔╝  ██║   ██║██║╚██╗██║██║  ██║       ██║   ██║███╗██║██║   ██║╚════╝██║   ██║   ██║██║███╗██║██╔══╝  ██╔══██╗\n",
    "#  ██████╔╝███████╗   ██║   ╚██████╔╝██║ ╚████║██████╔╝       ██║   ╚███╔███╔╝╚██████╔╝      ██║   ╚██████╔╝╚███╔███╔╝███████╗██║  ██║\n",
    "#  ╚═════╝ ╚══════╝   ╚═╝    ╚═════╝ ╚═╝  ╚═══╝╚═════╝        ╚═╝    ╚══╝╚══╝  ╚═════╝       ╚═╝    ╚═════╝  ╚══╝╚══╝ ╚══════╝╚═╝  ╚═╝\n",
    "\n",
    "# V1\n",
    "recomendador_v5_1 = TwoTowerRecommenderSystem(\n",
    "    Two_Tower_Model=SparseTwoTower, \n",
    "    User_Tower=UserTowerV1,\n",
    "    Item_Tower=ItemTowerV1,\n",
    "    user_input_size=df_train_users.shape[1] - 1,\n",
    "    item_input_size=df_items.shape[1] - 1,\n",
    ")\n",
    "recomendador_v5_1.load_data_train(data_items=df_items, data_users=df_train_users, data_interactions=df_train_interacciones)\n",
    "recomendador_v5_1.train()\n",
    "\n",
    "\n",
    "# V2\n",
    "recomendador_v5_2 = TwoTowerRecommenderSystem(\n",
    "    Two_Tower_Model=SparseTwoTower, \n",
    "    User_Tower=UserTowerV2,\n",
    "    Item_Tower=ItemTowerV2,\n",
    "    user_input_size=df_train_users.shape[1] - 1,\n",
    "    item_input_size=df_items.shape[1] - 1,\n",
    ")\n",
    "recomendador_v5_2.load_data_train(data_items=df_items, data_users=df_train_users, data_interactions=df_train_interacciones)\n",
    "recomendador_v5_2.train()\n",
    "\n",
    "\n",
    "# V3\n",
    "recomendador_v5_3 = TwoTowerRecommenderSystem(\n",
    "    Two_Tower_Model=SparseTwoTower, \n",
    "    User_Tower=UserTowerV3,\n",
    "    Item_Tower=ItemTowerV3,\n",
    "    user_input_size=df_train_users.shape[1] - 1,\n",
    "    item_input_size=df_items.shape[1] - 1,\n",
    ")\n",
    "recomendador_v5_3.load_data_train(data_items=df_items, data_users=df_train_users, data_interactions=df_train_interacciones)\n",
    "recomendador_v5_3.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Evaluation Results => Precision@5: 0.7029, Recall@5: 0.3353, NDCG@5: 0.7638\n",
      "[+] Evaluation Results => Precision@5: 0.7360, Recall@5: 0.3667, NDCG@5: 0.7776\n",
      "[+] Evaluation Results => Precision@5: 0.7474, Recall@5: 0.3791, NDCG@5: 0.7949\n",
      "\n",
      "[+] Evaluation Results => Precision@10: 0.7194, Recall@10: 0.6900, NDCG@10: 0.8269\n",
      "[+] Evaluation Results => Precision@10: 0.7371, Recall@10: 0.7004, NDCG@10: 0.8440\n",
      "[+] Evaluation Results => Precision@10: 0.7149, Recall@10: 0.6799, NDCG@10: 0.8290\n",
      "\n",
      "[+] Evaluation Results => Precision@15: 0.6316, Recall@15: 0.8520, NDCG@15: 0.8504\n",
      "[+] Evaluation Results => Precision@15: 0.6316, Recall@15: 0.8520, NDCG@15: 0.8564\n",
      "[+] Evaluation Results => Precision@15: 0.6316, Recall@15: 0.8520, NDCG@15: 0.8523\n"
     ]
    }
   ],
   "source": [
    "recomendador_v5_1.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=5)\n",
    "recomendador_v5_2.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=5)\n",
    "recomendador_v5_3.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=5)\n",
    "print()\n",
    "recomendador_v5_1.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=10)\n",
    "recomendador_v5_2.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=10)\n",
    "recomendador_v5_3.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=10)\n",
    "print()\n",
    "recomendador_v5_1.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=15)\n",
    "recomendador_v5_2.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=15)\n",
    "recomendador_v5_3.evaluate(val_users=df_validation_users, val_interactions=df_validation_interacciones, k=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "recommended_items_v1_1 = {}\n",
    "recommended_items_v1_2 = {}\n",
    "recommended_items_v1_3 = {}\n",
    "\n",
    "\n",
    "recommended_items_v2_1 = {}\n",
    "recommended_items_v2_2 = {}\n",
    "recommended_items_v2_3 = {}\n",
    "\n",
    "\n",
    "recommended_items_v3_1 = {}\n",
    "recommended_items_v3_2 = {}\n",
    "recommended_items_v3_3 = {}\n",
    "\n",
    "\n",
    "recommended_items_v4_1 = {}\n",
    "recommended_items_v4_2 = {}\n",
    "recommended_items_v4_3 = {}\n",
    "\n",
    "\n",
    "recommended_items_v5_1 = {}\n",
    "recommended_items_v5_2 = {}\n",
    "recommended_items_v5_3 = {}\n",
    "\n",
    "\n",
    "# Iterar sobre todos los usuarios en el conjunto de prueba\n",
    "for user_id in df_test_users['id_estudiante']:\n",
    "    \n",
    "    # Obtener las características del usuario actual\n",
    "    user = df_test_users[df_test_users['id_estudiante'] == user_id]\n",
    "    interacted = [i for i, interaction in enumerate(df_test_interacciones[df_test_interacciones['id_estudiante'] == user_id].iloc[:, 1:].values.flatten()) if interaction == 1]\n",
    "    \n",
    "    # Recomendaciones\n",
    "    recommendations_v1_1 = recomendador_v1_1.recommend(user, interacted, top_k=10)\n",
    "    recommendations_v1_2 = recomendador_v1_2.recommend(user, interacted, top_k=10)\n",
    "    recommendations_v1_3 = recomendador_v1_3.recommend(user, interacted, top_k=10)\n",
    "\n",
    "    recommendations_v2_1 = recomendador_v2_1.recommend(user, interacted, top_k=10)\n",
    "    recommendations_v2_2 = recomendador_v2_2.recommend(user, interacted, top_k=10)\n",
    "    recommendations_v2_3 = recomendador_v2_3.recommend(user, interacted, top_k=10)\n",
    "\n",
    "    recommendations_v3_1 = recomendador_v3_1.recommend(user, interacted, top_k=10)\n",
    "    recommendations_v3_2 = recomendador_v3_2.recommend(user, interacted, top_k=10)\n",
    "    recommendations_v3_3 = recomendador_v3_3.recommend(user, interacted, top_k=10)\n",
    "\n",
    "    recommendations_v4_1 = recomendador_v4_1.recommend(user, interacted, top_k=10)\n",
    "    recommendations_v4_2 = recomendador_v4_2.recommend(user, interacted, top_k=10)\n",
    "    recommendations_v4_3 = recomendador_v4_3.recommend(user, interacted, top_k=10)\n",
    "\n",
    "    recommendations_v5_1 = recomendador_v5_1.recommend(user, interacted, top_k=10)\n",
    "    recommendations_v5_2 = recomendador_v5_2.recommend(user, interacted, top_k=10)\n",
    "    recommendations_v5_3 = recomendador_v5_3.recommend(user, interacted, top_k=10)\n",
    "\n",
    "    # Items recomendados\n",
    "\n",
    "    recommended_items_v1_1[user_id] = [rec[0] for rec in recommendations_v1_1] \n",
    "    recommended_items_v1_2[user_id] = [rec[0] for rec in recommendations_v1_2]\n",
    "    recommended_items_v1_3[user_id] = [rec[0] for rec in recommendations_v1_3]\n",
    "\n",
    "    recommended_items_v2_1[user_id] = [rec[0] for rec in recommendations_v2_1] \n",
    "    recommended_items_v2_2[user_id] = [rec[0] for rec in recommendations_v2_2]\n",
    "    recommended_items_v2_3[user_id] = [rec[0] for rec in recommendations_v2_3]\n",
    "\n",
    "    recommended_items_v3_1[user_id] = [rec[0] for rec in recommendations_v3_1] \n",
    "    recommended_items_v3_2[user_id] = [rec[0] for rec in recommendations_v3_2]\n",
    "    recommended_items_v3_3[user_id] = [rec[0] for rec in recommendations_v3_3]\n",
    "\n",
    "    recommended_items_v4_1[user_id] = [rec[0] for rec in recommendations_v4_1] \n",
    "    recommended_items_v4_2[user_id] = [rec[0] for rec in recommendations_v4_2]\n",
    "    recommended_items_v4_3[user_id] = [rec[0] for rec in recommendations_v4_3]\n",
    "\n",
    "    recommended_items_v5_1[user_id] = [rec[0] for rec in recommendations_v5_1] \n",
    "    recommended_items_v5_2[user_id] = [rec[0] for rec in recommendations_v5_2]\n",
    "    recommended_items_v5_3[user_id] = [rec[0] for rec in recommendations_v5_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage: 0.7170, Novelty: -3.9399, Popularity Bias: 273.5753\n",
      "Coverage: 0.7170, Novelty: -4.8605, Popularity Bias: 280.6068\n",
      "Coverage: 0.7170, Novelty: -4.7358, Popularity Bias: 276.4434\n"
     ]
    }
   ],
   "source": [
    "recomendador_v1_1.evaluate_relevance(\n",
    "    recommended_items=recommended_items_v1_1,\n",
    "    popularity_bias=popularidad_items\n",
    ")\n",
    "\n",
    "recomendador_v1_2.evaluate_relevance(\n",
    "    recommended_items=recommended_items_v1_2,\n",
    "    popularity_bias=popularidad_items\n",
    ")\n",
    "\n",
    "recomendador_v1_3.evaluate_relevance(\n",
    "    recommended_items=recommended_items_v1_3,\n",
    "    popularity_bias=popularidad_items\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage: 0.6038, Novelty: -1.3579, Popularity Bias: 265.7973\n",
      "Coverage: 0.5472, Novelty: 0.2485, Popularity Bias: 248.8973\n",
      "Coverage: 0.5660, Novelty: -0.2651, Popularity Bias: 253.7370\n"
     ]
    }
   ],
   "source": [
    "recomendador_v2_1.evaluate_relevance(\n",
    "    recommended_items=recommended_items_v2_1,\n",
    "    popularity_bias=popularidad_items\n",
    ")\n",
    "\n",
    "recomendador_v2_2.evaluate_relevance(\n",
    "    recommended_items=recommended_items_v2_2,\n",
    "    popularity_bias=popularidad_items\n",
    ")\n",
    "\n",
    "recomendador_v2_3.evaluate_relevance(\n",
    "    recommended_items=recommended_items_v2_3,\n",
    "    popularity_bias=popularidad_items\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage: 0.5849, Novelty: -0.3396, Popularity Bias: 254.0420\n",
      "Coverage: 0.3774, Novelty: 4.0733, Popularity Bias: 30.5146\n",
      "Coverage: 0.7170, Novelty: 10.7912, Popularity Bias: 31.0721\n"
     ]
    }
   ],
   "source": [
    "recomendador_v3_1.evaluate_relevance(\n",
    "    recommended_items=recommended_items_v3_1,\n",
    "    popularity_bias=popularidad_items\n",
    ")\n",
    "\n",
    "recomendador_v3_2.evaluate_relevance(\n",
    "    recommended_items=recommended_items_v3_2,\n",
    "    popularity_bias=popularidad_items\n",
    ")\n",
    "\n",
    "recomendador_v3_3.evaluate_relevance(\n",
    "    recommended_items=recommended_items_v3_3,\n",
    "    popularity_bias=popularidad_items\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage: 0.3585, Novelty: 11.0126, Popularity Bias: 2.2379\n",
      "Coverage: 0.5472, Novelty: -0.8016, Popularity Bias: 259.5658\n",
      "Coverage: 0.3774, Novelty: 0.9374, Popularity Bias: 30.3521\n"
     ]
    }
   ],
   "source": [
    "recomendador_v4_1.evaluate_relevance(\n",
    "    recommended_items=recommended_items_v4_1,\n",
    "    popularity_bias=popularidad_items\n",
    ")\n",
    "\n",
    "recomendador_v4_2.evaluate_relevance(\n",
    "    recommended_items=recommended_items_v4_2,\n",
    "    popularity_bias=popularidad_items\n",
    ")\n",
    "\n",
    "recomendador_v4_3.evaluate_relevance(\n",
    "    recommended_items=recommended_items_v4_3,\n",
    "    popularity_bias=popularidad_items\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage: 0.7358, Novelty: 1.4648, Popularity Bias: 223.2032\n",
      "Coverage: 0.2453, Novelty: 13.3012, Popularity Bias: 0.0000\n",
      "Coverage: 0.3585, Novelty: 10.9551, Popularity Bias: 2.5854\n"
     ]
    }
   ],
   "source": [
    "recomendador_v5_1.evaluate_relevance(\n",
    "    recommended_items=recommended_items_v5_1,\n",
    "    popularity_bias=popularidad_items\n",
    ")\n",
    "\n",
    "recomendador_v5_2.evaluate_relevance(\n",
    "    recommended_items=recommended_items_v5_2,\n",
    "    popularity_bias=popularidad_items\n",
    ")\n",
    "\n",
    "recomendador_v5_3.evaluate_relevance(\n",
    "    recommended_items=recommended_items_v5_3,\n",
    "    popularity_bias=popularidad_items\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".sistema-de-recomendacion-unab-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
