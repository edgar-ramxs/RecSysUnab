# def calculate_score_dataset(df: pd.DataFrame, features: Dict[str, Tuple[float, float]], weights: Dict[str, float], new_column: str) -> pd.DataFrame:
#     """
#     Calcula un score normalizado para cada fila del DataFrame, basado en las características y sus pesos.

#     Parameters:
#     - df (pd.DataFrame): El DataFrame con las características de los ítems.
#     - features (Dict[str, Tuple[float, float]]): Diccionario con las características y sus rangos min, max en el formato {"característica": (min, max)}.
#     - weights (Dict[str, float]): Diccionario con los pesos asignados a cada característica, los cuales deben sumar 1.
#     - new_column (str): Nombre de la columna nueva donde se guardarán los scores calculados.

#     Returns:
#     - pd.DataFrame: El DataFrame con la nueva columna 'score'.
#     """
#     # Función para calcular el score normalizado para cada fila
#     def calculate_score_row(values: Dict[str, float]) -> float:
#         score = 0
#         for feature, (min_val, max_val) in features.items():
#             if feature in values:
#                 # Normaliza el valor de la característica
#                 normalized_value = (values[feature] - min_val) / (max_val - min_val) if max_val != min_val else 0
#                 # Suma la contribución ponderada al score
#                 score += weights.get(feature, 0) * normalized_value
#         return score
    
#     # Aplicar el cálculo de score a cada fila del DataFrame
#     df[new_column] = df.apply(
#         lambda row: calculate_score_row({feature: row[feature] for feature in features}),
#         axis=1
#     )
    
#     return df








20.911.861-0




https://medium.com/@lifengyi_6964/multimodal-and-large-language-model-recommendation-system-awesome-paper-list-a05e5fd81a79

# Recommender Systems in the Era of Large Language Models  

- [Articulo](https://arxiv.org/pdf/2307.02046)
- [Publicacion](https://www.linkedin.com/pulse/trends-recsys-two-towers-llms-vijay-raghavan-ph-d-m-b-a-/)


# Scaling deep retrieval with TensorFlow Recommenders and Vertex AI Matching Engine
- [ENLACE](https://cloud.google.com/blog/products/ai-machine-learning/scaling-deep-retrieval-tensorflow-two-towers-architecture)
- https://developers.google.com/machine-learning/recommendation/overview/candidate-generation?hl=es-419#embedding-space

# Introducing TensorFlow Recommenders

- [Artiuclo](https://blog.tensorflow.org/2020/09/introducing-tensorflow-recommenders.html?hl=es-419&_gl=1*1874wvg*_ga*NzE5MDE3NjczLjE3MTI5NDc0Mjk.*_ga_W0YLR4190T*MTcxODMzMjAzOS4xOS4xLjE3MTgzMzIyMzIuMC4wLjA.)



https://medium.com/@eng.saavedra/sistemas-de-recomendaci%C3%B3n-parte-5-modelos-complejos-c8db70e8b7b

RecSysUnab.py --exers [0,1,0,0,0,1....50] --id n 

https://medium.com/@eng.saavedra/sistemas-de-recomendaci%C3%B3n-parte-5-modelos-complejos-c8db70e8b7b
https://towardsdatascience.com/two-tower-networks-and-negative-sampling-in-recommender-systems-fdc88411601b

# SURPRISE
https://monirah-abdulaziz.medium.com/building-movie-recommendation-system-with-surprise-and-python-e905de755c61

# MATRIX factorization
https://medium.com/@chenycy/build-recommendation-systems-openais-embeddings-matrix-factorization-and-deep-learning-0cac62008f0c

https://actsusanli.medium.com/building-a-recommender-system-with-implicit-feedback-datasets-using-alternating-least-squares-64d4f5ba3c57


https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Building%20Recommender%20System%20with%20Surprise.ipynb
https://towardsdatascience.com/building-and-testing-recommender-systems-with-surprise-step-by-step-d4ba702ef80b
https://nbviewer.org/github/NicolasHug/Surprise/blob/master/examples/notebooks/KNNBasic_analysis.ipynb


https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Collaborative%20Filtering%20Model%20with%20TensorFlow.ipynb
https://towardsdatascience.com/building-a-collaborative-filtering-recommender-system-with-tensorflow-82e63d27b420


https://arxiv.org/abs/cs/0702144


https://medium.com/@eng.saavedra/sistemas-de-recomendaci%C3%B3n-parte-4-evaluaciones-cfd1f96b887a


model_combined_score_3 = tf.keras.models.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(x_train_combined_score_3.shape[1],)),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(1, activation='linear')
])

model_combined_score_3.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error')
model_combined_score_3.summary()

history_combined_score_3 = model_combined_score_3.fit(x_train_combined_score_3, y_train_combined_score_3, epochs=50, batch_size=32, validation_split=0.2, verbose=1)

print()
loss_combined_score_3 = model_combined_score_3.evaluate(x_test_combined_score_3, y_test_combined_score_3)
print(f'Loss en el conjunto de prueba: {loss_combined_score_3}')
predictions_combined_score_3 = model_combined_score_3.predict(x_test_combined_score_3)









from sklearn.metrics import precision_score, recall_score

# Supongamos que tienes las siguientes listas:
# etiquetas_reales: una lista que indica qué ítems son relevantes (1 si es relevante, 0 si no).
# recomendaciones: una lista que contiene las recomendaciones del modelo (1 si el ítem fue recomendado, 0 si no).

# Ejemplo de etiquetas reales y recomendaciones
# Esto debería ser un resultado de tu evaluación del modelo
# Simulación de etiquetas reales para un conjunto de usuarios
etiquetas_reales = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0])  # Etiquetas de relevancia
recomendaciones = np.array([1, 0, 0, 1, 1, 0, 0, 0, 1, 1])  # Recomendaciones del modelo

# Calcular precisión y recall
precision = precision_score(etiquetas_reales, recomendaciones)
recall = recall_score(etiquetas_reales, recomendaciones)

print(f"Precisión: {precision:.4f}")
print(f"Recall: {recall:.4f}")








    def recommend(self, user_features: list[int], user_interacted_items: list[int], top_n: int = 10) -> list[tuple]:
        # Convertir características del usuario a un tensor
        user_input = torch.tensor(user_features).float().unsqueeze(0)

        # Poner el modelo en modo de evaluación
        self.model.eval()

        # Obtener las puntuaciones de todos los ítems
        with torch.no_grad():
            item_scores = self.model(user_input, self.items_inputs)

        # Asignar puntuaciones bajas a los ítems ya interactuados
        item_scores[user_interacted_items] = - \
            float('inf')  # Máscara lógica para filtrar

        # Obtener los índices de los top_n ítems con mayores puntuaciones
        top_indices = torch.topk(item_scores, top_n).indices.squeeze().tolist()
        scores = item_scores[top_indices].tolist()

        # Crear una lista de tuplas (índice del ítem, puntuación)
        recomendaciones = list(zip(top_indices, scores))
        return recomendaciones










import tensorflow as tf
from tensorflow.keras.layers import Embedding, Dense, Flatten, Input, Concatenate, Dot
from tensorflow.keras.models import Model

# Supongamos que tenemos los siguientes datos
num_users = 100  # Número de usuarios
num_exercises = 50  # Número de ejercicios
embedding_dim = 16  # Dimensión de los embeddings

# Torre de Usuario
user_career_input = Input(shape=(1,), name="user_career")  # Carrera
user_exercises_input = Input(shape=(num_exercises,), name="user_exercises")  # Ejercicios completados (binario)
user_scores_input = Input(shape=(3,), name="user_scores")  # Puntajes A, B y C

# Embedding de carrera
user_career_embedding = Embedding(input_dim=num_users, output_dim=embedding_dim)(user_career_input)
user_career_embedding = Flatten()(user_career_embedding)

# Concatenamos todas las características del usuario
user_features = Concatenate()([user_career_embedding, user_exercises_input, user_scores_input])

# Pasamos las características de usuario por una capa densa
user_dense = Dense(embedding_dim, activation="relu")(user_features)

# Torre de Ejercicio
exercise_hito_input = Input(shape=(1,), name="exercise_hito")  # Hito
exercise_knowledge_input = Input(shape=(1,), name="exercise_knowledge")  # Conocimiento
exercise_skill_input = Input(shape=(1,), name="exercise_skill")  # Habilidad

# Embeddings de hito, conocimiento y habilidad
exercise_hito_embedding = Embedding(input_dim=10, output_dim=embedding_dim)(exercise_hito_input)
exercise_knowledge_embedding = Embedding(input_dim=10, output_dim=embedding_dim)(exercise_knowledge_input)
exercise_skill_embedding = Embedding(input_dim=10, output_dim=embedding_dim)(exercise_skill_input)

# Aplanamos y concatenamos todas las características de ejercicio
exercise_features = Concatenate()([
    Flatten()(exercise_hito_embedding),
    Flatten()(exercise_knowledge_embedding),
    Flatten()(exercise_skill_embedding)
])

# Pasamos las características del ejercicio por una capa densa
exercise_dense = Dense(embedding_dim, activation="relu")(exercise_features)

# Cálculo de similitud entre usuario y ejercicio (producto punto)
similarity = Dot(axes=1)([user_dense, exercise_dense])

# Modelo de recomendación completo
model = Model(inputs=[
    user_career_input, user_exercises_input, user_scores_input,
    exercise_hito_input, exercise_knowledge_input, exercise_skill_input
], outputs=similarity)

model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])

# Resumen del modelo
model.summary()


# Ejemplo de datos de entrenamiento
import numpy as np

# IDs ficticios para entrenamiento
user_careers = np.random.randint(0, num_users, size=1000)
user_exercises = np.random.randint(0, 2, size=(1000, num_exercises))  # Lista binaria de ejercicios hechos
user_scores = np.random.randint(0, 10, size=(1000, 3))  # Puntajes A, B y C
exercise_hitos = np.random.randint(0, 10, size=1000)
exercise_knowledge = np.random.randint(0, 10, size=1000)
exercise_skill = np.random.randint(0, 10, size=1000)
labels = np.random.randint(0, 2, size=1000)  # 1 si el usuario hizo el ejercicio, 0 si no

# Entrenar el modelo
model.fit(
    [user_careers, user_exercises, user_scores, exercise_hitos, exercise_knowledge, exercise_skill],
    labels,
    epochs=10,
    batch_size=32
)

# Datos del usuario
user_career = np.array([2])  # ID de la carrera del usuario
user_exercises = np.array([[1, 0, 1, 0, ..., 1]])  # Lista binaria de ejercicios realizados (asegúrate que sea del tamaño correcto)
user_scores = np.array([[5, 7, 3]])  # Puntajes A, B y C del usuario

# Datos del ejercicio
exercise_hito = np.array([3])  # Hito del ejercicio
exercise_knowledge = np.array([2])  # Conocimiento requerido
exercise_skill = np.array([4])  # Habilidad requerida

# Hacer la predicción de afinidad
prediction = model.predict([user_career, user_exercises, user_scores, exercise_hito, exercise_knowledge, exercise_skill])

print("Afinidad usuario-ejercicio:", prediction[0][0])





from sklearn.metrics.pairwise import cosine_similarity

# Ejemplo de perfil de un nuevo usuario
new_user_profile = np.array([[2, 5, 7, 3]])  # [carrera, puntaje A, puntaje B, puntaje C]

# Perfiles de ejercicios (matriz de n_ejercicios x características)
exercise_profiles = np.array([
    [3, 4, 6, 2],  # Ejercicio 1
    [2, 5, 7, 3],  # Ejercicio 2, etc.
    ...
])

# Calcular la similitud entre el nuevo usuario y todos los ejercicios
similarity_scores = cosine_similarity(new_user_profile, exercise_profiles)
recommended_exercise_indices = similarity_scores.argsort()[0][-5:]  # Índices de los 5 ejercicios más similares


# Ejemplo de lista de ejercicios y su popularidad
exercises_popularity = {
    "Ejercicio A": 150,
    "Ejercicio B": 125,
    "Ejercicio C": 100,
    ...
}

# Ordenar ejercicios por popularidad y seleccionar los más altos
popular_exercises = sorted(exercises_popularity, key=exercises_popularity.get, reverse=True)[:5]
print("Recomendación de ejercicios populares:", popular_exercises)



# Supongamos que `content_based_recommendations` es la lista obtenida de la estrategia 2
# Y `popular_exercises` es la lista de la estrategia 3

def hybrid_recommendation(user_profile, top_n=5):
    if is_new_user(user_profile):
        # Para nuevos usuarios, mezclamos basados en contenido y popularidad
        recommendations = popular_exercises[:top_n] + content_based_recommendations[:top_n]
    else:
        # Para usuarios con historial, usamos el modelo de dos torres
        recommendations = model.predict(user_profile)[:top_n]
    
    return recommendations[:top_n]

# Ejemplo de uso
recommendations = hybrid_recommendation(new_user_profile)
print("Recomendaciones híbridas:", recommendations)




import numpy as np
import tensorflow as tf
from keras.layers import Embedding, Dense, Flatten
from keras.models import Model
from keras.layers import Input, Concatenate

# Datos simulados
num_users = 1000    # Número de usuarios
num_movies = 500    # Número de películas
embedding_dim = 32  # Dimensión de los embeddings

# Datos de usuario y película
user_ids = np.random.randint(0, num_users, size=10000)
movie_ids = np.random.randint(0, num_movies, size=10000)
labels = np.random.randint(0, 2, size=10000)  # 1 si le gustó, 0 si no


# Datos simulados
num_users = 1000    # Número de usuarios
num_movies = 500    # Número de películas
embedding_dim = 32  # Dimensión de los embeddings

# Datos de usuario y película
user_ids = np.random.randint(0, num_users, size=10000)
movie_ids = np.random.randint(0, num_movies, size=10000)
labels = np.random.randint(0, 2, size=10000)  # 1 si le gustó, 0 si no

labels

# Torre para usuarios
user_input = Input(shape=(1,), name="user_id")
user_embedding = Embedding(input_dim=num_users, output_dim=embedding_dim, name="user_embedding")(user_input)
user_embedding = Flatten()(user_embedding)

# Torre para películas
movie_input = Input(shape=(1,), name="movie_id")
movie_embedding = Embedding(input_dim=num_movies, output_dim=embedding_dim, name="movie_embedding")(movie_input)
movie_embedding = Flatten()(movie_embedding)

# Similaridad entre usuario y película (producto punto)
dot_product = tf.keras.layers.Dot(axes=1)([user_embedding, movie_embedding])

# Modelo completo
model = Model(inputs=[user_input, movie_input], outputs=dot_product)
model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])

# Entrenamiento del modelo
model.fit([user_ids, movie_ids], labels, epochs=10, batch_size=64)

# Predicción de la preferencia de un usuario para una película específica
user_id = np.array([4])  # ID del usuario
movie_id = np.array([10])  # ID de la película

predicted_preference = model.predict([user_id, movie_id])
print(f"Predicción de que al usuario {user_id[0]} le gustará la película {movie_id[0]}: {predicted_preference[0][0]}")






model_combined_score_1 = tf.keras.models.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(x_train_combined_score_1.shape[1],)),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(1, activation='linear')
])

model_combined_score_1.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error')
model_combined_score_1.summary()

history_combined_score_1 = model_combined_score_1.fit(x_train_combined_score_1, y_train_combined_score_1, epochs=50, batch_size=32, validation_split=0.2, verbose=1)

print()
loss_combined_score_1 = model_combined_score_1.evaluate(x_test_combined_score_1, y_test_combined_score_1)
print(f'Loss en el conjunto de prueba: {loss_combined_score_1}')
predictions_combined_score_1 = model_combined_score_1.predict(x_test_combined_score_1)

































# # Crear el modelo
# model = TwoTowerModel(user_feature_size, item_feature_size, embedding_size)

# # Definir el optimizador y la función de pérdida
# optimizer = optim.Adam(model.parameters(), lr=0.001)
# criterion = nn.BCELoss()

# # Extraer las características de los DataFrames y convertirlas en tensores
# item_input = torch.tensor(df_items.iloc[:, 1:].values).float()  # Datos de ejercicios
# user_input = torch.tensor(df_users.iloc[:, 1:].values).float()  # Datos de usuarios

# # Etiquetas simuladas (si el ejercicio debe recomendarse o no)
# labels = torch.randint(0, 2, (len(df_users),)).float()

# # Entrenar el modelo
# epochs = 10
# for epoch in range(epochs):
#     optimizer.zero_grad()
    
#     # Pasar las entradas a través del modelo
#     output = model(user_input, item_input)
    
#     # Calcular la pérdida
#     loss = criterion(output, labels)
    
#     # Retropropagación
#     loss.backward()
    
#     # Actualizar los pesos
#     optimizer.step()
    
#     print(f"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}")



# def recommend_for_user(model, user_features, item_features, top_n=10):
#     """
#     Recomendaciones para un solo usuario basado en sus características.

#     Args:
#         model: El modelo de recomendación preentrenado.
#         user_features: Un array con las características del usuario.
#         item_features: Un tensor con las características de todos los ítems.
#         top_n: Número de recomendaciones a devolver.

#     Returns:
#         recomendaciones: Índices de los ítems recomendados.
#         scores: Puntuaciones de recomendación.
#     """
#     # Asegurarse de que las características del usuario son un tensor y de la forma correcta
#     user_input = torch.tensor(user_features).float().unsqueeze(0)  # Añadir una dimensión para el batch
    
#     # Expandir las características del ítem para tener las combinaciones usuario-ítem
#     num_items = item_features.size(0)
#     item_input_expanded = item_features.repeat(user_input.size(0), 1)  # Repetir las características de los ítems para el usuario
    
#     # Poner el modelo en modo de evaluación
#     model.eval()
    
#     # Obtener las puntuaciones de todos los ítems
#     with torch.no_grad():
#         item_scores = model(user_input, item_input_expanded)
    
#     # Obtener los índices de los top_n ítems
#     top_indices = torch.topk(item_scores, top_n).indices.squeeze().tolist()
#     scores = item_scores[top_indices].tolist()
    
#     return top_indices, scores

































# import tensorflow as tf
# import tensorflow_recommenders as tfrs
# import pandas as pd

# # Supongamos que el DataFrame tiene las siguientes columnas: 'id_user', 'id_item', 'title_item', 'ranking_item'
# data = pd.DataFrame({
#     "id_user": ["user1", "user2", "user3", "user42"],
#     "id_item": ["item1", "item2", "item3", "item42"],
#     "title_item": ["Movie A", "Movie B", "Movie C", "Movie D"],
#     "ranking_item": [5, 3, 4, 2]
# })

# # Convertimos el DataFrame a un tf.data.Dataset
# ratings = tf.data.Dataset.from_tensor_slices({
#     "title_item": data["title_item"].values,
#     "id_user": data["id_user"].values,
# })

# # Configuramos el modelo Two Tower
# class TwoTowerRecommendationModel(tfrs.Model):
#     def __init__(self):
#         super().__init__()
#         embedding_dim = 32

#         # Modelo para el usuario
#         self.user_model = tf.keras.Sequential([
#             tf.keras.layers.experimental.preprocessing.StringLookup(
#                 max_tokens=data["id_user"].nunique()),
#             tf.keras.layers.Embedding(data["id_user"].nunique() + 1, embedding_dim)
#         ])

#         # Modelo para el ítem
#         self.item_model = tf.keras.Sequential([
#             tf.keras.layers.experimental.preprocessing.StringLookup(
#                 max_tokens=data["title_item"].nunique()),
#             tf.keras.layers.Embedding(data["title_item"].nunique() + 1, embedding_dim)
#         ])

#         # Definimos la tarea de recuperación con métricas de top-k
#         self.task = tfrs.tasks.Retrieval(
#             metrics=tfrs.metrics.FactorizedTopK(
#                 candidates=ratings.batch(128).map(self.item_model)
#             )
#         )

#     def compute_loss(self, features, training=False):
#         user_embeddings = self.user_model(features["id_user"])
#         item_embeddings = self.item_model(features["title_item"])
#         return self.task(user_embeddings, item_embeddings)

# # Entrenamiento del modelo
# model = TwoTowerRecommendationModel()
# model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))
# model.fit(ratings.batch(128), epochs=5, verbose=1)

# # Indexación para recomendaciones
# index = tfrs.layers.ann.BruteForce(model.user_model)
# index.index(ratings.batch(128).map(model.item_model), ratings)

# # Obtener recomendaciones para un usuario específico
# _, recommended_titles = index(tf.constant(["user42"]))
# print(f"Recommendations for user 'user42': {recommended_titles[0, :3]}")



























































# # def dividir_data_entrenamiento(df_usuarios: pd.DataFrame, matrix_factorization: pd.DataFrame, id_columna: str = 'id_estudiante'):
    

# #     return 0


# # ejercicios = dataset.query(" `ejercicios_hito1` == -1 and `ejercicios_hito2` == -1 and `ejercicios_hito3` == -1 and `ejercicios_hito4` == -1")
# # dataset.drop(ejercicios.index, inplace=True)
# # print(dataset.shape)

# # solemnes = dataset.query(" `solemne_1` < 4.0 and `solemne_2` < 4.0 and `solemne_3` < 4.0 and `solemne_4` < 4.0 ")
# # dataset.drop(solemnes.index, inplace=True)
# # print(dataset.shape)





# # 'Abstraction1', 'Abstraction2', 'Abstraction2.1', 
# # 'Decomposition1', 'Decomposition2', 'Decomposition3', 'Decomposition4'
# # 'Pattern_Recognition_and_Generalization1', 'Pattern_Recognition_and_Generalization2', 'Pattern_Recognition_and_Generalization3', 'Pattern_Recognition_and_Generalization4'

# df_eval[['ï»¿codigo', 'id', 'titulo', 'Dificulty', 'Algorithmic_thinking2', 'Algorithmic_thinking3', 'Algorithmic_thinking4', 'Algorithmic_thinking5']]

# df_catalogo 

# round((1.50/4)*15)









# scaler = MinMaxScaler()
# columns_to_normalize = ['hito', 'skill', 'knowledge', 'puntos', 'dificultad','score_a', 'score_d', 'score_p', 'score_s']
# df_catalogo[columns_to_normalize] = scaler.fit_transform(df_catalogo[columns_to_normalize])
# df_catalogo[['id_ejercicio', 'nombre', 'hito', 'skill', 'knowledge', 'puntos', 'dificultad', 'score_a', 'score_d', 'score_p', 'score_s', 'ratio_interaccion']].head()











@Article{electronics10141611,
    AUTHOR = {Urdaneta-Ponte, María Cora and Mendez-Zorrilla, Amaia and Oleagordia-Ruiz, Ibon},
    TITLE = {Recommendation Systems for Education: Systematic Review},
    JOURNAL = {Electronics},
    VOLUME = {10},
    YEAR = {2021},
    NUMBER = {14},
    ARTICLE-NUMBER = {1611},
    URL = {https://www.mdpi.com/2079-9292/10/14/1611},
    ISSN = {2079-9292},
    ABSTRACT = {Recommendation systems have emerged as a response to overload in terms of increased amounts of information online, which has become a problem for users regarding the time spent on their search and the amount of information retrieved by it. In the field of recommendation systems in education, the relevance of recommended educational resources will improve the student’s learning process, and hence the importance of being able to suitably and reliably ensure relevant, useful information. The purpose of this systematic review is to analyze the work undertaken on recommendation systems that support educational practices with a view to acquiring information related to the type of education and areas dealt with, the developmental approach used, and the elements recommended, as well as being able to detect any gaps in this area for future research work. A systematic review was carried out that included 98 articles from a total of 2937 found in main databases (IEEE, ACM, Scopus and WoS), about which it was able to be established that most are geared towards recommending educational resources for users of formal education, in which the main approaches used in recommendation systems are the collaborative approach, the content-based approach, and the hybrid approach, with a tendency to use machine learning in the last two years. Finally, possible future areas of research and development in this field are presented.},
    DOI = {10.3390/electronics10141611}
}

@InProceedings{10.1007/978-3-319-73450-7_89,
    author="Rivera, Abdon Carrera
    and Tapia-Leon, Mariela
    and Lujan-Mora, Sergio",
    editor="Rocha, {\'A}lvaro
    and Guarda, Teresa",
    title="Recommendation Systems in Education: A Systematic Mapping Study",
    booktitle="Proceedings of the International Conference on Information Technology {\&} Systems (ICITS 2018)",
    year="2018",
    publisher="Springer International Publishing",
    address="Cham",
    pages="937--947",
    abstract="Several researchers study recommendation systems to assist users in the retrieval of relevant goods and services, mostly used in e-commerce. However, there is limited information of the impact of recommender systems in other domains like education. Thus, the objective of this study is to summarize the current knowledge that is available as regards recommendation systems that have been employed within the education domain to support educational practices. By performing a systematic mapping study, a total of 44 research papers have been selected, reviewed and analyzed from an initial set of 1181 papers. Our results provide some findings regarding how recommendation systems can be used to support main areas in education, what approaches techniques or algorithms recommender systems use and how they address different issues in the academic world. Moreover, this work has also been useful to detect some research gaps and key areas where further investigation should be performed, like the introduction of data mining and artificial intelligence in recommender system algorithms to improve personalization of academic choices.",
    isbn="978-3-319-73450-7"
}

@article{DBLP:journals/corr/abs-2101-06286,
    author = {Mohammad Mehdi Afsar and Trafford Crump and Behrouz H. Far},
    title = {Reinforcement learning based recommender systems: {A} survey},
    journal = {CoRR},
    volume = {abs/2101.06286},
    year = {2021},
    url = {https://arxiv.org/abs/2101.06286},
    eprinttype = {arXiv},
    eprint = {2101.06286},
    timestamp = {Tue, 07 Sep 2021 16:26:03 +0200},
    biburl = {https://dblp.org/rec/journals/corr/abs-2101-06286.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{daSilva2023,
    title = {A systematic literature review on educational recommender systems for teaching and learning: research trends, limitations and opportunities},
    author = {da Silva, Felipe Leite and Slodkowski, Bruna Kin and da Silva, Ketia Kellen Ara{\'u}jo and Cazella, S{\'i}lvio C{\'e}sar},
    journal = {Education and Information Technologies},
    volume = {28},
    number = {3},
    pages = {3289--3328},
    year = {2023},
    month = {Mar},
    issn = {1573-7608},
    doi = {10.1007/s10639-022-11341-9},
    url = {https://doi.org/10.1007/s10639-022-11341-9},
    abstract = {Recommender systems have become one of the main tools for personalized content filtering in the educational domain. Those who support teaching and learning activities, particularly, have gained increasing attention in the past years. This growing interest has motivated the emergence of new approaches and models in the field, in spite of it, there is a gap in literature about the current trends on how recommendations have been produced, how recommenders have been evaluated as well as what are the research limitations and opportunities for advancement in the field. In this regard, this paper reports the main findings of a systematic literature review covering these four dimensions. The study is based on the analysis of a set of primary studies (N = 16 out of 756, published from 2015 to 2020) included according to defined criteria. Results indicate that the hybrid approach has been the leading strategy for recommendation production. Concerning the purpose of the evaluation, the recommenders were evaluated mainly regarding the quality of accuracy and a reduced number of studies were found that investigated their pedagogical effectiveness. This evidence points to a potential research opportunity for the development of multidimensional evaluation frameworks that effectively support the verification of the impact of recommendations on the teaching and learning process. Also, we identify and discuss main limitations to clarify current difficulties that demand attention for future research.}
}