{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "255\n",
      "8\n",
      "273\n",
      "2303\n",
      "514\n"
     ]
    }
   ],
   "source": [
    "c = '00000001'\n",
    "print(int(str(c), 2))\n",
    "\n",
    "c = '11111111'\n",
    "print(int(str(c), 2))\n",
    "\n",
    "c = '1000'\n",
    "print(int(str(c), 2))\n",
    "\n",
    "\n",
    "# 1\t0\t0\t0\t1\t1\t0\t0\t0\t1\t1\t1\n",
    "minimo = '000100010001'\n",
    "maximo = '100011111111'\n",
    "print(int(str(minimo), 2))\n",
    "print(int(str(maximo), 2))\n",
    "\n",
    "c = '001000000010'\n",
    "print(int(str(c), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from learntools.time_series.utils import plot_periodogram, seasonal_plot\n",
    "from learntools.time_series.style import *\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown, display\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "# Function to display markdown\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "# Define directory and read data\n",
    "comp_dir = Path('../input/amazon-product-reviews')\n",
    "electronics_data = pd.read_csv(\n",
    "    comp_dir / \"ratings_Electronics (1).csv\",\n",
    "    dtype={'rating': 'int8'},\n",
    "    names=['userId', 'productId', 'rating', 'timestamp'],\n",
    "    index_col=None,\n",
    "    header=0\n",
    ")\n",
    "\n",
    "# Display some basic information\n",
    "printmd(f\"Number of Rating: {electronics_data.shape[0]:,}\")\n",
    "printmd(f\"Columns: {np.array2string(electronics_data.columns.values)}\")\n",
    "printmd(f\"Number of Users: {len(electronics_data.userId.unique()):,}\")\n",
    "printmd(f\"Number of Products: {len(electronics_data.productId.unique()):,}\")\n",
    "electronics_data.describe()['rating'].reset_index()\n",
    "\n",
    "# Check for missing values\n",
    "printmd('**Number of missing values**:')\n",
    "pd.DataFrame(\n",
    "    electronics_data.isnull().sum().reset_index()\n",
    ").rename(columns={0: \"Total missing\", \"index\": \"Columns\"})\n",
    "\n",
    "# Process data by date\n",
    "data_by_date = electronics_data.copy()\n",
    "data_by_date.timestamp = pd.to_datetime(electronics_data.timestamp, unit=\"s\")\n",
    "data_by_date = data_by_date.sort_values(by=\"timestamp\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "printmd(\"Number of Ratings each day:\")\n",
    "data_by_date.groupby(\"timestamp\")[\"rating\"].count().tail(10).reset_index()\n",
    "\n",
    "# Add year and month columns\n",
    "data_by_date[\"year\"] = data_by_date.timestamp.dt.year\n",
    "data_by_date[\"month\"] = data_by_date.timestamp.dt.month\n",
    "rating_by_year = data_by_date.groupby([\"year\", \"month\"])[\"rating\"].count().reset_index()\n",
    "\n",
    "# Create date column and plot data\n",
    "rating_by_year[\"date\"] = pd.to_datetime(rating_by_year[\"year\"].astype(str) + \"-\" + rating_by_year[\"month\"].astype(str) + \"-1\")\n",
    "rating_by_year.plot(x=\"date\", y=\"rating\")\n",
    "plt.title(\"Number of Ratings over Years\")\n",
    "plt.show()\n",
    "\n",
    "# Group by product and calculate statistics\n",
    "rating_by_product = electronics_data.groupby(\"productId\").agg({\n",
    "    \"userId\": \"count\",\n",
    "    \"rating\": \"mean\"\n",
    "}).rename(columns={\"userId\": \"Number of Ratings\", \"rating\": \"Average Rating\"}).reset_index()\n",
    "\n",
    "# Filter top-rated products\n",
    "cutoff = 50\n",
    "top_rated = rating_by_product.loc[\n",
    "    rating_by_product[\"Number of Ratings\"] > cutoff\n",
    "].sort_values(by=\"Average Rating\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Define TensorFlow model\n",
    "class RankingModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        embedding_dimension = 32\n",
    "\n",
    "        # Embedding layers for users and products\n",
    "        self.user_embeddings = tf.keras.Sequential([\n",
    "            tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=unique_userIds, mask_token=None),\n",
    "            tf.keras.layers.Embedding(len(unique_userIds) + 1, embedding_dimension)\n",
    "        ])\n",
    "\n",
    "        self.product_embeddings = tf.keras.Sequential([\n",
    "            tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=unique_productIds, mask_token=None),\n",
    "            tf.keras.layers.Embedding(len(unique_productIds) + 1, embedding_dimension)\n",
    "        ])\n",
    "\n",
    "        # Ratings layers\n",
    "        self.ratings = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "\n",
    "    def call(self, userId, productId):\n",
    "        user_embeddings = self.user_embeddings(userId)\n",
    "        product_embeddings = self.product_embeddings(productId)\n",
    "        return self.ratings(tf.concat([user_embeddings, product_embeddings], axis=1))\n",
    "\n",
    "class amazonModel(tfrs.models.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ranking_model = RankingModel()\n",
    "        self.task = tfrs.tasks.Ranking(\n",
    "            loss=tf.keras.losses.MeanSquaredError(),\n",
    "            metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, features, training=False):\n",
    "        rating_predictions = self.ranking_model(features[\"userId\"], features[\"productId\"])\n",
    "        return self.task(labels=features[\"rating\"], predictions=rating_predictions)\n",
    "\n",
    "# Filter recent data\n",
    "cutoff_no_rat = 50\n",
    "cutoff_year = 2011\n",
    "recent_data = data_by_date.loc[data_by_date[\"year\"] > cutoff_year]\n",
    "print(f\"Number of Rating: {recent_data.shape[0]:,}\")\n",
    "print(f\"Number of Users: {len(recent_data.userId.unique()):,}\")\n",
    "print(f\"Number of Products: {len(recent_data.productId.unique()):,}\")\n",
    "del data_by_date  # Free up memory\n",
    "\n",
    "recent_prod = recent_data.loc[\n",
    "    recent_data.groupby(\"productId\")[\"rating\"].transform('count').ge(cutoff_no_rat)\n",
    "].reset_index(drop=True).drop([\"timestamp\", \"year\", \"month\"], axis=1)\n",
    "del recent_data  # Free up memory\n",
    "\n",
    "# Prepare data for training\n",
    "userIds = recent_prod.userId.unique()\n",
    "productIds = recent_prod.productId.unique()\n",
    "total_ratings = len(recent_prod.index)\n",
    "\n",
    "ratings = tf.data.Dataset.from_tensor_slices({\n",
    "    \"userId\": tf.cast(recent_prod.userId.values, tf.string),\n",
    "    \"productId\": tf.cast(recent_prod.productId.values, tf.string),\n",
    "    \"rating\": tf.cast(recent_prod.rating.values, tf.int8)\n",
    "})\n",
    "\n",
    "# Shuffle and split data\n",
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "train = shuffled.take(int(total_ratings * 0.8))\n",
    "test = shuffled.skip(int(total_ratings * 0.8)).take(int(total_ratings * 0.2))\n",
    "\n",
    "unique_productIds = productIds\n",
    "unique_userIds = userIds\n",
    "\n",
    "# Compile and train model\n",
    "model = amazonModel()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n",
    "\n",
    "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
    "cached_test = test.batch(4096).cache()\n",
    "model.fit(cached_train, epochs=10)\n",
    "\n",
    "# Evaluate model\n",
    "model.evaluate(cached_test, return_dict=True)\n",
    "\n",
    "# Recommend products for a random user\n",
    "user_rand = userIds[123]\n",
    "test_rating = {}\n",
    "for m in test.take(5):\n",
    "    test_rating[m[\"productId\"].numpy()] = RankingModel()(\n",
    "        tf.convert_to_tensor([user_rand]), tf.convert_to_tensor([m[\"productId\"]])\n",
    "    )\n",
    "\n",
    "print(f\"Top 5 recommended products for User {user_rand}:\")\n",
    "for m in sorted(test_rating, key=test_rating.get, reverse=True):\n",
    "    print(m.decode())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los sistemas de calificación ponderada se utilizan para puntuar la calificación de cada película. Esta es la fórmula de la puntuación ponderada.\n",
    "WR = (v ÷ (v+m)) × R + (m ÷ (v+m)) × C\n",
    "R es la puntuación media del artículo.\n",
    "v es el número de votos del artículo.\n",
    "m es el mínimo de votos necesarios para figurar en los artículos populares (definido por > percentil 80 del total de votos).\n",
    "C es la valoración media de todo el conjunto de datos.\n",
    "\n",
    "\n",
    "\n",
    "Vemos que cada método tiene su punto fuerte. Lo mejor sería poder combinar todos esos puntos fuertes y ofrecer una recomendación mejor. Esta idea nos lleva a otra mejora de la recomendación, que es el método híbrido. Por ejemplo, podemos combinar las recomendaciones de filtrado colaborativo basadas en el contenido y en los elementos para aprovechar las características de ambos dominios (géneros e interacción usuario-elemento).\n",
    "\n",
    "Traducción realizada con la versión gratuita del traductor DeepL.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.externals import joblib\n",
    "import scipy.sparse\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "\n",
    "\n",
    "electronics_data=pd.read_csv(\"/kaggle/input/amazon-product-reviews/ratings_Electronics (1).csv\",names=['userId', 'productId','Rating','timestamp'])\n",
    "\n",
    "\n",
    "# Display the data\n",
    "\n",
    "electronics_data.head()\n",
    "\n",
    "#Shape of the data\n",
    "electronics_data.shape\n",
    "\n",
    "#Taking subset of the dataset\n",
    "electronics_data=electronics_data.iloc[:1048576,0:]\n",
    "\n",
    "#Check the datatypes\n",
    "electronics_data.dtypes\n",
    "\n",
    "\n",
    "electronics_data.info()\n",
    "\n",
    "\n",
    "\n",
    "#Five point summary \n",
    "\n",
    "electronics_data.describe()['Rating'].T\n",
    "\n",
    "\n",
    "#Find the minimum and maximum ratings\n",
    "print('Minimum rating is: %d' %(electronics_data.Rating.min()))\n",
    "print('Maximum rating is: %d' %(electronics_data.Rating.max()))\n",
    "\n",
    "#Check for missing values\n",
    "print('Number of missing values across columns: \\n',electronics_data.isnull().sum())\n",
    "\n",
    "\n",
    "# Check the distribution of the rating\n",
    "with sns.axes_style('white'):\n",
    "    g = sns.factorplot(\"Rating\", data=electronics_data, aspect=2.0,kind='count')\n",
    "    g.set_ylabels(\"Total number of ratings\")\n",
    "\n",
    "\n",
    "print(\"Total data \")\n",
    "print(\"-\"*50)\n",
    "print(\"\\nTotal no of ratings :\",electronics_data.shape[0])\n",
    "print(\"Total No of Users   :\", len(np.unique(electronics_data.userId)))\n",
    "print(\"Total No of products  :\", len(np.unique(electronics_data.productId)))\n",
    "\n",
    "\n",
    "#Dropping the Timestamp column\n",
    "\n",
    "electronics_data.drop(['timestamp'], axis=1,inplace=True)\n",
    "\n",
    "#Analysis of rating given by the user \n",
    "\n",
    "no_of_rated_products_per_user = electronics_data.groupby(by='userId')['Rating'].count().sort_values(ascending=False)\n",
    "\n",
    "no_of_rated_products_per_user.head()\n",
    "\n",
    "no_of_rated_products_per_user.describe()\n",
    "\n",
    "quantiles = no_of_rated_products_per_user.quantile(np.arange(0,1.01,0.01), interpolation='higher')\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title(\"Quantiles and their Values\")\n",
    "quantiles.plot()\n",
    "# quantiles with 0.05 difference\n",
    "plt.scatter(x=quantiles.index[::5], y=quantiles.values[::5], c='orange', label=\"quantiles with 0.05 intervals\")\n",
    "# quantiles with 0.25 difference\n",
    "plt.scatter(x=quantiles.index[::25], y=quantiles.values[::25], c='m', label = \"quantiles with 0.25 intervals\")\n",
    "plt.ylabel('No of ratings by user')\n",
    "plt.xlabel('Value at the quantile')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "print('\\n No of rated product more than 50 per user : {}\\n'.format(sum(no_of_rated_products_per_user >= 50)) )\n",
    "\n",
    "#Getting the new dataframe which contains users who has given 50 or more ratings\n",
    "\n",
    "new_df=electronics_data.groupby(\"productId\").filter(lambda x:x['Rating'].count() >=50)\n",
    "\n",
    "no_of_ratings_per_product = new_df.groupby(by='productId')['Rating'].count().sort_values(ascending=False)\n",
    "\n",
    "fig = plt.figure(figsize=plt.figaspect(.5))\n",
    "ax = plt.gca()\n",
    "plt.plot(no_of_ratings_per_product.values)\n",
    "plt.title('# RATINGS per Product')\n",
    "plt.xlabel('Product')\n",
    "plt.ylabel('No of ratings per product')\n",
    "ax.set_xticklabels([])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Average rating of the product \n",
    "\n",
    "new_df.groupby('productId')['Rating'].mean().head()\n",
    "\n",
    "\n",
    "new_df.groupby('productId')['Rating'].mean().sort_values(ascending=False).head()\n",
    "\n",
    "#Total no of rating for product\n",
    "\n",
    "new_df.groupby('productId')['Rating'].count().sort_values(ascending=False).head()\n",
    "\n",
    "ratings_mean_count = pd.DataFrame(new_df.groupby('productId')['Rating'].mean())\n",
    "\n",
    "ratings_mean_count['rating_counts'] = pd.DataFrame(new_df.groupby('productId')['Rating'].count())\n",
    "\n",
    "ratings_mean_count.head()\n",
    "ratings_mean_count['rating_counts'].max()\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.rcParams['patch.force_edgecolor'] = True\n",
    "ratings_mean_count['rating_counts'].hist(bins=50)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.rcParams['patch.force_edgecolor'] = True\n",
    "ratings_mean_count['Rating'].hist(bins=50)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.rcParams['patch.force_edgecolor'] = True\n",
    "sns.jointplot(x='Rating', y='rating_counts', data=ratings_mean_count, alpha=0.4)\n",
    "\n",
    "popular_products = pd.DataFrame(new_df.groupby('productId')['Rating'].count())\n",
    "most_popular = popular_products.sort_values('Rating', ascending=False)\n",
    "most_popular.head(30).plot(kind = \"bar\")\n",
    "\n",
    "\n",
    "\n",
    "from surprise import KNNWithMeans\n",
    "from surprise import Dataset\n",
    "from surprise import accuracy\n",
    "from surprise import Reader\n",
    "import os\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#Reading the dataset\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(new_df,reader)\n",
    "\n",
    "\n",
    "#Splitting the dataset\n",
    "trainset, testset = train_test_split(data, test_size=0.3,random_state=10)\n",
    "\n",
    "\n",
    "# Use user_based true/false to switch between user-based or item-based collaborative filtering\n",
    "algo = KNNWithMeans(k=5, sim_options={'name': 'pearson_baseline', 'user_based': False})\n",
    "algo.fit(trainset)\n",
    "\n",
    "\n",
    "\n",
    "# run the trained model against the testset\n",
    "test_pred = algo.test(testset)\n",
    "\n",
    "\n",
    "# get RMSE\n",
    "print(\"Item-based Model : Test Set\")\n",
    "accuracy.rmse(test_pred, verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "new_df1=new_df.head(10000)\n",
    "ratings_matrix = new_df1.pivot_table(values='Rating', index='userId', columns='productId', fill_value=0)\n",
    "ratings_matrix.head()\n",
    "ratings_matrix.shape\n",
    "\n",
    "\n",
    "X = ratings_matrix.T\n",
    "X.head()\n",
    "\n",
    "X.shape\n",
    "X1 = X\n",
    "\n",
    "#Decomposing the Matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "SVD = TruncatedSVD(n_components=10)\n",
    "decomposed_matrix = SVD.fit_transform(X)\n",
    "decomposed_matrix.shape\n",
    "\n",
    "\n",
    "#Correlation Matrix\n",
    "\n",
    "correlation_matrix = np.corrcoef(decomposed_matrix)\n",
    "correlation_matrix.shape\n",
    "\n",
    "\n",
    "X.index[75]\n",
    "\n",
    "i = \"B00000K135\"\n",
    "\n",
    "product_names = list(X.index)\n",
    "product_ID = product_names.index(i)\n",
    "product_ID\n",
    "\n",
    "\n",
    "correlation_product_ID = correlation_matrix[product_ID]\n",
    "correlation_product_ID.shape\n",
    "\n",
    "Recommend = list(X.index[correlation_product_ID > 0.65])\n",
    "\n",
    "# Removes the item already bought by the customer\n",
    "Recommend.remove(i) \n",
    "\n",
    "Recommend[0:24]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "%matplotlib inline\n",
    "\n",
    "# Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from surprise import Reader, Dataset, SVD\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "df = pd.read_csv(\"/kaggle/input/amazon-product-reviews/ratings_Electronics (1).csv\",\n",
    "                             names=['userId', 'productId','rating','timestamp'])\n",
    "\n",
    "df.head()\n",
    "\n",
    "print(\"Total Reviews:\",df.shape[0])\n",
    "print(\"Total Columns:\",df.shape[1])\n",
    "\n",
    "# Taking subset of the dataset\n",
    "df = df.iloc[:5000,0:]\n",
    "\n",
    "print(\"Total Reviews:\",df.shape[0])\n",
    "print(\"Total Columns:\",df.shape[1])\n",
    "\n",
    "print(\"Total number of ratings :\",df.rating.nunique())\n",
    "print(\"Total number of users   :\", df.userId.nunique())\n",
    "print(\"Total number of products  :\", df.productId.nunique())\n",
    "\n",
    "df.info()\n",
    "\n",
    "# Check missing value\n",
    "df.isnull().sum()\n",
    "\n",
    "# Check Duplicate data\n",
    "df[df.duplicated()].any()\n",
    "\n",
    "# rating describe summary \n",
    "df.describe()['rating']\n",
    "\n",
    "print(\"Unique value of Rating:\",df.rating.unique())\n",
    "\n",
    "# Find the minimum and maximum ratings\n",
    "print('Minimum rating is: %d' %(df.rating.min()))\n",
    "print('Maximum rating is: %d' %(df.rating.max()))\n",
    "\n",
    "# Average rating of products\n",
    "ratings = pd.DataFrame(df.groupby('productId')['rating'].mean())\n",
    "ratings['ratings_count'] = pd.DataFrame(df.groupby('productId')['rating'].count())\n",
    "ratings['ratings_average'] = pd.DataFrame(df.groupby('productId')['rating'].mean())\n",
    "ratings.head(10)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "ratings['rating'].hist(bins=70)\n",
    "\n",
    "sns.jointplot(x='rating',y='ratings_count',data=ratings,alpha=0.5)\n",
    "\n",
    "# Most top 30 products\n",
    "popular_products = pd.DataFrame(df.groupby('productId')['rating'].count())\n",
    "most_popular = popular_products.sort_values('rating', ascending=False)\n",
    "most_popular.head(30).plot(kind = \"bar\",figsize=(12, 4))\n",
    "\n",
    "vote_counts = ratings[ratings['ratings_count'].notnull()]['ratings_count'].astype('int')\n",
    "vote_averages = ratings[ratings['ratings_average'].notnull()]['ratings_average'].astype('int')\n",
    "C = vote_averages.mean()\n",
    "print(\"Average rating of product across the whole dataset is\",C)\n",
    "\n",
    "m = vote_counts.quantile(0.95)\n",
    "print(\"Minimum votes required to be listed in the chart is\",m)\n",
    "\n",
    "ratings.head()\n",
    "\n",
    "qualified = ratings[(ratings['ratings_count'] >= m) & (ratings['ratings_count'].notnull()) & (ratings['ratings_average'].notnull())][['ratings_count', 'ratings_average']]\n",
    "\n",
    "qualified['ratings_count'] = qualified['ratings_count'].astype('int')\n",
    "qualified['ratings_average'] = qualified['ratings_average'].astype('int')\n",
    "qualified.head().sort_values(by='ratings_count', ascending=False)\n",
    "\n",
    "qualified.shape\n",
    "\n",
    "def weighted_rating(x):\n",
    "    v = x['ratings_count']\n",
    "    R = x['ratings_average']\n",
    "    return (v/(v+m) * R) + (m/(m+v) * C)\n",
    "\n",
    "qualified['wr'] = qualified.apply(weighted_rating, axis=1)\n",
    "qualified = qualified.sort_values('wr', ascending=False).head(20)\n",
    "qualified.head(10)\n",
    "\n",
    "# Add color\n",
    "from matplotlib import cm\n",
    "color = cm.inferno_r(np.linspace(.4, .8, 30))\n",
    "\n",
    "rating_plot_count = qualified['ratings_count'].plot.bar(figsize=(12, 4),color=color)\n",
    "rating_plot_count.set_title(\"Rating Count Bar-Plot\")\n",
    "rating_plot_count.set_xlabel(\"productId\")\n",
    "rating_plot_count.set_ylabel(\"Count\")\n",
    "\n",
    "\n",
    "rating_plot_avg = qualified['ratings_average'].plot.bar(figsize=(12, 4),color=color)\n",
    "rating_plot_avg.set_title(\"Rating Average Bar-Plot\")\n",
    "rating_plot_avg.set_xlabel(\"productId\")\n",
    "rating_plot_avg.set_ylabel(\"rating\")\n",
    "\n",
    "wr_plot = qualified['wr'].plot.bar(figsize=(12, 4),color=color)\n",
    "wr_plot.set_title(\"Weight Rating Bar-Plot\")\n",
    "wr_plot.set_xlabel(\"productId\")\n",
    "wr_plot.set_ylabel(\"rating\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "reader = Reader()\n",
    "df.head()\n",
    "\n",
    "data = Dataset.load_from_df(df[['userId', 'productId', 'rating']], reader)\n",
    "\n",
    "# Use the famous SVD algorithm\n",
    "svd = SVD()\n",
    "\n",
    "# Run 5-fold cross-validation and then print results\n",
    "cross_validate(svd, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n",
    "\n",
    "trainset = data.build_full_trainset()\n",
    "svd.fit(trainset)\n",
    "\n",
    "df[df['userId'] == 'AKM1MP6P0OYPR']\n",
    "\n",
    "svd.predict(uid='A17HMM1M7T9PJ1', iid='0970407998', r_ui=None)\n",
    "svd.predict(uid='A17HMM1M7T9PJ1', iid='0970407998', r_ui=None).est\n",
    "\n",
    "\n",
    "df_users=df.groupby('userId').filter(lambda x: x['rating'].count()>=50)\n",
    "df_users.head()\n",
    "df_users.shape\n",
    "\n",
    "\n",
    "matrix=pd.pivot_table(data=df_users, values='rating', index='userId',columns='productId')\n",
    "matrix.head()\n",
    "\n",
    "# Function that takes in productId and useId as input and outputs up to 5 most similar products.\n",
    "def hybrid_recommendations(userId, productId):\n",
    "    \n",
    "    # Get the Id of the top five products that are correlated with the ProductId chosen by the user.\n",
    "    top_five=matrix.corrwith(matrix[productId]).sort_values(ascending=False).head(5)\n",
    "    \n",
    "    # Predict the ratings the user might give to these top 5 most correlated products.\n",
    "    est_rating=[]\n",
    "    for x in list(top_five.index):\n",
    "        if str(top_five[x])!='nan':\n",
    "            est_rating.append(svd.predict(userId, iid=x, r_ui=None).est)\n",
    "           \n",
    "    return pd.DataFrame({'productId':list(top_five.index)[:len(est_rating)], 'estimated_rating':est_rating}).sort_values(by='estimated_rating', ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "hybrid_recommendations('A2NYK9KWFMJV4Y', 'B00LI4ZZO8')\n",
    "\n",
    "# df.head()\n",
    "\n",
    "# df['userId'].value_counts()\n",
    "\n",
    "# # Check specific userId review\n",
    "# df[df['userId'] == 'A3LDPF5FMB782Z']\n",
    "\n",
    "# # predict based on this data\n",
    "# svd.predict('A3LDPF5FMB782Z', '140053271X', 5.0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "# Build a model.\n",
    "class RankingModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        embedding_dimension = 32\n",
    "\n",
    "        self.user_embeddings = tf.keras.Sequential([\n",
    "                                    tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "                                        vocabulary=unique_userIds, mask_token=None),\n",
    "                                        # add addional embedding to account for unknow tokens\n",
    "                                    tf.keras.layers.Embedding(len(unique_userIds)+1, embedding_dimension)\n",
    "                                    ])\n",
    "\n",
    "        self.product_embeddings = tf.keras.Sequential([\n",
    "                                    tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "                                        vocabulary=unique_productIds, mask_token=None),\n",
    "                                    # add addional embedding to account for unknow tokens\n",
    "                                    tf.keras.layers.Embedding(len(unique_productIds)+1, embedding_dimension)\n",
    "                                    ])\n",
    "        # Set up a retrieval task and evaluation metrics over the\n",
    "        # entire dataset of candidates.\n",
    "        self.ratings = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "                            tf.keras.layers.Dense(64,  activation=\"relu\"),\n",
    "                            tf.keras.layers.Dense(1)\n",
    "                              ])\n",
    "    def call(self, userId, productId):\n",
    "        user_embeddings  = self.user_embeddings (userId)\n",
    "        product_embeddings = self.product_embeddings(productId)\n",
    "        return self.ratings(tf.concat([user_embeddings,product_embeddings], axis=1))\n",
    "\n",
    "# Build a model.\n",
    "class amazonModel(tfrs.models.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ranking_model: tf.keras.Model = RankingModel()\n",
    "        self.task: tf.keras.layers.Layer   = tfrs.tasks.Ranking(\n",
    "                                                    loss    =  tf.keras.losses.MeanSquaredError(),\n",
    "                                                    metrics = [tf.keras.metrics.RootMeanSquaredError()])\n",
    "            \n",
    "\n",
    "    def compute_loss(self, features, training=False):\n",
    "        rating_predictions = self.ranking_model(features[\"userId\"], features[\"productId\"]  )\n",
    "\n",
    "        return self.task( labels=features[\"rating\"], predictions=rating_predictions)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cutoff_no_rat = 50    ## Only count products which received more than or equal 50\n",
    "cutoff_year   = 2011  ## Only count Rating after 2011\n",
    "recent_data   = data_by_date.loc[data_by_date[\"year\"] > cutoff_year]\n",
    "print(\"Number of Rating: {:,}\".format(recent_data.shape[0]) )\n",
    "print(\"Number of Users: {:,}\".format(len(recent_data.userId.unique()) ) )\n",
    "print(\"Number of Products: {:,}\".format(len(recent_data.productId.unique())  ) )\n",
    "del data_by_date  ### Free up memory ###\n",
    "recent_prod   = recent_data.loc[recent_data.groupby(\"productId\")[\"rating\"].transform('count').ge(cutoff_no_rat)].reset_index(\n",
    "                    drop=True).drop([\"timestamp\",\"year\",\"month\"],axis=1)\n",
    "del recent_data  ### Free up memory ###\n",
    "\n",
    "\n",
    "userIds    = recent_prod.userId.unique()\n",
    "productIds = recent_prod.productId.unique()\n",
    "total_ratings= len(recent_prod.index)\n",
    "\n",
    "\n",
    "ratings = tf.data.Dataset.from_tensor_slices( {\"userId\":tf.cast( recent_prod.userId.values  ,tf.string),\n",
    "                                \"productId\":tf.cast( recent_prod.productId.values,tf.string),\n",
    "                                \"rating\":tf.cast( recent_prod.rating.values  ,tf.int8,) } )\n",
    "\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take( int(total_ratings*0.8) )\n",
    "test = shuffled.skip(int(total_ratings*0.8)).take(int(total_ratings*0.2))\n",
    "\n",
    "unique_productIds = productIds\n",
    "unique_userIds    = userIds\n",
    "\n",
    "\n",
    "model = amazonModel()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad( learning_rate=0.1 ))\n",
    "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
    "cached_test = test.batch(4096).cache()\n",
    "model.fit(cached_train, epochs=10)\n",
    "\n",
    "\n",
    "# Evaluate.\n",
    "model.evaluate(cached_test, return_dict=True)\n",
    "\n",
    "\n",
    "user_rand = userIds[123]\n",
    "test_rating = {}\n",
    "for m in test.take(5):\n",
    "    test_rating[m[\"productId\"].numpy()]=RankingModel()(tf.convert_to_tensor([user_rand]),tf.convert_to_tensor([m[\"productId\"]]))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Top 5 recommended products for User {}: \".format(user_rand))\n",
    "for m in sorted(test_rating, key=test_rating.get, reverse=True):\n",
    "    print(m.decode())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "from scipy.sparse import csr_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from surprise import Reader, Dataset, SVD\n",
    "from surprise.model_selection import KFold\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "# Import both datasets\n",
    "\n",
    "df5=pd.read_csv('../input/amazon-cell-phones-reviews/20191226-items.csv')\n",
    "df6=pd.read_csv('../input/amazon-cell-phones-reviews/20191226-reviews.csv')\n",
    "\n",
    "df5.rename(columns={'rating':'avgRating'}, inplace=True)\n",
    "columns = ['url', 'reviewUrl', 'totalReviews', 'originalPrice']\n",
    "df5.drop(columns, inplace=True, axis=1)\n",
    "# Drop uneeded columns before merging both datasets\n",
    "\n",
    "columns = ['date', 'verified', 'title', 'body', 'helpfulVotes']\n",
    "df6.drop(columns, inplace=True, axis=1)\n",
    "\n",
    "df6\n",
    "# Merging 2 df to create training data for the SVD\n",
    "\n",
    "ratings = pd.merge(df5, df6, how='inner', on='asin')\n",
    "\n",
    "columns = ['brand', 'price', 'image', 'avgRating']\n",
    "ratings.drop(columns, inplace=True, axis=1)\n",
    "\n",
    "ratings = ratings[['name', 'asin', 'title', 'rating']]\n",
    "ratings = ratings.sort_values(by=['name'], ascending=True)\n",
    "ratings = ratings.reset_index(drop=True)\n",
    "\n",
    "ratings\n",
    "\n",
    "\n",
    "# Create a pivot table to see how the data columns correspond to one another\n",
    "\n",
    "matrix=pd.pivot_table(data=ratings[['name', 'asin', 'rating']], values='rating', index='name',columns='asin')\n",
    "matrix.head()\n",
    "\n",
    "# Initialize the SVD model and train the model on the created dataset\n",
    "\n",
    "svd = SVD()\n",
    "reader = Reader()\n",
    "data = Dataset.load_from_df(ratings[['name', 'asin', 'rating']], reader)\n",
    "cross_validate(svd, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n",
    "\n",
    "trainset = data.build_full_trainset()\n",
    "svd.fit(trainset)\n",
    "\n",
    "# Function that takes in productId and userId as input and outputs up to 5 most similar products.\n",
    "def hybrid_recommendations(userId, productId):\n",
    "    \n",
    "    # Get the Id of the top five products that are correlated with the ProductId chosen by the user.\n",
    "    top_five=matrix.corrwith(matrix[productId]).sort_values(ascending=False).head(15)\n",
    "    \n",
    "    # Predict the ratings the user might give to these top 5 most correlated products.\n",
    "    est_rating=[]\n",
    "    for x in list(top_five.index):\n",
    "        if str(top_five[x])!='nan':\n",
    "            est_rating.append(svd.predict(userId, iid=x, r_ui=None).est)\n",
    "           \n",
    "    return pd.DataFrame({'productId':list(top_five.index)[:len(est_rating)], 'estimated_rating':est_rating}).sort_values(by='estimated_rating', ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "find_product = hybrid_recommendations('John', 'B018OMP8ES')\n",
    "\n",
    "# Function that maps the productId to the productName\n",
    "def product_mapping(recommender_df): \n",
    "    rows_list = []\n",
    "    rows_list2 = []\n",
    "\n",
    "    # Append productName into finalDf\n",
    "    for x in recommender_df['productId']:\n",
    "        cut_row = ratings.loc[ratings['asin'] == x]\n",
    "        cut_row = cut_row['title'][0:1].values\n",
    "        rows_list.append(cut_row[0])\n",
    "\n",
    "    # Copy over the estimated ratings into the finalDf\n",
    "    for i in recommender_df['estimated_rating']:\n",
    "        rows_list2.append(i)\n",
    "\n",
    "    # Creating the finalDf\n",
    "    cut_dict = {\"product\": rows_list, \"estimated_rating\": rows_list2}\n",
    "    cut_df = pd.DataFrame(cut_dict)\n",
    "    \n",
    "    return cut_df\n",
    "\n",
    "# Test 1\n",
    "product_mapping(find_product)\n",
    "\n",
    "# Test 2\n",
    "product_mapping(hybrid_recommendations('Peter ', 'B077T4MVZ6'))\n",
    "\n",
    "# Test 3\n",
    "product_mapping(hybrid_recommendations('Sarah ', 'B081H6STQQ'))\n",
    "\n",
    "\n",
    "\n",
    "# Save the model into a joblib file\n",
    "from joblib import dump, load\n",
    "\n",
    "dump(svd, 'svd.joblib')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compute the accuracy of models\n",
    "from surprise import accuracy\n",
    "\n",
    "# Class is used to parse a file containing ratings, data should be in structure - user ; item ; rating\n",
    "from surprise.reader import Reader\n",
    "\n",
    "# Class for loading datasets\n",
    "from surprise.dataset import Dataset\n",
    "\n",
    "# For tuning model hyperparameters\n",
    "from surprise.model_selection import GridSearchCV\n",
    "\n",
    "# For splitting the rating data in train and test datasets\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "# For implementing similarity-based recommendation system\n",
    "from surprise.prediction_algorithms.knns import KNNBasic\n",
    "\n",
    "# For implementing matrix factorization based recommendation system\n",
    "from surprise.prediction_algorithms.matrix_factorization import SVD\n",
    "\n",
    "# for implementing K-Fold cross-validation\n",
    "from surprise.model_selection import KFold\n",
    "\n",
    "# For implementing clustering-based recommendation system\n",
    "from surprise import CoClustering\n",
    "\n",
    "\n",
    "def precision_recall_at_k(model, k = 10, threshold = 3.5):\n",
    "    \"\"\"Return precision and recall at k metrics for each user\"\"\"\n",
    "\n",
    "    # First map the predictions to each user\n",
    "    user_est_true = defaultdict(list)\n",
    "\n",
    "    # Making predictions on the test data\n",
    "    predictions = model.test(testset)\n",
    "\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key = lambda x: x[0], reverse = True)\n",
    "\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n",
    "                              for (est, true_r) in user_ratings[:k])\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        # When n_rec_k is 0, Precision is undefined. Therefore, we are setting Precision to 0 when n_rec_k is 0\n",
    "\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        # When n_rel is 0, Recall is undefined. Therefore, we are setting Recall to 0 when n_rel is 0\n",
    "\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
    "\n",
    "    # Mean of all the predicted precisions are calculated.\n",
    "    precision = round((sum(prec for prec in precisions.values()) / len(precisions)), 3)\n",
    "\n",
    "    # Mean of all the predicted recalls are calculated.\n",
    "    recall = round((sum(rec for rec in recalls.values()) / len(recalls)), 3)\n",
    "\n",
    "    accuracy.rmse(predictions)\n",
    "\n",
    "    print('Precision: ', precision) # Command to print the overall precision\n",
    "\n",
    "    print('Recall: ', recall) # Command to print the overall recall\n",
    "\n",
    "    print('F_1 score: ', round((2*precision*recall)/(precision+recall), 3)) # Formula to compute the F-1 score\n",
    "\n",
    "\n",
    "\n",
    "# Instantiating Reader scale with expected rating scale\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "# Loading the rating dataset\n",
    "df = Dataset.load_from_df(df[['user_id', 'prod_id', 'rating']], reader)\n",
    "\n",
    "# Splitting the data into train and test datasets\n",
    "trainset, testset = train_test_split(df, test_size=0.7, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Declaring the similarity options\n",
    "sim_options = {'name': 'cosine', 'user_based': True}\n",
    "\n",
    "# Initialize the KNNBasic model using sim_options declared, Verbose = False, and setting random_state = 1\n",
    "algo_knn_user = KNNBasic(sim_options=sim_options, verbose=False, random_state=1)\n",
    "\n",
    "# Fit the model on the training data\n",
    "algo_knn_user.fit(trainset)\n",
    "\n",
    "# Let us compute precision@k, recall@k, and f_1 score using the precision_recall_at_k function defined above\n",
    "precision_recall_at_k(algo_knn_user)\n",
    "\n",
    "\n",
    "# Predicting rating for a sample user with an interacted product\n",
    "algo_knn_user.predict('A3LDPF5FMB782Z', '1400501466', r_ui=5, verbose=True)\n",
    "\n",
    "\n",
    "# Unique user_id where prod_id is not equal to \"1400501466\"\n",
    "df_final.loc[df_final['prod_id'] != \"1400501466\", 'user_id'].unique()\n",
    "\n",
    "\n",
    "# Predicting rating for a sample user with a non interacted product\n",
    "algo_knn_user.predict('A34BZM6S9L7QI4', '1400501466', verbose=True)\n",
    "\n",
    "# Setting up parameter grid to tune the hyperparameters\n",
    "param_grid = {'k': [20, 30, 40], 'min_k': [3, 6, 9],\n",
    "              'sim_options': {'name': ['msd', 'cosine', 'pearson'],\n",
    "                              'user_based': [True]}\n",
    "              }\n",
    "\n",
    "# Performing 3-fold cross-validation to tune the hyperparameters\n",
    "gs = GridSearchCV(KNNBasic, param_grid, measures=['rmse'], cv=3, n_jobs=-1)\n",
    "\n",
    "# Fitting the data\n",
    "gs.fit(df)\n",
    "\n",
    "# Best RMSE score\n",
    "print(gs.best_score['rmse'])\n",
    "\n",
    "# Combination of parameters that gave the best RMSE score\n",
    "print(gs.best_params['rmse'])\n",
    "\n",
    "\n",
    "\n",
    "# Using the optimal similarity measure for user-user based collaborative filtering\n",
    "sim_options = {'name': 'cosine',\n",
    "               'user_based': True}\n",
    "\n",
    "# Creating an instance of KNNBasic with optimal hyperparameter values\n",
    "similarity_algo_optimized = KNNBasic(sim_options=sim_options, k=40, min_k=6, verbose=False, random_state=1)\n",
    "\n",
    "# Training the algorithm on the trainset\n",
    "similarity_algo_optimized.fit(trainset)\n",
    "\n",
    "# Let us compute precision@k and recall@k also with k =10\n",
    "precision_recall_at_k(similarity_algo_optimized)\n",
    "\n",
    "# sim_user_user_optimized model to recommend for userId \"A3LDPF5FMB782Z\" and productId 1400501466\n",
    "similarity_algo_optimized.predict('A3LDPF5FMB782Z', '1400501466', r_ui=5, verbose=True)\n",
    "\n",
    "\n",
    "# sim_user_user_optimized model to recommend for userId \"A34BZM6S9L7QI4\" and productId \"1400501466\"\n",
    "similarity_algo_optimized.predict('A34BZM6S9L7QI4', '1400501466', verbose=True)\n",
    "\n",
    "\n",
    "# 0 is the inner id of the above user\n",
    "similarity_algo_optimized.get_neighbors(0, k=5)\n",
    "\n",
    "\n",
    "def get_recommendations(data, user_id, top_n, algo):\n",
    "\n",
    "    # Creating an empty list to store the recommended product ids\n",
    "    recommendations = []\n",
    "\n",
    "    # Creating an user item interactions matrix\n",
    "    user_item_interactions_matrix = data.pivot(index = 'user_id', columns = 'prod_id', values = 'rating')\n",
    "\n",
    "    # Extracting those product ids which the user_id has not interacted yet\n",
    "    non_interacted_products = user_item_interactions_matrix.loc[user_id][user_item_interactions_matrix.loc[user_id].isnull()].index.tolist()\n",
    "\n",
    "    # Looping through each of the product ids which user_id has not interacted yet\n",
    "    for item_id in non_interacted_products:\n",
    "\n",
    "        # Predicting the ratings for those non interacted product ids by this user\n",
    "        est = algo.predict(user_id, item_id).est\n",
    "\n",
    "        # Appending the predicted ratings\n",
    "        recommendations.append((item_id, est))\n",
    "\n",
    "    # Sorting the predicted ratings in descending order\n",
    "    recommendations.sort(key = lambda x: x[1], reverse = True)\n",
    "\n",
    "    return recommendations[:top_n] # Returing top n highest predicted rating products for this user\n",
    "\n",
    "\n",
    "# Making top 5 recommendations for user_id \"A3LDPF5FMB782Z\" with a similarity-based recommendation engine\n",
    "recommendations = get_recommendations(df_final, 'A3LDPF5FMB782Z', 5, algo_knn_user)\n",
    "\n",
    "\n",
    "# Building the dataframe for above recommendations with columns \"prod_id\" and \"predicted_ratings\"\n",
    "pd.DataFrame(recommendations, columns=['prod_Id', 'predicted_ratings'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".sistema-de-recomendacion-unab-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
